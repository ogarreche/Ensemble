{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensemble with NSL-KDD\n",
    "# Parameters\n",
    "\n",
    "#----------------------------------------------\n",
    "# 0 for not using it as base learner\n",
    "# 1 for using it as base learner\n",
    "\n",
    "use_model_ada = 1 \n",
    "use_model_dnn = 1 \n",
    "use_model_mlp = 1 \n",
    "use_model_lgbm = 1 \n",
    "use_model_rf = 1 \n",
    "use_model_svm = 1\n",
    "use_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "# 0 for training the model\n",
    "# 1 for using the saved version of the model\n",
    "\n",
    "load_model_ada = 0 \n",
    "load_model_dnn = 0 \n",
    "load_model_mlp = 0 \n",
    "load_model_lgbm = 0 \n",
    "load_model_rf = 0 \n",
    "load_model_svm = 0\n",
    "load_model_knn = 0 \n",
    "#----------------------------------------------\n",
    "\n",
    "# load_model_ada = 1\n",
    "# load_model_dnn = 1 \n",
    "# load_model_mlp = 1 \n",
    "# load_model_lgbm = 1 \n",
    "# load_model_rf = 1 \n",
    "# load_model_svm = 1\n",
    "# load_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the name of the output text file\n",
    "output_file_name = \"ensemble_prob_FS.txt\"\n",
    "with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('---- Start Ensemble Model Info - v0 ----', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oarreche@ads.iu.edu/anaconda3/envs/HITL/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# #Defining metric functions\n",
    "# def ACC(TP,TN,FP,FN):\n",
    "#     Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "#     return Acc\n",
    "# def ACC_2 (TP, FN):\n",
    "#     ac = (TP/(TP+FN))\n",
    "#     return ac\n",
    "# def PRECISION(TP,FP):\n",
    "#     eps = 1e-7\n",
    "#     Precision = TP/(TP+FP+eps)\n",
    "    \n",
    "\n",
    "#     return Precision\n",
    "# def RECALL(TP,FN):\n",
    "#     Recall = TP/(TP+FN)\n",
    "#     return Recall\n",
    "# def F1(Recall, Precision):\n",
    "#     F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "#     return F1\n",
    "# def BACC(TP,TN,FP,FN):\n",
    "#     BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "#     return BACC\n",
    "# def MCC(TP,TN,FP,FN):\n",
    "#     eps = 1e-7\n",
    "#     MCC = (TN*TP-FN*FP)/(((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**.5)\n",
    "#     return MCC\n",
    "# def AUC_ROC(y_test_bin,y_score):\n",
    "#     fpr = dict()\n",
    "#     tpr = dict()\n",
    "#     roc_auc = dict()\n",
    "#     auc_avg = 0\n",
    "#     counting = 0\n",
    "#     for i in range(n_classes):\n",
    "#       fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "#      # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "#       #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "#       auc_avg += auc(fpr[i], tpr[i])\n",
    "#       counting = i+1\n",
    "#     return auc_avg/counting\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# attach the column names to the dataset\n",
    "feature=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "          \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "          \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "          \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "          \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"]\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "\n",
    "train='KDDTrain+.txt'\n",
    "test='KDDTest+.txt'\n",
    "\n",
    "df=pd.read_csv(train,names=feature)\n",
    "df_test=pd.read_csv(test,names=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 43)\n",
      "Dimensions of the Test set: (22544, 43)\n",
      "Label distribution Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "xterm                13\n",
      "rootkit              13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "udpstorm              2\n",
      "loadmodule            2\n",
      "phf                   2\n",
      "sqlattack             2\n",
      "perl                  2\n",
      "worm                  2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n",
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in service:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n",
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 64 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n",
      "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n",
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n",
      "(125973, 123)\n",
      "(22544, 123)\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "X_train has shape: (125973, 122) \n",
      "y_train has shape: (125973, 1)\n",
      "X_test has shape: (22544, 122) \n",
      "y_test has shape: (22544, 1)\n",
      "Counter({0: 67343, 1: 45927, 2: 11656, 3: 995, 4: 52})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n",
    "\n",
    "\n",
    "df.drop(['difficulty'],axis=1,inplace=True)\n",
    "df_test.drop(['difficulty'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('Label distribution Training set:')\n",
    "print(df['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(df_test['label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "\n",
    "\n",
    "# protocol type\n",
    "unique_protocol=sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(df_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n",
    "\n",
    "df_cat_data.head()\n",
    "\n",
    "\n",
    "trainservice=df['service'].tolist()\n",
    "testservice= df_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference\n",
    "\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "\n",
    "newdf=df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "\n",
    "\n",
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())\n",
    "\n",
    "\n",
    "# Specify your selected features. Note that you'll need to modify this list according to your final processed dataframe\n",
    "#Uncomment the below lines to use these top 20 features from shap analysis\n",
    "#selected_features = [\"root_shell\",\"service_telnet\",\"num_shells\",\"service_uucp\",\"dst_host_same_src_port_rate\"\n",
    "#                     ,\"dst_host_rerror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_srv_count\",\"service_private\",\"logged_in\",\n",
    "#                    \"dst_host_serror_rate\",\"serror_rate\",\"srv_serror_rate\",\"flag_S0\",\"diff_srv_rate\",\"dst_host_srv_diff_host_rate\",\"num_file_creations\",\"flag_RSTR\"#,\"dst_host_same_srv_rate\",\"service_Idap\",\"label\"]\n",
    "                     \n",
    "\n",
    "# Select those features from your dataframe\n",
    "#newdf = newdf[selected_features]\n",
    "#newdf_test = newdf_test[selected_features]\n",
    "\n",
    "# Now your dataframe only contains your selected features.\n",
    "\n",
    "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
    "multi_data = newdf.copy()\n",
    "multi_label = pd.DataFrame(multi_data.label)\n",
    "\n",
    "multi_data_test=newdf_test.copy()\n",
    "multi_label_test = pd.DataFrame(multi_data_test.label)\n",
    "\n",
    "\n",
    "# using standard scaler for normalizing\n",
    "std_scaler = StandardScaler()\n",
    "def standardization(df,col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "    return df\n",
    "\n",
    "numeric_col = multi_data.select_dtypes(include='number').columns\n",
    "data = standardization(multi_data,numeric_col)\n",
    "numeric_col_test = multi_data_test.select_dtypes(include='number').columns\n",
    "data_test = standardization(multi_data_test,numeric_col_test)\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2_test = preprocessing.LabelEncoder()\n",
    "enc_label = multi_label.apply(le2.fit_transform)\n",
    "enc_label_test = multi_label_test.apply(le2_test.fit_transform)\n",
    "multi_data = multi_data.copy()\n",
    "multi_data_test = multi_data_test.copy()\n",
    "\n",
    "multi_data['intrusion'] = enc_label\n",
    "multi_data_test['intrusion'] = enc_label_test\n",
    "\n",
    "#y_mul = multi_data['intrusion']\n",
    "multi_data\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "\n",
    "multi_data.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data\n",
    "multi_data_test.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "y_train_multi= multi_data[['intrusion']]\n",
    "X_train_multi= multi_data.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_train has shape:',X_train_multi.shape,'\\ny_train has shape:',y_train_multi.shape)\n",
    "\n",
    "y_test_multi= multi_data_test[['intrusion']]\n",
    "X_test_multi= multi_data_test.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_test has shape:',X_test_multi.shape,'\\ny_test has shape:',y_test_multi.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train_multi['intrusion'])\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train_multi = LabelBinarizer().fit_transform(y_train_multi)\n",
    "\n",
    "y_test_multi = LabelBinarizer().fit_transform(y_test_multi)\n",
    "\n",
    "\n",
    "Y_train=y_train_multi.copy()\n",
    "X_train=X_train_multi.copy()\n",
    "\n",
    "Y_test=y_test_multi.copy()\n",
    "X_test=X_test_multi.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\n\\n# Number of best features you want to select\\nk = 15\\n\\n# Initialize a dataframe to store the scores for each feature against each class\\nfeature_scores = pd.DataFrame(index=X_train.columns)\\n\\n# Loop through each class\\nfor class_index in range(Y_train.shape[1]):\\n    \\n    # Get the current class labels\\n    y_train_current_class = Y_train[:, class_index]\\n    \\n    # Select K best features for the current class\\n    best_features = SelectKBest(score_func=f_classif, k=\\'all\\')\\n    fit = best_features.fit(X_train, y_train_current_class)\\n\\n    # Get the scores\\n    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\\n    \\n    # Concatenate the scores to the main dataframe\\n    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\\n\\n# Get the sum of the scores for each feature\\nfeature_scores[\\'total\\'] = feature_scores.sum(axis=1)\\n\\n# Get the top k features in a list\\ntop_k_features = feature_scores.nlargest(k, \\'total\\').index.tolist()\\n\\nprint(top_k_features)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# In[24]:\n",
    "\n",
    "'''\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Number of best features you want to select\n",
    "k = 15\n",
    "\n",
    "# Initialize a dataframe to store the scores for each feature against each class\n",
    "feature_scores = pd.DataFrame(index=X_train.columns)\n",
    "\n",
    "# Loop through each class\n",
    "for class_index in range(Y_train.shape[1]):\n",
    "    \n",
    "    # Get the current class labels\n",
    "    y_train_current_class = Y_train[:, class_index]\n",
    "    \n",
    "    # Select K best features for the current class\n",
    "    best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "    fit = best_features.fit(X_train, y_train_current_class)\n",
    "\n",
    "    # Get the scores\n",
    "    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\n",
    "    \n",
    "    # Concatenate the scores to the main dataframe\n",
    "    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\n",
    "\n",
    "# Get the sum of the scores for each feature\n",
    "feature_scores['total'] = feature_scores.sum(axis=1)\n",
    "\n",
    "# Get the top k features in a list\n",
    "top_k_features = feature_scores.nlargest(k, 'total').index.tolist()\n",
    "\n",
    "print(top_k_features)\n",
    "\n",
    "'''\n",
    "# In[32]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.10249223e-01, -7.67859947e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.73736981e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.76224074e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02, -1.21190076e+00, -4.64315895e-02],\n",
       "       ...,\n",
       "       [-9.29714678e-02, -7.36430591e-03, -3.87394518e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-8.68282658e-02, -7.36430591e-03, -3.87568593e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [ 1.61587463e-01, -7.46804833e-03,  1.06953862e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Assuming you have features X and labels Y\n",
    "# X, Y = make_classification()\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=100)\n",
    "\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "X_train.values\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_class_train = np.argmax(y_train_multi, axis=1)\n",
    "single_class_test = np.argmax(y_test_multi, axis=1)\n",
    "\n",
    "\n",
    "df1 = X_train_multi.assign(Label = single_class_train)\n",
    "df2 =  X_test_multi.assign(Label = single_class_test)\n",
    "\n",
    "frames = [df1,  df2]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "feature_selection = [\n",
    "                    'dst_host_same_srv_rate',\n",
    "                    'dst_host_srv_count',\n",
    "                    'dst_host_same_src_port_rate',\n",
    "                    'logged_in',\n",
    "                    'dst_host_serror_rate',\n",
    "                    'count',\n",
    "                    'srv_count',\n",
    "                    'dst_host_rerror_rate',\n",
    "                    'Label'\n",
    "                    ]\n",
    "\n",
    "df_og = df\n",
    "df = df[feature_selection]\n",
    "\n",
    "# df.pop('dst host same srv rate')\n",
    "# df.pop('dst host srv count')\n",
    "# df.pop('dst host same src port rate')\n",
    "# df.pop('logged in')\n",
    "# df.pop('dst host serror rate')\n",
    "# df.pop('count')\n",
    "# df.pop('srv count')\n",
    "# df.pop('dst host rerror rate')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "\n",
    "y1, y2 = pd.factorize(y)\n",
    "\n",
    "y_0 = pd.DataFrame(y1)\n",
    "y_1 = pd.DataFrame(y1)\n",
    "y_2 = pd.DataFrame(y1)\n",
    "y_3 = pd.DataFrame(y1)\n",
    "y_4 = pd.DataFrame(y1)\n",
    "\n",
    "\n",
    "# y_0 = y_0.replace(0, 0)\n",
    "# y_0 = y_0.replace(1, 1)\n",
    "y_0 = y_0.replace(2, 1)\n",
    "y_0 = y_0.replace(3, 1)\n",
    "y_0 = y_0.replace(4, 1)\n",
    "\n",
    "\n",
    "y_1 = y_1.replace(1, 999)\n",
    "y_1 = y_1.replace(0, 1)\n",
    "# y_1 = y_1.replace(1, 0)\n",
    "y_1 = y_1.replace(2, 1)\n",
    "y_1 = y_1.replace(3, 1)\n",
    "y_1 = y_1.replace(4, 1)\n",
    "y_1 = y_1.replace(999, 1)\n",
    "\n",
    "\n",
    "y_2 = y_2.replace(0, 1)\n",
    "y_2 = y_2.replace(1, 1)\n",
    "y_2 = y_2.replace(2, 0)\n",
    "y_2 = y_2.replace(3, 1)\n",
    "y_2 = y_2.replace(4, 1)\n",
    "\n",
    "\n",
    "y_3 = y_3.replace(0, 1)\n",
    "# y_3 = y_3.replace(1, 1)\n",
    "y_3 = y_3.replace(2, 1)\n",
    "y_3 = y_3.replace(3, 0)\n",
    "y_3 = y_3.replace(4, 1)\n",
    "\n",
    "\n",
    "y_4 = y_4.replace(0, 1)\n",
    "# y_4 = y_4.replace(1, 1)\n",
    "y_4 = y_4.replace(2, 1)\n",
    "y_4 = y_4.replace(3, 1)\n",
    "y_4 = y_4.replace(4, 0)\n",
    "\n",
    "\n",
    "\n",
    "df = df.assign(Label = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset between level 00 and level 01\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "split = 0.5 # 0.7\n",
    "\n",
    "# X_00,X_01, y_00, y_01 = sklearn.model_selection.train_test_split(X, y, train_size=split)\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 77054, 1: 53387, 2: 14077, 3: 3880, 4: 119})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts2 = Counter(y)\n",
    "print(label_counts2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base learner Split\n",
    "# split = 0.7\n",
    "\n",
    "# X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_00, y_00, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst_host_same_srv_rate</th>\n",
       "      <th>dst_host_srv_count</th>\n",
       "      <th>dst_host_same_src_port_rate</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>dst_host_serror_rate</th>\n",
       "      <th>count</th>\n",
       "      <th>srv_count</th>\n",
       "      <th>dst_host_rerror_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>113397</th>\n",
       "      <td>1.066401</td>\n",
       "      <td>0.807092</td>\n",
       "      <td>-0.383108</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>-0.559851</td>\n",
       "      <td>-0.106530</td>\n",
       "      <td>2.548205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135078</th>\n",
       "      <td>0.898090</td>\n",
       "      <td>1.022079</td>\n",
       "      <td>-0.431856</td>\n",
       "      <td>1.123125</td>\n",
       "      <td>-0.358118</td>\n",
       "      <td>-0.544813</td>\n",
       "      <td>-0.248420</td>\n",
       "      <td>-0.602719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106474</th>\n",
       "      <td>-0.826915</td>\n",
       "      <td>-0.692425</td>\n",
       "      <td>-0.447834</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>1.608759</td>\n",
       "      <td>-0.097002</td>\n",
       "      <td>-0.120298</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110039</th>\n",
       "      <td>-1.138756</td>\n",
       "      <td>-1.026654</td>\n",
       "      <td>-0.480197</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>1.608759</td>\n",
       "      <td>0.287250</td>\n",
       "      <td>-0.354343</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112503</th>\n",
       "      <td>1.066401</td>\n",
       "      <td>1.258754</td>\n",
       "      <td>-0.480197</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>-0.708312</td>\n",
       "      <td>-0.078996</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87261</th>\n",
       "      <td>1.066401</td>\n",
       "      <td>1.258754</td>\n",
       "      <td>-0.415471</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>-0.647181</td>\n",
       "      <td>-0.216669</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102882</th>\n",
       "      <td>1.066401</td>\n",
       "      <td>-1.035688</td>\n",
       "      <td>2.756092</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>-0.717045</td>\n",
       "      <td>-0.354343</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15251</th>\n",
       "      <td>-1.161030</td>\n",
       "      <td>-1.035688</td>\n",
       "      <td>-0.480197</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.279805</td>\n",
       "      <td>3.710587</td>\n",
       "      <td>-0.368110</td>\n",
       "      <td>2.352482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47685</th>\n",
       "      <td>-0.002766</td>\n",
       "      <td>0.147666</td>\n",
       "      <td>1.202673</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>3.728053</td>\n",
       "      <td>6.653245</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17342</th>\n",
       "      <td>1.066401</td>\n",
       "      <td>0.770960</td>\n",
       "      <td>2.756092</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.639532</td>\n",
       "      <td>-0.725778</td>\n",
       "      <td>-0.134065</td>\n",
       "      <td>-0.387635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74258 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dst_host_same_srv_rate  dst_host_srv_count  \\\n",
       "113397                1.066401            0.807092   \n",
       "135078                0.898090            1.022079   \n",
       "106474               -0.826915           -0.692425   \n",
       "110039               -1.138756           -1.026654   \n",
       "112503                1.066401            1.258754   \n",
       "...                        ...                 ...   \n",
       "87261                 1.066401            1.258754   \n",
       "102882                1.066401           -1.035688   \n",
       "15251                -1.161030           -1.035688   \n",
       "47685                -0.002766            0.147666   \n",
       "17342                 1.066401            0.770960   \n",
       "\n",
       "        dst_host_same_src_port_rate  logged_in  dst_host_serror_rate  \\\n",
       "113397                    -0.383108  -0.809262             -0.639532   \n",
       "135078                    -0.431856   1.123125             -0.358118   \n",
       "106474                    -0.447834  -0.809262              1.608759   \n",
       "110039                    -0.480197  -0.809262              1.608759   \n",
       "112503                    -0.480197   1.235694             -0.639532   \n",
       "...                             ...        ...                   ...   \n",
       "87261                     -0.415471   1.235694             -0.639532   \n",
       "102882                     2.756092   1.235694             -0.639532   \n",
       "15251                     -0.480197  -0.809262             -0.279805   \n",
       "47685                      1.202673  -0.809262             -0.639532   \n",
       "17342                      2.756092  -0.809262             -0.639532   \n",
       "\n",
       "           count  srv_count  dst_host_rerror_rate  \n",
       "113397 -0.559851  -0.106530              2.548205  \n",
       "135078 -0.544813  -0.248420             -0.602719  \n",
       "106474 -0.097002  -0.120298             -0.387635  \n",
       "110039  0.287250  -0.354343             -0.387635  \n",
       "112503 -0.708312  -0.078996             -0.387635  \n",
       "...          ...        ...                   ...  \n",
       "87261  -0.647181  -0.216669             -0.387635  \n",
       "102882 -0.717045  -0.354343             -0.387635  \n",
       "15251   3.710587  -0.368110              2.352482  \n",
       "47685   3.728053   6.653245             -0.387635  \n",
       "17342  -0.725778  -0.134065             -0.387635  \n",
       "\n",
       "[74258 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113397    0\n",
       "135078    0\n",
       "106474    1\n",
       "110039    1\n",
       "112503    0\n",
       "         ..\n",
       "87261     0\n",
       "102882    0\n",
       "15251     2\n",
       "47685     1\n",
       "17342     2\n",
       "Name: Label, Length: 74258, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEVEL 0 - Weak models - Base Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 3)                 27        \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 95\n",
      "Trainable params: 95\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------START of WEAK LEARNERS (BASE MODELS) - STACK 00 -----------------', file = f)\n",
    "\n",
    "#Defining Basemodels\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "\n",
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "\n",
    "#DNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# #Model Parameters\n",
    "# dropout_rate = 0.01\n",
    "# nodes = 70\n",
    "# out_layer = 5\n",
    "# optimizer='adam'\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "# epochs=1\n",
    "# batch_size=2*256\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "dnn = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "dnn.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn.summary()\n",
    "\n",
    "\n",
    "\n",
    "# dnn = Sequential()\n",
    "# dnn.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "# dnn.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "# dnn.add(Dense(5))  # Output layer\n",
    "\n",
    "# dnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # summary of model layers\n",
    "# dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SVM\n",
    "# # Wrap SGDClassifier with MultiOutputClassifier\n",
    "# multi_target_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# multi_target_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.73990035 0.76521681 0.75680043 0.57598815 0.75106053]\n",
      "Mean accuracy: 0.7177932540894639\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.9107191  0.91698088 0.92041476 0.92296815 0.9143492 ]\n",
      "Mean accuracy: 0.9170864168778934\n",
      "---------------------------------------------------------------------------------\n",
      "Training SVM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.85974953 0.86978185 0.86385672 0.86472291 0.86364555]\n",
      "Mean accuracy: 0.8643513111831409\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.97333692 0.97326959 0.97225963 0.97299845 0.97320046]\n",
      "Mean accuracy: 0.9730130110275945\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.96653649 0.980878   0.98074333 0.97690391 0.92916302]\n",
      "Mean accuracy: 0.9668449510763738\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96397792 0.96613251 0.96404525 0.9643795  0.96128207]\n",
      "Mean accuracy: 0.9639634481762942\n",
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 4.2463 - accuracy: 0.3804 - val_loss: 1.7660 - val_accuracy: 0.5323\n",
      "Epoch 2/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 2.0050 - accuracy: 0.4545 - val_loss: 1.4801 - val_accuracy: 0.5449\n",
      "Epoch 3/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.5880 - accuracy: 0.4725 - val_loss: 1.3617 - val_accuracy: 0.5224\n",
      "Epoch 4/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.5114 - accuracy: 0.4747 - val_loss: 1.3728 - val_accuracy: 0.5225\n",
      "Epoch 5/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4622 - accuracy: 0.4723 - val_loss: 1.4407 - val_accuracy: 0.5241\n",
      "Epoch 6/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4629 - accuracy: 0.4651 - val_loss: 1.3267 - val_accuracy: 0.5245\n",
      "Epoch 7/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4801 - accuracy: 0.4643 - val_loss: 1.3550 - val_accuracy: 0.5430\n",
      "Epoch 8/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4658 - accuracy: 0.4699 - val_loss: 1.3252 - val_accuracy: 0.5151\n",
      "Epoch 9/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4051 - accuracy: 0.4620 - val_loss: 1.3067 - val_accuracy: 0.5149\n",
      "Epoch 10/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.3891 - accuracy: 0.4580 - val_loss: 1.2973 - val_accuracy: 0.5168\n",
      "Epoch 11/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.3927 - accuracy: 0.4442 - val_loss: 1.3032 - val_accuracy: 0.4875\n",
      "Epoch 12/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.3687 - accuracy: 0.4212 - val_loss: 1.2130 - val_accuracy: 0.4801\n"
     ]
    }
   ],
   "source": [
    "#Training Basemodels\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "n_splits = 5  # You can adjust the number of folds as needed\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training Model')\n",
    "with open(output_file_name, \"a\") as f: print('Training weak models - level 0', file = f)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1 and load_model_ada == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_rf == 1 and load_model_rf == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf = rf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_rf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf, 'rf_base_model.joblib')\n",
    "\n",
    "if use_model_svm == 1 and load_model_svm == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    # clf.score(X_train, y_train)\n",
    "    time_taken = end - start\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(clf, 'svm_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_knn == 1 and load_model_knn == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf, 'knn_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_lgbm == 1 and load_model_lgbm == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_base_model.joblib')\n",
    "\n",
    "if use_model_mlp == 1 and load_model_mlp == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_dnn == 1 and load_model_dnn == 0:\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    # Convert Y_test back to its original format\n",
    "    # y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    # dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # End the timer\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(dnn, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    dnn.save(\"DNN_base_model.h5\")\n",
    "\n",
    "    # Calculate the time taken and print it out\n",
    "    # print(f'Time taken for training: {time_taken} seconds')\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Define your Keras model as a function\n",
    "# def create_model(optimizer='adam', hidden_layer_size=16):\n",
    "#     # model = Sequential()\n",
    "#     # model.add(Dense(hidden_layer_size, input_dim=input_size, activation='relu'))\n",
    "#     # model.add(Dense(1, activation='sigmoid'))\n",
    "#     # model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "#     dnn = tf.keras.Sequential()\n",
    "\n",
    "#     # Input layer\n",
    "#     dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "#     # Dense layers with dropout\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "#     dnn.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "#     dnn.summary()\n",
    "#     return dnn\n",
    "\n",
    "# # Create a KerasClassifier\n",
    "# dnn = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam', 'sgd'],\n",
    "#     'hidden_layer_size': [8, 16, 32]\n",
    "# }\n",
    "\n",
    "# # Create the StratifiedKFold\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Create GridSearchCV\n",
    "# grid = GridSearchCV(estimator=dnn, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best accuracy\n",
    "# print(\"Best Parameters: \", grid_result.best_params_)\n",
    "# print(\"Best Accuracy: \", grid_result.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if load_model_ada == 1:\n",
    "    ada = joblib.load('ada_base_model.joblib')\n",
    "\n",
    "if load_model_svm == 1:\n",
    "    clf =  joblib.load('svm_base_model.joblib')\n",
    "\n",
    "if load_model_dnn == 1:\n",
    "    dnn = load_model(\"DNN_base_model.h5\")\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf = joblib.load('knn_base_model.joblib')\n",
    "\n",
    "if load_model_mlp == 1:\n",
    "    MLP = joblib.load('mlp_base_model.joblib')\n",
    "\n",
    "if load_model_rf == 1:\n",
    "    rf = joblib.load('rf_base_model.joblib')\n",
    "\n",
    "if load_model_lgbm == 1:\n",
    "    lgbm = joblib.load('lgbm_base_model.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "# preds_svm = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# y_scores = y_pred\n",
    "# y_true = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base leaners predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction SVM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction DNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction MLP\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction KNN\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "with open(output_file_name, \"a\") as f: print('Generating Predictions', file = f)\n",
    "\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf = rf.predict(X_test)\n",
    "    preds_rf_prob = rf.predict_proba(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    print('Prediction SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "    start = time.time()\n",
    "    preds_svm = clf.predict(X_test)\n",
    "    # preds_svm_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    #Since SVM does not deal with prob by nature we use a meta learner\n",
    "    # https://stackoverflow.com/questions/55250963/how-to-get-probabilities-for-sgdclassifier-linearsvm\n",
    "\n",
    "    model = CalibratedClassifierCV(clf)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm = lgbm.predict(X_test)\n",
    "    preds_lgbm_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    print('Prediction DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #DNN\n",
    "    start = time.time()\n",
    "    pred_dnn = dnn.predict(X_test)\n",
    "    preds_dnn_prob = pred_dnn\n",
    "    preds_dnn = np.argmax(pred_dnn,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada = ada.predict(X_test)\n",
    "    preds_ada_prob = ada.predict_proba(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test)\n",
    "    preds_mlp_prob = y_pred\n",
    "    preds_mlp = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf.predict(X_test)\n",
    "    preds_knn_prob =knn_clf.predict_proba(X_test)\n",
    "\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# model = CalibratedClassifierCV(clf)\n",
    "\n",
    "# model.fit(X, y)\n",
    "# preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "# print(preds_ada_prob)\n",
    "# print(preds_knn_prob)\n",
    "# print(preds_dnn_prob)\n",
    "# print(preds_mlp_prob)\n",
    "# print(preds_rf_prob)\n",
    "# print(preds_svm_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.86755451e-01 9.15808640e-02 7.78774782e-02 4.31012582e-02\n",
      "  6.84948135e-04]\n",
      " [2.62305098e-01 2.71329463e-02 6.45857927e-01 6.23936481e-02\n",
      "  2.31038051e-03]\n",
      " [1.69951428e-02 9.10446456e-01 5.10282199e-02 2.14799179e-02\n",
      "  5.02632551e-05]\n",
      " ...\n",
      " [9.42972082e-01 3.31526500e-02 1.34223706e-03 2.24056309e-02\n",
      "  1.27400343e-04]\n",
      " [9.12741440e-01 3.97164126e-02 1.80198033e-02 2.95111286e-02\n",
      "  1.12159303e-05]\n",
      " [5.57736620e-01 3.72736907e-01 5.55977328e-02 1.35376604e-02\n",
      "  3.91079556e-04]]\n",
      "[0 2 1 ... 0 0 0]\n",
      "[2 0 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(preds_svm_prob)\n",
    "preds_3 = np.argmax(preds_svm_prob,axis = 1)\n",
    "print(preds_3)\n",
    "\n",
    "print(preds_svm)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRICS - Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# >>> \n",
    "# >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "# 0.99...\n",
    "# >>> roc_auc_score(y, clf.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3162994\ttest: 1.3152727\tbest: 1.3152727 (0)\ttotal: 73.6ms\tremaining: 7.29s\n",
      "10:\tlearn: 0.4762946\ttest: 0.4729843\tbest: 0.4729843 (10)\ttotal: 273ms\tremaining: 2.21s\n",
      "20:\tlearn: 0.2779608\ttest: 0.2743050\tbest: 0.2743050 (20)\ttotal: 430ms\tremaining: 1.62s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30:\tlearn: 0.1976391\ttest: 0.1945537\tbest: 0.1945537 (30)\ttotal: 531ms\tremaining: 1.18s\n",
      "40:\tlearn: 0.1608685\ttest: 0.1584695\tbest: 0.1584695 (40)\ttotal: 613ms\tremaining: 883ms\n",
      "50:\tlearn: 0.1386506\ttest: 0.1365561\tbest: 0.1365561 (50)\ttotal: 692ms\tremaining: 665ms\n",
      "60:\tlearn: 0.1253986\ttest: 0.1236924\tbest: 0.1236924 (60)\ttotal: 772ms\tremaining: 494ms\n",
      "70:\tlearn: 0.1154201\ttest: 0.1142025\tbest: 0.1142025 (70)\ttotal: 850ms\tremaining: 347ms\n",
      "80:\tlearn: 0.1086751\ttest: 0.1077649\tbest: 0.1077649 (80)\ttotal: 931ms\tremaining: 218ms\n",
      "90:\tlearn: 0.1029298\ttest: 0.1022821\tbest: 0.1022821 (90)\ttotal: 1.01s\tremaining: 99.7ms\n",
      "99:\tlearn: 0.0978748\ttest: 0.0973233\tbest: 0.0973233 (99)\ttotal: 1.08s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.09732327488\n",
      "bestIteration = 99\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3    4\n",
      "0  37691.0    284.0   288.0   310.0  0.0\n",
      "1    330.0  26179.0   140.0    18.0  0.0\n",
      "2    312.0     47.0  6680.0    22.0  0.0\n",
      "3    348.0     10.0    20.0  1523.0  0.0\n",
      "4     32.0      4.0     0.0    21.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.970562490741863\n",
      "Precision total:  0.7403722100119083\n",
      "Recall total:  0.7412066832896944\n",
      "F1 total:  0.7407826385359696\n",
      "BACC total:  0.7412066832896944\n",
      "MCC total:  0.9502107326058163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "cat_00 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "cat_00.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = cat_00.predict(X_test)\n",
    "preds_cat_prob = cat_00.predict_proba(X_test)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Catboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_cat\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    cat_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    cat_acc_00 = Acc\n",
    "    cat_pre_00 = Precision\n",
    "    cat_rec_00 = Recall\n",
    "    cat_f1_00 = F1\n",
    "    cat_bacc_00 = BACC\n",
    "    cat_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your features and labels as X and y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_00 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb = xgb_00.predict(dtest)\n",
    "# preds_xgb_prob = xgb_00.predict_proba(dtest)\n",
    "\n",
    "\n",
    "# Get class probabilities\n",
    "# Assuming binary classification, get the probability for the positive class (class 1)\n",
    "preds_xgb_margin = xgb_00.predict(dtest, output_margin=True)\n",
    "preds_xgb_prob = 1 / (1 + np.exp(-preds_xgb_margin))\n",
    "\n",
    "# Print or use positive_class_probabilities as needed\n",
    "# print(positive_class_probabilities)\n",
    "\n",
    "\n",
    "# Convert predicted probabilities to class labels (if necessary)\n",
    "# y_pred_labels = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0      1.0     2.0     3.0  4.0\n",
      "0.0  37487.0    361.0   403.0   322.0  0.0\n",
      "1.0    795.0  25689.0   151.0    32.0  0.0\n",
      "2.0    347.0    132.0  6519.0    63.0  0.0\n",
      "3.0    631.0     14.0    31.0  1225.0  0.0\n",
      "4.0     31.0      3.0     2.0    21.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy total:  0.9550357532420313\n",
      "Precision total:  0.7177268205303429\n",
      "Recall total:  0.7005618139133347\n",
      "F1 total:  0.7084949959099308\n",
      "BACC total:  0.7005618139133347\n",
      "MCC total:  0.9237570309178226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    xgb_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    xgb_acc_00 = Acc\n",
    "    xgb_pre_00 = Precision\n",
    "    xgb_rec_00 = Recall\n",
    "    xgb_f1_00 = F1\n",
    "    xgb_bacc_00 = BACC\n",
    "    xgb_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "# pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2     3    4\n",
      "0  38192.0    187.0   191.0   3.0  0.0\n",
      "1   1798.0  24763.0   106.0   0.0  0.0\n",
      "2   1161.0    355.0  5545.0   0.0  0.0\n",
      "3   1641.0     37.0   169.0  54.0  0.0\n",
      "4     48.0      2.0     4.0   3.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9231742953716048\n",
      "Precision total:  0.7380881443061241\n",
      "Recall total:  0.5464858303636617\n",
      "F1 total:  0.5587285632916946\n",
      "BACC total:  0.5464858303636617\n",
      "MCC total:  0.8693646979033856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('RF base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_rf\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    rf_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    rf_acc_00 = Acc\n",
    "    rf_pre_00 = Precision\n",
    "    rf_rec_00 = Recall\n",
    "    rf_f1_00 = F1\n",
    "    rf_bacc_00 = BACC\n",
    "    rf_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1      2    3    4\n",
      "0  34876.0  2961.0  736.0  0.0  0.0\n",
      "1  21282.0  5366.0   19.0  0.0  0.0\n",
      "2   4503.0  2437.0  121.0  0.0  0.0\n",
      "3   1331.0   464.0  106.0  0.0  0.0\n",
      "4     33.0    12.0   12.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.5435435435435435\n",
      "Precision total:  0.23228438339215812\n",
      "Recall total:  0.224502924856971\n",
      "F1 total:  0.2013062027461246\n",
      "BACC total:  0.224502924856971\n",
      "MCC total:  0.1329918769284351\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('DNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_dnn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    dnn_acc_00 = Acc\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    dnn_acc_00 = Acc\n",
    "    dnn_pre_00 = Precision\n",
    "    dnn_rec_00 = Recall\n",
    "    dnn_f1_00 = F1\n",
    "    dnn_bacc_00 = BACC\n",
    "    dnn_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2      3     4\n",
      "0  30527.0   5357.0  1791.0  882.0  16.0\n",
      "1    708.0  19208.0  6530.0  219.0   2.0\n",
      "2    775.0    441.0  5739.0  106.0   0.0\n",
      "3    963.0     48.0    14.0  864.0  12.0\n",
      "4     25.0      6.0     1.0   23.0   2.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.7586959156465883\n",
      "Precision total:  0.5148897595750748\n",
      "Recall total:  0.5628118482439187\n",
      "F1 total:  0.5232524495141136\n",
      "BACC total:  0.5628118482439187\n",
      "MCC total:  0.6247111151222441\n"
     ]
    }
   ],
   "source": [
    "#ADA\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    ada_acc_00 = Acc\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "\n",
    "    ada_acc_00 = Acc\n",
    "    ada_pre_00 = Precision\n",
    "    ada_rec_00 = Recall\n",
    "    ada_f1_00 = F1\n",
    "    ada_bacc_00 = BACC\n",
    "    ada_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2    3    4\n",
      "0  36019.0    910.0  1635.0  9.0  0.0\n",
      "1   2131.0  24339.0   197.0  0.0  0.0\n",
      "2   1231.0   1869.0  3961.0  0.0  0.0\n",
      "3   1622.0     82.0   197.0  0.0  0.0\n",
      "4     42.0      7.0     8.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8661441710769066\n",
      "Precision total:  0.4865043558587877\n",
      "Recall total:  0.48149153505386855\n",
      "F1 total:  0.4829959148201005\n",
      "BACC total:  0.48149153505386855\n",
      "MCC total:  0.768422624124082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('SVM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_svm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    svm_acc_00 = Acc\n",
    "    svm_pre_00 = Precision\n",
    "    svm_rec_00 = Recall\n",
    "    svm_f1_00 = F1\n",
    "    svm_bacc_00 = BACC\n",
    "    svm_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3    4\n",
      "0  37684.0    306.0   245.0   330.0  8.0\n",
      "1    215.0  26399.0    26.0    27.0  0.0\n",
      "2    300.0     44.0  6683.0    34.0  0.0\n",
      "3    267.0      6.0    22.0  1606.0  0.0\n",
      "4     36.0      3.0     0.0    16.0  2.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.974615871476858\n",
      "Precision total:  0.784230172592528\n",
      "Recall total:  0.7586551316929457\n",
      "F1 total:  0.7597319520860626\n",
      "BACC total:  0.7586551316929457\n",
      "MCC total:  0.957101590350768\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "\n",
    "    knn_acc_00 = Acc\n",
    "    knn_pre_00 = Precision\n",
    "    knn_rec_00 = Recall\n",
    "    knn_f1_00 = F1\n",
    "    knn_bacc_00 = BACC\n",
    "    knn_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3    4\n",
      "0  37658.0    346.0   270.0   299.0  0.0\n",
      "1    351.0  26196.0    90.0    30.0  0.0\n",
      "2    473.0     60.0  6504.0    24.0  0.0\n",
      "3    429.0     34.0    38.0  1400.0  0.0\n",
      "4     37.0      2.0     1.0    16.0  1.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9663340470515358\n",
      "Precision total:  0.9367785393943964\n",
      "Recall total:  0.72674616229928\n",
      "F1 total:  0.7366784048823265\n",
      "BACC total:  0.72674616229928\n",
      "MCC total:  0.9429046243180847\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    mlp_acc_00 = Acc\n",
    "    mlp_pre_00 = Precision\n",
    "    mlp_rec_00 = Recall\n",
    "    mlp_f1_00 = F1\n",
    "    mlp_bacc_00 = BACC\n",
    "    mlp_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  37851.0    220.0   181.0   274.0  47.0\n",
      "1    149.0  26468.0    24.0    18.0   8.0\n",
      "2    172.0     19.0  6841.0    22.0   7.0\n",
      "3    195.0      1.0    13.0  1682.0  10.0\n",
      "4     34.0      0.0     0.0    16.0   7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9810124025370662\n",
      "Precision total:  0.7740801027644095\n",
      "Recall total:  0.7900534539974579\n",
      "F1 total:  0.7813739805540139\n",
      "BACC total:  0.7900534539974579\n",
      "MCC total:  0.9679610450939468\n"
     ]
    }
   ],
   "source": [
    "#lgbm\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_lgbm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    lgbm_acc_00 = Acc\n",
    "    lgbm_pre_00 = Precision\n",
    "    lgbm_rec_00 = Recall\n",
    "    lgbm_f1_00 = F1\n",
    "    lgbm_bacc_00 = BACC\n",
    "    lgbm_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the stronger model - STACK level 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74259 74259\n"
     ]
    }
   ],
   "source": [
    "print(len(preds_dnn_prob), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123264    2\n",
      "56743     0\n",
      "88220     1\n",
      "36372     1\n",
      "109357    1\n",
      "         ..\n",
      "117763    1\n",
      "25489     0\n",
      "89481     1\n",
      "13032     0\n",
      "87389     0\n",
      "Name: Label, Length: 74259, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        index  Label\n",
      "0      123264      2\n",
      "1       56743      0\n",
      "2       88220      1\n",
      "3       36372      1\n",
      "4      109357      1\n",
      "...       ...    ...\n",
      "74254  117763      1\n",
      "74255   25489      0\n",
      "74256   89481      1\n",
      "74257   13032      0\n",
      "74258   87389      0\n",
      "\n",
      "[74259 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        123264\n",
       "1         56743\n",
       "2         88220\n",
       "3         36372\n",
       "4        109357\n",
       "          ...  \n",
       "74254    117763\n",
       "74255     25489\n",
       "74256     89481\n",
       "74257     13032\n",
       "74258     87389\n",
       "Name: index, Length: 74259, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_series = y_test.to_frame()\n",
    "y_test_reset_index = df_from_series.reset_index()\n",
    "# y_test2 = y_test.reset_index(inplace=True)\n",
    "print(y_test_reset_index)\n",
    "y_test_reset_index.pop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_reset_index.values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dnn_2 = []\n",
    "preds_svm_2 = []\n",
    "preds_rf_2 = []\n",
    "preds_mlp_2 = []\n",
    "preds_ada_2 = []\n",
    "preds_knn_2 = []\n",
    "preds_lgbm_2 = []\n",
    "preds_cat_2 = []\n",
    "preds_xgb_2 = []\n",
    "\n",
    "for i in range(0,len(preds_dnn_prob)):  \n",
    "    # print(i)\n",
    "    # print(preds_dnn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_dnn_2.append(preds_dnn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_svm_2.append(preds_svm_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_rf_2.append(preds_rf_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_mlp_2.append(preds_mlp_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_ada_2.append(preds_ada_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_knn_2.append(preds_knn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_lgbm_2.append(preds_lgbm_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_cat_2.append(preds_cat_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_xgb_2.append(preds_xgb_prob[i][y_test_reset_index.values[i][0]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05081991 0.79070027 0.99919391 ... 0.99399468 0.96087444 2.        ]\n",
      " [0.20403558 0.98587062 0.99986679 ... 0.99796718 0.99363387 0.        ]\n",
      " [0.18833527 0.99344781 0.9999872  ... 0.99824577 0.99533552 1.        ]\n",
      " ...\n",
      " [0.01682695 0.9315379  0.99981855 ... 0.99506191 0.98959243 1.        ]\n",
      " [0.29020634 0.88943441 0.99942833 ... 0.98743547 0.93419242 0.        ]\n",
      " [0.18264192 0.98510389 0.99979137 ... 0.99588572 0.99194461 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('------------START of STRONGER LEARNER - STACK 01 -----------------', file = f)\n",
    "\n",
    "\n",
    "# Stack the vectors horizontally to create a matrix\n",
    "column_features = ['dnn','rf','lgbm','ada','knn','mlp','svm','cat','xgb','label']\n",
    "training_matrix = np.column_stack((\n",
    "                          preds_dnn_2,\n",
    "                          preds_rf_2,\n",
    "                          preds_lgbm_2,\n",
    "                          preds_ada_2,\n",
    "                          preds_knn_2, \n",
    "                          preds_mlp_2,\n",
    "                          preds_svm_2,\n",
    "                          preds_cat_2,\n",
    "                          preds_xgb_2,\n",
    "                          y_test\n",
    "                          ))\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_level_01 = pd.DataFrame(training_matrix, columns=column_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_level_01.to_csv('models7dataset_prob.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_01 = df_level_01.pop('label')\n",
    "X_01 = df_level_01\n",
    "df_level_01 = df_level_01.assign(label = y_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050820</td>\n",
       "      <td>0.790700</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.319236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.077877</td>\n",
       "      <td>0.993995</td>\n",
       "      <td>0.960874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204036</td>\n",
       "      <td>0.985871</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.263216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.262305</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.993634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188335</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.269991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910446</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>0.995336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.243874</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.995455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.938978</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.997787</td>\n",
       "      <td>0.981621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.263535</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>0.998767</td>\n",
       "      <td>0.995455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>0.172091</td>\n",
       "      <td>0.822487</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.236307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.973484</td>\n",
       "      <td>0.945671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.245178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.989592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.290206</td>\n",
       "      <td>0.889434</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902577</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.934192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.182642</td>\n",
       "      <td>0.985104</td>\n",
       "      <td>0.999791</td>\n",
       "      <td>0.271456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.790693</td>\n",
       "      <td>0.995886</td>\n",
       "      <td>0.991945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dnn        rf      lgbm       ada  knn       mlp       svm  \\\n",
       "0      0.050820  0.790700  0.999194  0.319236  1.0  0.999794  0.077877   \n",
       "1      0.204036  0.985871  0.999867  0.263216  1.0  0.999937  0.262305   \n",
       "2      0.188335  0.993448  0.999987  0.269991  1.0  1.000000  0.910446   \n",
       "3      0.243874  0.993448  0.999994  0.268088  1.0  0.999999  0.017458   \n",
       "4      0.236328  0.938978  0.999887  0.268088  1.0  1.000000  0.022950   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "74254  0.263535  0.993448  0.999996  0.268088  1.0  0.999997  0.014842   \n",
       "74255  0.172091  0.822487  0.995918  0.236307  1.0  0.999989  0.007036   \n",
       "74256  0.016827  0.931538  0.999819  0.245178  1.0  0.999938  0.913073   \n",
       "74257  0.290206  0.889434  0.999428  0.243277  1.0  1.000000  0.902577   \n",
       "74258  0.182642  0.985104  0.999791  0.271456  1.0  0.999964  0.790693   \n",
       "\n",
       "            cat       xgb  \n",
       "0      0.993995  0.960874  \n",
       "1      0.997967  0.993634  \n",
       "2      0.998246  0.995336  \n",
       "3      0.998620  0.995455  \n",
       "4      0.997787  0.981621  \n",
       "...         ...       ...  \n",
       "74254  0.998767  0.995455  \n",
       "74255  0.973484  0.945671  \n",
       "74256  0.995062  0.989592  \n",
       "74257  0.987435  0.934192  \n",
       "74258  0.995886  0.991945  \n",
       "\n",
       "[74259 rows x 9 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2.0\n",
       "1        0.0\n",
       "2        1.0\n",
       "3        1.0\n",
       "4        1.0\n",
       "        ... \n",
       "74254    1.0\n",
       "74255    0.0\n",
       "74256    1.0\n",
       "74257    0.0\n",
       "74258    0.0\n",
       "Name: label, Length: 74259, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.050820</td>\n",
       "      <td>0.790700</td>\n",
       "      <td>0.999194</td>\n",
       "      <td>0.319236</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999794</td>\n",
       "      <td>0.077877</td>\n",
       "      <td>0.993995</td>\n",
       "      <td>0.960874</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.204036</td>\n",
       "      <td>0.985871</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.263216</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999937</td>\n",
       "      <td>0.262305</td>\n",
       "      <td>0.997967</td>\n",
       "      <td>0.993634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.188335</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.269991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910446</td>\n",
       "      <td>0.998246</td>\n",
       "      <td>0.995336</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.243874</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.017458</td>\n",
       "      <td>0.998620</td>\n",
       "      <td>0.995455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.236328</td>\n",
       "      <td>0.938978</td>\n",
       "      <td>0.999887</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.997787</td>\n",
       "      <td>0.981621</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.263535</td>\n",
       "      <td>0.993448</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.268088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>0.998767</td>\n",
       "      <td>0.995455</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>0.172091</td>\n",
       "      <td>0.822487</td>\n",
       "      <td>0.995918</td>\n",
       "      <td>0.236307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999989</td>\n",
       "      <td>0.007036</td>\n",
       "      <td>0.973484</td>\n",
       "      <td>0.945671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.016827</td>\n",
       "      <td>0.931538</td>\n",
       "      <td>0.999819</td>\n",
       "      <td>0.245178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999938</td>\n",
       "      <td>0.913073</td>\n",
       "      <td>0.995062</td>\n",
       "      <td>0.989592</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.290206</td>\n",
       "      <td>0.889434</td>\n",
       "      <td>0.999428</td>\n",
       "      <td>0.243277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.902577</td>\n",
       "      <td>0.987435</td>\n",
       "      <td>0.934192</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.182642</td>\n",
       "      <td>0.985104</td>\n",
       "      <td>0.999791</td>\n",
       "      <td>0.271456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999964</td>\n",
       "      <td>0.790693</td>\n",
       "      <td>0.995886</td>\n",
       "      <td>0.991945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dnn        rf      lgbm       ada  knn       mlp       svm  \\\n",
       "0      0.050820  0.790700  0.999194  0.319236  1.0  0.999794  0.077877   \n",
       "1      0.204036  0.985871  0.999867  0.263216  1.0  0.999937  0.262305   \n",
       "2      0.188335  0.993448  0.999987  0.269991  1.0  1.000000  0.910446   \n",
       "3      0.243874  0.993448  0.999994  0.268088  1.0  0.999999  0.017458   \n",
       "4      0.236328  0.938978  0.999887  0.268088  1.0  1.000000  0.022950   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "74254  0.263535  0.993448  0.999996  0.268088  1.0  0.999997  0.014842   \n",
       "74255  0.172091  0.822487  0.995918  0.236307  1.0  0.999989  0.007036   \n",
       "74256  0.016827  0.931538  0.999819  0.245178  1.0  0.999938  0.913073   \n",
       "74257  0.290206  0.889434  0.999428  0.243277  1.0  1.000000  0.902577   \n",
       "74258  0.182642  0.985104  0.999791  0.271456  1.0  0.999964  0.790693   \n",
       "\n",
       "            cat       xgb  label  \n",
       "0      0.993995  0.960874    2.0  \n",
       "1      0.997967  0.993634    0.0  \n",
       "2      0.998246  0.995336    1.0  \n",
       "3      0.998620  0.995455    1.0  \n",
       "4      0.997787  0.981621    1.0  \n",
       "...         ...       ...    ...  \n",
       "74254  0.998767  0.995455    1.0  \n",
       "74255  0.973484  0.945671    0.0  \n",
       "74256  0.995062  0.989592    1.0  \n",
       "74257  0.987435  0.934192    0.0  \n",
       "74258  0.995886  0.991945    0.0  \n",
       "\n",
       "[74259 rows x 10 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "\n",
    "X_train_01,X_test_01, y_train_01, y_test_01 = sklearn.model_selection.train_test_split(X_01, y_01, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define EarlyStopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Compile the model\n",
    "# # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # Train the model with EarlyStopping callback\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# # Save the trained model\n",
    "# # model.save(\"CNN_CIC_1.h5\")\n",
    "# model = load_model(\"CNN_CIC_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 3)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train_01.shape[1]\n",
    "\n",
    "dnn_01 = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# # Dense layers with dropout\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(3*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "# dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "\n",
    "\n",
    "dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn_01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "307/325 [===========================>..] - ETA: 0s - loss: 1.2898 - accuracy: 0.4602"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 1s 3ms/step - loss: 1.2808 - accuracy: 0.4624 - val_loss: 1.0339 - val_accuracy: 0.5224\n",
      "Epoch 2/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.0643 - accuracy: 0.5080 - val_loss: 0.9619 - val_accuracy: 0.5225\n",
      "Epoch 3/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.9707 - accuracy: 0.5183 - val_loss: 0.8783 - val_accuracy: 0.5230\n",
      "Epoch 4/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.9027 - accuracy: 0.5300 - val_loss: 0.8202 - val_accuracy: 0.5515\n",
      "Epoch 5/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8715 - accuracy: 0.5415 - val_loss: 0.7960 - val_accuracy: 0.5625\n",
      "Epoch 6/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8513 - accuracy: 0.5493 - val_loss: 0.7780 - val_accuracy: 0.5695\n",
      "Epoch 7/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8362 - accuracy: 0.5601 - val_loss: 0.7677 - val_accuracy: 0.5844\n",
      "Epoch 8/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8224 - accuracy: 0.5702 - val_loss: 0.7570 - val_accuracy: 0.6029\n",
      "Epoch 9/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8099 - accuracy: 0.5744 - val_loss: 0.7525 - val_accuracy: 0.6065\n",
      "Epoch 10/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8081 - accuracy: 0.5763 - val_loss: 0.7507 - val_accuracy: 0.6035\n",
      "Epoch 11/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8030 - accuracy: 0.5771 - val_loss: 0.7485 - val_accuracy: 0.6043\n",
      "Epoch 12/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8004 - accuracy: 0.5766 - val_loss: 0.7475 - val_accuracy: 0.6037\n",
      "Epoch 13/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7961 - accuracy: 0.5791 - val_loss: 0.7419 - val_accuracy: 0.6078\n",
      "Epoch 14/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7963 - accuracy: 0.5796 - val_loss: 0.7412 - val_accuracy: 0.6072\n",
      "Epoch 15/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7913 - accuracy: 0.5802 - val_loss: 0.7404 - val_accuracy: 0.6061\n",
      "Epoch 16/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7921 - accuracy: 0.5796 - val_loss: 0.7448 - val_accuracy: 0.6026\n",
      "Epoch 17/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7925 - accuracy: 0.5789 - val_loss: 0.7383 - val_accuracy: 0.6065\n",
      "Epoch 18/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7916 - accuracy: 0.5808 - val_loss: 0.7418 - val_accuracy: 0.6033\n",
      "Epoch 19/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7862 - accuracy: 0.5811 - val_loss: 0.7382 - val_accuracy: 0.6062\n",
      "Epoch 20/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7858 - accuracy: 0.5812 - val_loss: 0.7374 - val_accuracy: 0.6062\n",
      "Epoch 21/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7883 - accuracy: 0.5802 - val_loss: 0.7354 - val_accuracy: 0.6058\n",
      "Epoch 22/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7836 - accuracy: 0.5813 - val_loss: 0.7338 - val_accuracy: 0.6066\n",
      "Epoch 23/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7847 - accuracy: 0.5817 - val_loss: 0.7340 - val_accuracy: 0.6118\n",
      "Epoch 24/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7803 - accuracy: 0.5840 - val_loss: 0.7350 - val_accuracy: 0.6055\n",
      "Epoch 25/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7816 - accuracy: 0.5838 - val_loss: 0.7328 - val_accuracy: 0.6119\n",
      "Epoch 26/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7813 - accuracy: 0.5837 - val_loss: 0.7362 - val_accuracy: 0.6052\n",
      "Epoch 27/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7800 - accuracy: 0.5840 - val_loss: 0.7373 - val_accuracy: 0.6056\n",
      "Epoch 28/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7801 - accuracy: 0.5842 - val_loss: 0.7304 - val_accuracy: 0.6142\n",
      "Epoch 29/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7800 - accuracy: 0.5860 - val_loss: 0.7367 - val_accuracy: 0.6045\n",
      "Epoch 30/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7806 - accuracy: 0.5848 - val_loss: 0.7303 - val_accuracy: 0.6128\n",
      "Epoch 31/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7757 - accuracy: 0.5870 - val_loss: 0.7334 - val_accuracy: 0.6054\n",
      "Epoch 32/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7768 - accuracy: 0.5860 - val_loss: 0.7339 - val_accuracy: 0.6052\n",
      "Epoch 33/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7757 - accuracy: 0.5861 - val_loss: 0.7302 - val_accuracy: 0.6124\n",
      "Epoch 34/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7763 - accuracy: 0.5886 - val_loss: 0.7301 - val_accuracy: 0.6119\n",
      "Epoch 35/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7756 - accuracy: 0.5912 - val_loss: 0.7340 - val_accuracy: 0.6091\n",
      "Epoch 36/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7777 - accuracy: 0.5903 - val_loss: 0.7323 - val_accuracy: 0.6093\n",
      "Epoch 37/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7766 - accuracy: 0.5896 - val_loss: 0.7302 - val_accuracy: 0.6123\n",
      "Epoch 38/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7751 - accuracy: 0.5916 - val_loss: 0.7310 - val_accuracy: 0.6127\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training DNN')\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# Convert Y_test back to its original format\n",
    "# y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Start the timer\n",
    "start = time.time()\n",
    "# dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size)\n",
    "dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# End the timer\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "# joblib.dump(dnn_01, 'dnn_level_01.joblib')\n",
    "dnn_01.save(\"dnn_level_01.h5\")\n",
    "\n",
    "# Calculate the time taken and print it out\n",
    "# print(f'Time taken for training: {time_taken} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_01 = load_model(\"dnn_level_01.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "start = time.time()\n",
    "pred_dnn = dnn_01.predict(X_test_01)\n",
    "preds_dnn_01 = np.argmax(pred_dnn,axis = 1)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_test_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "with open(output_file_name, \"a\") as f: print('Stack model - Strong learner - level 01', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0    2.0  3.0  4.0\n",
      "0.0  11243.0   308.0    0.0  0.0  0.0\n",
      "1.0   7711.0   306.0    1.0  0.0  0.0\n",
      "2.0    298.0  1814.0    3.0  0.0  0.0\n",
      "3.0      0.0    58.0  514.0  0.0  0.0\n",
      "4.0      0.0    10.0   12.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.6091659933566748\n",
      "Precision total:  0.4561130829802892\n",
      "Recall total:  0.5459240511334371\n",
      "F1 total:  0.48993146733213655\n",
      "BACC total:  0.5459240511334371\n",
      "MCC total:  0.39749208372924244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('DNN', file = f)\n",
    "pred_label = preds_dnn_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "dnn_acc_01 = Acc\n",
    "dnn_pre_01 = Precision\n",
    "dnn_rec_01 = Recall\n",
    "dnn_f1_01 = F1\n",
    "dnn_bacc_01 = BACC\n",
    "dnn_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "clf.fit(X_train_01, y_train_01)\n",
    "end = time.time()\n",
    "clf.score(X_train_01, y_train_01)\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "joblib.dump(clf, 'svm_level_01.joblib')\n",
    "\n",
    "\n",
    "clf = loaded_model = joblib.load('svm_level_01.joblib')\n",
    "\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "preds_svm_01 = clf.predict(X_test_01)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  8617.0  2617.0   313.0    4.0  0.0\n",
      "1.0  4902.0  2623.0   488.0    5.0  0.0\n",
      "2.0   229.0    83.0  1778.0   25.0  0.0\n",
      "3.0     0.0     0.0    72.0  500.0  0.0\n",
      "4.0     3.0     0.0     0.0   19.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.6067869647185564\n",
      "Precision total:  0.538852401791561\n",
      "Recall total:  0.5575845535423665\n",
      "F1 total:  0.5418726099415194\n",
      "BACC total:  0.5575845535423665\n",
      "MCC total:  0.33178215306482806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('SVM', file = f)\n",
    "pred_label = preds_svm_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "\n",
    "svm_acc_01 = Acc\n",
    "svm_pre_01 = Precision\n",
    "svm_rec_01 = Recall\n",
    "svm_f1_01 = F1\n",
    "svm_bacc_01 = BACC\n",
    "svm_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11338.0    74.0   139.0    0.0  0.0\n",
      "1.0    722.0  7090.0   206.0    0.0  0.0\n",
      "2.0    401.0   151.0  1563.0    0.0  0.0\n",
      "3.0      0.0     0.0     0.0  572.0  0.0\n",
      "4.0      0.0     0.0     0.0   13.0  9.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9234222102522668\n",
      "Precision total:  0.9352160549340095\n",
      "Recall total:  0.8027836906899554\n",
      "F1 total:  0.8431210275432454\n",
      "BACC total:  0.8027836906899554\n",
      "MCC total:  0.8708415232118788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if True == True:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf_01 = rf.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(model_rf_01, X_train_01, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf_01, 'rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    model_rf_01  = joblib.load('rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf_01 = model_rf_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('RF', file = f)\n",
    "pred_label = preds_rf_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "rf_acc_01 = Acc\n",
    "rf_pre_01 = Precision\n",
    "rf_rec_01 = Recall\n",
    "rf_f1_01 = F1\n",
    "rf_bacc_01 = BACC\n",
    "rf_mcc_01 = MCC\n",
    "\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9234222102522668"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  11518.0    20.0    13.0    0.0   0.0\n",
      "1.0     30.0  7976.0    12.0    0.0   0.0\n",
      "2.0      3.0    12.0  2100.0    0.0   0.0\n",
      "3.0      0.0     0.0     0.0  572.0   0.0\n",
      "4.0      0.0     0.0     0.0    0.0  22.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9959601400484783\n",
      "Precision total:  0.9962764789229521\n",
      "Recall total:  0.9969625383786198\n",
      "F1 total:  0.996618329132178\n",
      "BACC total:  0.9969625383786198\n",
      "MCC total:  0.9931771619500335\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    lgbm = joblib.load('lgbm_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm_01 = lgbm.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM', file = f)\n",
    "    pred_label = preds_lgbm_01\n",
    "\n",
    "    # pred_label = ypred\n",
    "    #pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    lgbm_acc_01 = Acc\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    lgbm_acc_01 = Acc\n",
    "    lgbm_pre_01 = Precision\n",
    "    lgbm_rec_01 = Recall\n",
    "    lgbm_f1_01 = F1\n",
    "    lgbm_bacc_01 = BACC\n",
    "    lgbm_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1       2      3     4\n",
      "0  10937.0   536.0    78.0    0.0   0.0\n",
      "1   1353.0  6507.0   158.0    0.0   0.0\n",
      "2     51.0    62.0  2002.0    0.0   0.0\n",
      "3      0.0     0.0     0.0  572.0   0.0\n",
      "4      0.0     0.0     0.0    0.0  22.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8995421492054942\n",
      "Precision total:  0.9393231012474725\n",
      "Recall total:  0.9409931095579122\n",
      "F1 total:  0.9391811063990051\n",
      "BACC total:  0.9409931095579122\n",
      "MCC total:  0.8312311815290724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    MLP = joblib.load('mlp_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test_01)\n",
    "    preds_mlp_01 = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    mlp_acc_01 = Acc\n",
    "    mlp_pre_01 = Precision\n",
    "    mlp_rec_01 = Recall\n",
    "    mlp_f1_01 = F1\n",
    "    mlp_bacc_01 = BACC\n",
    "    mlp_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0    1.0   2.0    3.0  4.0\n",
      "0.0  11530.0    7.0  14.0    0.0  0.0\n",
      "1.0   7858.0  144.0  16.0    0.0  0.0\n",
      "2.0   2041.0   32.0  42.0    0.0  0.0\n",
      "3.0      0.0    0.0   0.0  572.0  0.0\n",
      "4.0      0.0    0.0   0.0   22.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.5515755453810934\n",
      "Precision total:  0.5742474895492983\n",
      "Recall total:  0.4071999445070654\n",
      "F1 total:  0.3507740333236951\n",
      "BACC total:  0.4071999445070654\n",
      "MCC total:  0.2336449609932628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_01.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    ada = joblib.load('ada_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada_01 = ada.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    ada_acc_01 = Acc\n",
    "    ada_pre_01 = Precision\n",
    "    ada_rec_01 = Recall\n",
    "    ada_f1_01 = F1\n",
    "    ada_bacc_01 = BACC\n",
    "    ada_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  11281.0   192.0    78.0    0.0   0.0\n",
      "1.0    298.0  7669.0    49.0    2.0   0.0\n",
      "2.0     37.0    52.0  2014.0   12.0   0.0\n",
      "3.0      0.0     3.0     3.0  566.0   0.0\n",
      "4.0      0.0     0.0     2.0   10.0  10.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9668731483975223\n",
      "Precision total:  0.9675540177960912\n",
      "Recall total:  0.8658800286465154\n",
      "F1 total:  0.8961962839323581\n",
      "BACC total:  0.8658800286465154\n",
      "MCC total:  0.9440627196990393\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf_01, 'knn_01.joblib')\n",
    "\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf_01 = joblib.load('knn_01.joblib')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf_01.predict(X_test_01)\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    knn_acc_01 = Acc\n",
    "    knn_pre_01 = Precision\n",
    "    knn_rec_01 = Recall\n",
    "    knn_f1_01 = F1\n",
    "    knn_bacc_01 = BACC\n",
    "    knn_mcc_01 = MCC    \n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Logistic Regression Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LR \n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0   4.0\n",
      "0.0  8849.0  2460.0   242.0    0.0   0.0\n",
      "1.0  4823.0  2842.0   351.0    0.0   2.0\n",
      "2.0   230.0    54.0  1818.0    2.0  11.0\n",
      "3.0     0.0     0.0     0.0  571.0   1.0\n",
      "4.0     0.0     0.0     0.0   16.0   6.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.6322829697459377\n",
      "Precision total:  0.6381261354968755\n",
      "Recall total:  0.6502173659559842\n",
      "F1 total:  0.6386063966146803\n",
      "BACC total:  0.6502173659559842\n",
      "MCC total:  0.3735069829432364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Logistic Regression Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "logreg_01 = LogisticRegression()\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LR ')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LR', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    logreg_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(logreg_01, 'logreg_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    logreg_01 = joblib.load('logreg_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #lR\n",
    "    start = time.time()\n",
    "    preds_logreg =logreg_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#LR\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LR 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_logreg\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    lr_acc_01 = Acc\n",
    "    lr_pre_01 = Precision\n",
    "    lr_rec_01 = Recall\n",
    "    lr_f1_01 = F1\n",
    "    lr_bacc_01 = BACC\n",
    "    lr_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Voting\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# # model1 = LogisticRegression(random_state=1)\n",
    "# # model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "# voting = VotingClassifier(estimators=[\n",
    "#                                         ('ada', ada),\n",
    "#                                        ('rf', rf),\n",
    "#                                        ('svm', clf),\n",
    "#                                        ('knn', knn_clf), \n",
    "#                                        ('lgbm', lgbm),\n",
    "#                                       #  ('xgb', xgb_00),\n",
    "#                                        ('cat', cat_00),\n",
    "\n",
    "#                                          ('mlp', mlp)\n",
    "#                                         #  ,('dnn', dnn_01)\n",
    "\n",
    "#                                          ], voting='hard')\n",
    "# voting.fit(X_train_01,y_train_01)\n",
    "# # voring_acc = voting.score(X_test_01,y_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_voting = voting(X_test_01,y_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('CONFUSION MATRIX')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = preds_voting\n",
    "# # pred_label = label[ypred]\n",
    "\n",
    "# confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "# all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "# z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "# rows, cols = confusion_matrix.shape\n",
    "# z[:rows, :cols] = confusion_matrix\n",
    "# confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "# print(confusion_matrix)\n",
    "# with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "# FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "# FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "# TP = np.diag(confusion_matrix)\n",
    "# TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "# TP_total = sum(TP)\n",
    "# TN_total = sum(TN)\n",
    "# FP_total = sum(FP)\n",
    "# FN_total = sum(FN)\n",
    "\n",
    "# TP_total = np.array(TP_total,dtype=np.float64)\n",
    "# TN_total = np.array(TN_total,dtype=np.float64)\n",
    "# FP_total = np.array(FP_total,dtype=np.float64)\n",
    "# FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "# #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('METRICS')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# # Precision = PRECISION(TP_total, FP_total)\n",
    "# # Recall = RECALL(TP_total, FN_total)\n",
    "# # F1 = F1(Recall,Precision)\n",
    "# # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "# Acc = accuracy_score(y_test_01, pred_label)\n",
    "# Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "# Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "# F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "# BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "# MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "# voting_acc_01 = Acc\n",
    "# voting_pre_01 = Precision\n",
    "# voting_rec_01 = Recall\n",
    "# voting_f1_01 = F1\n",
    "# voting_bacc_01 = BACC\n",
    "# voting_mcc_01 = MCC\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "# print('Accuracy total: ', Acc)\n",
    "# print('Precision total: ', Precision )\n",
    "# print('Recall total: ', Recall )\n",
    "# print('F1 total: ', F1 )\n",
    "# print('BACC total: ', BACC)\n",
    "# print('MCC total: ', MCC)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# with open(output_file_name, \"a\") as f: print('Generating Predictions', file = f)\n",
    "\n",
    "# if use_model_rf == 1:\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction RF')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #RF\n",
    "#     start = time.time()\n",
    "#     preds_rf = rf.predict(X_test)\n",
    "#     preds_rf_prob = rf.predict_proba(X_test)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_svm == 1:\n",
    "\n",
    "#     print('Prediction SVM')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction SVM', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #SVM\n",
    "#     start = time.time()\n",
    "#     preds_svm = clf.predict(X_test)\n",
    "#     # preds_svm_prob = clf.predict_proba(X_test)\n",
    "\n",
    "#     #Since SVM does not deal with prob by nature we use a meta learner\n",
    "#     # https://stackoverflow.com/questions/55250963/how-to-get-probabilities-for-sgdclassifier-linearsvm\n",
    "\n",
    "#     model = CalibratedClassifierCV(clf)\n",
    "\n",
    "#     model.fit(X, y)\n",
    "#     preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_lgbm == 1:\n",
    "\n",
    "#     print('Prediction LGBM')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #LGBM\n",
    "#     start = time.time()\n",
    "#     preds_lgbm = lgbm.predict(X_test)\n",
    "#     preds_lgbm_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_dnn == 1:\n",
    "\n",
    "#     print('Prediction DNN')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction DNN', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #DNN\n",
    "#     start = time.time()\n",
    "#     pred_dnn = dnn.predict(X_test)\n",
    "#     preds_dnn_prob = pred_dnn\n",
    "#     preds_dnn = np.argmax(pred_dnn,axis = 1)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_ada == 1:\n",
    "\n",
    "#     print('Prediction ADA')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #ADA\n",
    "#     start = time.time()\n",
    "#     preds_ada = ada.predict(X_test)\n",
    "#     preds_ada_prob = ada.predict_proba(X_test)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction MLP')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction MLP', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_mlp == 1:\n",
    "\n",
    "#     #MLP\n",
    "#     start = time.time()\n",
    "#     y_pred = MLP.predict_proba(X_test)\n",
    "#     preds_mlp_prob = y_pred\n",
    "#     preds_mlp = np.argmax(y_pred,axis = 1)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction KNN')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction KNN', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_knn == 1:\n",
    "\n",
    "#     #KNN\n",
    "#     start = time.time()\n",
    "#     preds_knn =knn_clf.predict(X_test)\n",
    "#     preds_knn_prob =knn_clf.predict_proba(X_test)\n",
    "\n",
    "#     preds_knn\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3260412\ttest: 1.3261579\tbest: 1.3261579 (0)\ttotal: 21.1ms\tremaining: 2.09s\n",
      "10:\tlearn: 0.4784802\ttest: 0.4785154\tbest: 0.4785154 (10)\ttotal: 188ms\tremaining: 1.52s\n",
      "20:\tlearn: 0.2740512\ttest: 0.2751733\tbest: 0.2751733 (20)\ttotal: 295ms\tremaining: 1.11s\n",
      "30:\tlearn: 0.1901725\ttest: 0.1919907\tbest: 0.1919907 (30)\ttotal: 379ms\tremaining: 844ms\n",
      "40:\tlearn: 0.1487944\ttest: 0.1506366\tbest: 0.1506366 (40)\ttotal: 463ms\tremaining: 666ms\n",
      "50:\tlearn: 0.1235563\ttest: 0.1257061\tbest: 0.1257061 (50)\ttotal: 554ms\tremaining: 533ms\n",
      "60:\tlearn: 0.1065992\ttest: 0.1091684\tbest: 0.1091684 (60)\ttotal: 637ms\tremaining: 407ms\n",
      "70:\tlearn: 0.0923120\ttest: 0.0951884\tbest: 0.0951884 (70)\ttotal: 717ms\tremaining: 293ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80:\tlearn: 0.0824980\ttest: 0.0857693\tbest: 0.0857693 (80)\ttotal: 802ms\tremaining: 188ms\n",
      "90:\tlearn: 0.0756322\ttest: 0.0790558\tbest: 0.0790558 (90)\ttotal: 883ms\tremaining: 87.3ms\n",
      "99:\tlearn: 0.0693869\ttest: 0.0730510\tbest: 0.0730510 (99)\ttotal: 952ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.07305102566\n",
      "bestIteration = 99\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0   4.0\n",
      "0.0  8849.0  2460.0   242.0    0.0   0.0\n",
      "1.0  4823.0  2842.0   351.0    0.0   2.0\n",
      "2.0   230.0    54.0  1818.0    2.0  11.0\n",
      "3.0     0.0     0.0     0.0  571.0   1.0\n",
      "4.0     0.0     0.0     0.0   16.0   6.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.6322829697459377\n",
      "Precision total:  0.6381261354968755\n",
      "Recall total:  0.6502173659559842\n",
      "F1 total:  0.6386063966146803\n",
      "BACC total:  0.6502173659559842\n",
      "MCC total:  0.3735069829432364\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "cat_01 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "cat_01.fit(X_train_01, y_train_01, eval_set=(X_test_01, y_test_01), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = cat_01.predict(X_test_01)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('catboost', file = f)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "cat_acc_01 = Acc\n",
    "cat_pre_01 = Precision\n",
    "cat_rec_01 = Recall\n",
    "cat_f1_01 = F1\n",
    "cat_bacc_01 = BACC\n",
    "cat_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "cat_acc_01 = Acc\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  11372.0    90.0    89.0    0.0   0.0\n",
      "1.0    231.0  7728.0    59.0    0.0   0.0\n",
      "2.0     18.0    15.0  2082.0    0.0   0.0\n",
      "3.0      0.0     0.0     0.0  572.0   0.0\n",
      "4.0      0.0     0.0     0.0    0.0  22.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9774665589370679\n",
      "Precision total:  0.979760146756426\n",
      "Recall total:  0.9865464097413732\n",
      "F1 total:  0.9829905583077186\n",
      "BACC total:  0.9865464097413732\n",
      "MCC total:  0.9620764142463629\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_01, label=y_train_01)\n",
    "dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_01 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb_01 = xgb_01.predict(dtest)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    xgb_acc_01 = Acc\n",
    "    xgb_pre_01 = Precision\n",
    "    xgb_rec_01 = Recall\n",
    "    xgb_f1_01 = F1\n",
    "    xgb_bacc_01 = BACC\n",
    "    xgb_mcc_01 = MCC\n",
    "\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = tree.DecisionTreeClassifier()\n",
    "# model2 = KNeighborsClassifier()\n",
    "# model3= LogisticRegression()\n",
    "\n",
    "# model1.fit(x_train,y_train)\n",
    "# model2.fit(x_train,y_train)\n",
    "# model3.fit(x_train,y_train)\n",
    "\n",
    "# pred1=model1.predict_proba(x_test)\n",
    "# pred2=model2.predict_proba(x_test)\n",
    "# pred3=model3.predict_proba(x_test)\n",
    "\n",
    "# finalpred=(preds_svm_prob +\n",
    "#             preds_ada_prob +\n",
    "#             preds_knn_prob +\n",
    "#             preds_rf_prob +\n",
    "#             preds_dnn_prob +\n",
    "#             preds_lgbm_prob +\n",
    "#             preds_mlp_prob\n",
    "#             )/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voring_acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy cat: ', cat_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_00, file = f)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision cat: ', cat_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgb: ', xgb_pre_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall cat: ', cat_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgb: ', xgb_rec_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 cat: ', cat_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgb: ', xgb_f1_00, file = f)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voting_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision LR: ', lr_pre_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Precision Voting: ', voting_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision catboosting: ', cat_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgboost: ', xgb_pre_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall LR: ', lr_rec_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Recall Voting: ', voting_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall catboosting: ', cat_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgboost: ', xgb_rec_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 LR: ', lr_f1_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('F1 Voting: ', voting_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 catboosting: ', cat_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgboost: ', xgb_f1_01, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy cat: ', cat_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_00, file = f)\n",
    "\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision cat: ', cat_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision xgb: ', xgb_pre_00, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall cat: ', cat_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall xgb: ', xgb_rec_00, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 cat: ', cat_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 xgb: ', xgb_f1_00, file = f)\n",
    "\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voting_acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision LR: ', lr_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision Voting: ', voting_pre, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision catboosting: ', cat_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision xgboost: ', xgb_pre_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall LR: ', lr_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall Voting: ', voting_rec, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall catboosting: ', cat_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall xgboost: ', xgb_rec_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 LR: ', lr_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 Voting: ', voting_f1, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 catboosting: ', cat_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 xgboost: ', xgb_f1_01, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# split = 0.7\n",
    "\n",
    "# #AUC ROC\n",
    "# #---------------------------------------------------------------------\n",
    "\n",
    "# #AUCROC\n",
    "# aucroc =[]\n",
    "# y_array = [y_0,y_1,y_2,y_3,y_4]\n",
    "# for j in range(0,len(y_array)):\n",
    "#     # print(j)\n",
    "#     #------------------------------------------------------------------------------------------------------------\n",
    "#     X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y_array[j], train_size=split)\n",
    "    \n",
    "#     # evaluate the model\n",
    "\n",
    "#     knn_clf.fit(X_train,y_train)\n",
    "#     y_pred=knn_clf.predict(X_test) #These are the predicted output value\n",
    "#     # y_pred = knn_clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "\n",
    "#     # model = LGBMClassifier()\n",
    "#     # model.fit(X_train, y_train)\n",
    "#     # y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "    \n",
    "#     # Calculate AUC-ROC score\n",
    "#     auc_roc_score= roc_auc_score(y_true, y_scores,  average='weighted')  # Use 'micro' or 'macro' for different averaging strategies\n",
    "#     # print(\"AUC-ROC Score class:\", auc_roc_score)\n",
    "#     aucroc.append(auc_roc_score)\n",
    "#     #-------------------------------------------------------------------------------------------------------    -----\n",
    "#     # Calculate the average\n",
    "# average = sum(aucroc) / len(aucroc)\n",
    "\n",
    "# # Display the result\n",
    "# # with open(output_file_name, \"a\") as f:print(\"AUC ROC Average:\", average, file = f)\n",
    "# print(\"AUC ROC Average:\", average)\n",
    "\n",
    "# #End AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc_00 = 0 \n",
    "voting_acc_00 = 0\n",
    "\n",
    "lr_pre_00 = 0 \n",
    "voting_pre_00 = 0\n",
    "\n",
    "lr_rec_00 = 0 \n",
    "voting_rec_00 = 0\n",
    "\n",
    "lr_f1_00 = 0 \n",
    "voting_f1_00 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voting_acc_01 = 0\n",
    "\n",
    "\n",
    "voting_pre_01 = 0\n",
    "\n",
    "\n",
    "voting_rec_01 = 0\n",
    "\n",
    "\n",
    "voting_f1_01 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "| Accuracy   | Level 00           | Level 01           |\n",
      "+============+====================+====================+\n",
      "| ADA        | 0.7586959156465883 | 0.5515755453810934 |\n",
      "+------------+--------------------+--------------------+\n",
      "| SVM        | 0.8661441710769066 | 0.6067869647185564 |\n",
      "+------------+--------------------+--------------------+\n",
      "| DNN        | 0.5435435435435435 | 0.6091659933566748 |\n",
      "+------------+--------------------+--------------------+\n",
      "| MLP        | 0.9663340470515358 | 0.8995421492054942 |\n",
      "+------------+--------------------+--------------------+\n",
      "| KNN        | 0.974615871476858  | 0.9668731483975223 |\n",
      "+------------+--------------------+--------------------+\n",
      "| CAT        | 0.970562490741863  | 0.6322829697459377 |\n",
      "+------------+--------------------+--------------------+\n",
      "| XGB        | 0.9550357532420313 | 0.9774665589370679 |\n",
      "+------------+--------------------+--------------------+\n",
      "| LGBM       | 0.9810124025370662 | 0.9959601400484783 |\n",
      "+------------+--------------------+--------------------+\n",
      "| RF         | 0.9231742953716048 | 0.9234222102522668 |\n",
      "+------------+--------------------+--------------------+\n",
      "| LR         | 0                  | 0.6322829697459377 |\n",
      "+------------+--------------------+--------------------+\n",
      "| VOTING     | 0                  | 0                  |\n",
      "+------------+--------------------+--------------------+\n",
      "|            |                    |                    |\n",
      "+------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_acc = [ada_acc_00,\n",
    "                svm_acc_00,\n",
    "                dnn_acc_00,\n",
    "                mlp_acc_00,\n",
    "                knn_acc_00,\n",
    "                cat_acc_00,\n",
    "                xgb_acc_00,\n",
    "                lgbm_acc_00,\n",
    "                rf_acc_00,\n",
    "                lr_acc_00,\n",
    "                voting_acc_00]  \n",
    "level_01_acc = [ada_acc_01,\n",
    "                svm_acc_01,\n",
    "                dnn_acc_01,\n",
    "                mlp_acc_01,\n",
    "                knn_acc_01,\n",
    "                cat_acc_01,\n",
    "                xgb_acc_01,\n",
    "                lgbm_acc_01,\n",
    "                rf_acc_01,\n",
    "                lr_acc_01,\n",
    "                voting_acc_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Accuracy\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+--------------------+\n",
      "| Precision   | Level 00            | Level 01           |\n",
      "+=============+=====================+====================+\n",
      "| ADA         | 0.5148897595750748  | 0.5742474895492983 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| SVM         | 0.4865043558587877  | 0.538852401791561  |\n",
      "+-------------+---------------------+--------------------+\n",
      "| DNN         | 0.23228438339215812 | 0.4561130829802892 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| MLP         | 0.9367785393943964  | 0.9393231012474725 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| KNN         | 0.784230172592528   | 0.9675540177960912 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| CAT         | 0.7403722100119083  | 0.6381261354968755 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| XGB         | 0.7177268205303429  | 0.979760146756426  |\n",
      "+-------------+---------------------+--------------------+\n",
      "| LGBM        | 0.7740801027644095  | 0.9962764789229521 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| RF          | 0.7380881443061241  | 0.9352160549340095 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| LR          | 0                   | 0.6381261354968755 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| VOTING      | 0                   | 0                  |\n",
      "+-------------+---------------------+--------------------+\n",
      "|             |                     |                    |\n",
      "+-------------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_pre = [ada_pre_00,\n",
    "                svm_pre_00,\n",
    "                dnn_pre_00,\n",
    "                mlp_pre_00,\n",
    "                knn_pre_00,\n",
    "                cat_pre_00,\n",
    "                xgb_pre_00,\n",
    "                lgbm_pre_00,\n",
    "                rf_pre_00,\n",
    "                lr_pre_00,\n",
    "                voting_pre_00]  \n",
    "level_01_pre = [ada_pre_01,\n",
    "                svm_pre_01,\n",
    "                dnn_pre_01,\n",
    "                mlp_pre_01,\n",
    "                knn_pre_01,\n",
    "                cat_pre_01,\n",
    "                xgb_pre_01,\n",
    "                lgbm_pre_01,\n",
    "                rf_pre_01,\n",
    "                lr_pre_01,\n",
    "                voting_pre_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_pre[i]\n",
    "    data[i][2] = level_01_pre[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Precision\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+--------------------+\n",
      "| Recall   | Level 00            | Level 01           |\n",
      "+==========+=====================+====================+\n",
      "| ADA      | 0.5628118482439187  | 0.4071999445070654 |\n",
      "+----------+---------------------+--------------------+\n",
      "| SVM      | 0.48149153505386855 | 0.5575845535423665 |\n",
      "+----------+---------------------+--------------------+\n",
      "| DNN      | 0.224502924856971   | 0.5459240511334371 |\n",
      "+----------+---------------------+--------------------+\n",
      "| MLP      | 0.72674616229928    | 0.9409931095579122 |\n",
      "+----------+---------------------+--------------------+\n",
      "| KNN      | 0.7586551316929457  | 0.8658800286465154 |\n",
      "+----------+---------------------+--------------------+\n",
      "| CAT      | 0.7412066832896944  | 0.6502173659559842 |\n",
      "+----------+---------------------+--------------------+\n",
      "| XGB      | 0.7005618139133347  | 0.9865464097413732 |\n",
      "+----------+---------------------+--------------------+\n",
      "| LGBM     | 0.7900534539974579  | 0.9969625383786198 |\n",
      "+----------+---------------------+--------------------+\n",
      "| RF       | 0.5464858303636617  | 0.8027836906899554 |\n",
      "+----------+---------------------+--------------------+\n",
      "| LR       | 0                   | 0.6502173659559842 |\n",
      "+----------+---------------------+--------------------+\n",
      "| VOTING   | 0                   | 0                  |\n",
      "+----------+---------------------+--------------------+\n",
      "|          |                     |                    |\n",
      "+----------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_rec = [ada_rec_00,\n",
    "                svm_rec_00,\n",
    "                dnn_rec_00,\n",
    "                mlp_rec_00,\n",
    "                knn_rec_00,\n",
    "                cat_rec_00,\n",
    "                xgb_rec_00,\n",
    "                lgbm_rec_00,\n",
    "                rf_rec_00,\n",
    "                lr_rec_00,\n",
    "                voting_rec_00]  \n",
    "level_01_rec = [ada_rec_01,\n",
    "                svm_rec_01,\n",
    "                dnn_rec_01,\n",
    "                mlp_rec_01,\n",
    "                knn_rec_01,\n",
    "                cat_rec_01,\n",
    "                xgb_rec_01,\n",
    "                lgbm_rec_01,\n",
    "                rf_rec_01,\n",
    "                lr_rec_01,\n",
    "                voting_rec_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_rec[i]\n",
    "    data[i][2] = level_01_rec[i]\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Recall\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+\n",
      "| F1     | Level 00           | Level 01            |\n",
      "+========+====================+=====================+\n",
      "| ADA    | 0.5232524495141136 | 0.3507740333236951  |\n",
      "+--------+--------------------+---------------------+\n",
      "| SVM    | 0.4829959148201005 | 0.5418726099415194  |\n",
      "+--------+--------------------+---------------------+\n",
      "| DNN    | 0.2013062027461246 | 0.48993146733213655 |\n",
      "+--------+--------------------+---------------------+\n",
      "| MLP    | 0.7366784048823265 | 0.9391811063990051  |\n",
      "+--------+--------------------+---------------------+\n",
      "| KNN    | 0.7597319520860626 | 0.8961962839323581  |\n",
      "+--------+--------------------+---------------------+\n",
      "| CAT    | 0.7407826385359696 | 0.6386063966146803  |\n",
      "+--------+--------------------+---------------------+\n",
      "| XGB    | 0.7084949959099308 | 0.9829905583077186  |\n",
      "+--------+--------------------+---------------------+\n",
      "| LGBM   | 0.7813739805540139 | 0.996618329132178   |\n",
      "+--------+--------------------+---------------------+\n",
      "| RF     | 0.5587285632916946 | 0.8431210275432454  |\n",
      "+--------+--------------------+---------------------+\n",
      "| LR     | 0                  | 0.6386063966146803  |\n",
      "+--------+--------------------+---------------------+\n",
      "| VOTING | 0                  | 0                   |\n",
      "+--------+--------------------+---------------------+\n",
      "|        |                    |                     |\n",
      "+--------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_f1[i]\n",
    "    data[i][2] = level_01_f1[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"F1\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| Models   | ACC-00             |  ACC-01            | PRE-00              |  PRE-01            | REC-00              |  REC-01            | F1-00              |  F1-01              |\n",
      "+==========+====================+====================+=====================+====================+=====================+====================+====================+=====================+\n",
      "| ADA      | 0.7586959156465883 | 0.5515755453810934 | 0.5148897595750748  | 0.5742474895492983 | 0.5628118482439187  | 0.4071999445070654 | 0.5232524495141136 | 0.3507740333236951  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| SVM      | 0.8661441710769066 | 0.6067869647185564 | 0.4865043558587877  | 0.538852401791561  | 0.48149153505386855 | 0.5575845535423665 | 0.4829959148201005 | 0.5418726099415194  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| DNN      | 0.5435435435435435 | 0.6091659933566748 | 0.23228438339215812 | 0.4561130829802892 | 0.224502924856971   | 0.5459240511334371 | 0.2013062027461246 | 0.48993146733213655 |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| MLP      | 0.9663340470515358 | 0.8995421492054942 | 0.9367785393943964  | 0.9393231012474725 | 0.72674616229928    | 0.9409931095579122 | 0.7366784048823265 | 0.9391811063990051  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| KNN      | 0.974615871476858  | 0.9668731483975223 | 0.784230172592528   | 0.9675540177960912 | 0.7586551316929457  | 0.8658800286465154 | 0.7597319520860626 | 0.8961962839323581  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| CAT      | 0.970562490741863  | 0.6322829697459377 | 0.7403722100119083  | 0.6381261354968755 | 0.7412066832896944  | 0.6502173659559842 | 0.7407826385359696 | 0.6386063966146803  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| XGB      | 0.9550357532420313 | 0.9774665589370679 | 0.7177268205303429  | 0.979760146756426  | 0.7005618139133347  | 0.9865464097413732 | 0.7084949959099308 | 0.9829905583077186  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| LGBM     | 0.9810124025370662 | 0.9959601400484783 | 0.7740801027644095  | 0.9962764789229521 | 0.7900534539974579  | 0.9969625383786198 | 0.7813739805540139 | 0.996618329132178   |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| RF       | 0.9231742953716048 | 0.9234222102522668 | 0.7380881443061241  | 0.9352160549340095 | 0.5464858303636617  | 0.8027836906899554 | 0.5587285632916946 | 0.8431210275432454  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| LR       | 0                  | 0.6322829697459377 | 0                   | 0.6381261354968755 | 0                   | 0.6502173659559842 | 0                  | 0.6386063966146803  |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "| VOTING   | 0                  | 0                  | 0                   | 0                  | 0                   | 0                  | 0                  | 0                   |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n",
      "|          |                    |                    |                     |                    |                     |                    |                    |                     |\n",
      "+----------+--------------------+--------------------+---------------------+--------------------+---------------------+--------------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(9)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "    data[i][3] = level_00_pre[i] \n",
    "    data[i][4] = level_01_pre[i]\n",
    "\n",
    "    data[i][5] = level_00_rec[i] \n",
    "    data[i][6] = level_01_rec[i]\n",
    "\n",
    "    data[i][7] = level_00_f1[i]\n",
    "    data[i][8] = level_01_f1[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Models\", \"ACC-00\", \" ACC-01\",\"PRE-00\", \" PRE-01\",\"REC-00\", \" REC-01\",\"F1-00\", \" F1-01\",]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
