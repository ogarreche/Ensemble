{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensemble with NSL-KDD\n",
    "# Parameters\n",
    "\n",
    "#----------------------------------------------\n",
    "# 0 for not using it as base learner\n",
    "# 1 for using it as base learner\n",
    "\n",
    "use_model_ada = 1 \n",
    "use_model_dnn = 1 \n",
    "use_model_mlp = 1 \n",
    "use_model_lgbm = 1 \n",
    "use_model_rf = 1 \n",
    "use_model_svm = 1\n",
    "use_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "# 0 for training the model\n",
    "# 1 for using the saved version of the model\n",
    "\n",
    "load_model_ada = 0 \n",
    "load_model_dnn = 0 \n",
    "load_model_mlp = 0 \n",
    "load_model_lgbm = 0 \n",
    "load_model_rf = 0 \n",
    "load_model_svm = 0\n",
    "load_model_knn = 0 \n",
    "#----------------------------------------------\n",
    "\n",
    "# load_model_ada = 1\n",
    "# load_model_dnn = 1 \n",
    "# load_model_mlp = 1 \n",
    "# load_model_lgbm = 1 \n",
    "# load_model_rf = 1 \n",
    "# load_model_svm = 1\n",
    "# load_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the name of the output text file\n",
    "output_file_name = \"ensemble_class_AF.txt\"\n",
    "with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('---- Start Ensemble Model Info - v0 ----', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Defining metric functions\n",
    "def ACC(TP,TN,FP,FN):\n",
    "    Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "    return Acc\n",
    "def ACC_2 (TP, FN):\n",
    "    ac = (TP/(TP+FN))\n",
    "    return ac\n",
    "def PRECISION(TP,FP):\n",
    "    eps = 1e-7\n",
    "    Precision = TP/(TP+FP+eps)\n",
    "    \n",
    "\n",
    "    return Precision\n",
    "def RECALL(TP,FN):\n",
    "    Recall = TP/(TP+FN)\n",
    "    return Recall\n",
    "def F1(Recall, Precision):\n",
    "    F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "    return F1\n",
    "def BACC(TP,TN,FP,FN):\n",
    "    BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "    return BACC\n",
    "def MCC(TP,TN,FP,FN):\n",
    "    eps = 1e-7\n",
    "    MCC = (TN*TP-FN*FP)/(((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**.5)\n",
    "    return MCC\n",
    "def AUC_ROC(y_test_bin,y_score):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    auc_avg = 0\n",
    "    counting = 0\n",
    "    for i in range(n_classes):\n",
    "      fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "     # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "      #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "      auc_avg += auc(fpr[i], tpr[i])\n",
    "      counting = i+1\n",
    "    return auc_avg/counting\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# attach the column names to the dataset\n",
    "feature=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "          \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "          \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "          \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "          \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"]\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "\n",
    "train='KDDTrain+.txt'\n",
    "test='KDDTest+.txt'\n",
    "\n",
    "df=pd.read_csv(train,names=feature)\n",
    "df_test=pd.read_csv(test,names=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 43)\n",
      "Dimensions of the Test set: (22544, 43)\n",
      "Label distribution Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "rootkit              13\n",
      "xterm                13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "loadmodule            2\n",
      "worm                  2\n",
      "sqlattack             2\n",
      "phf                   2\n",
      "udpstorm              2\n",
      "perl                  2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n",
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in service:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n",
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 64 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n",
      "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n",
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n",
      "(125973, 123)\n",
      "(22544, 123)\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "X_train has shape: (125973, 122) \n",
      "y_train has shape: (125973, 1)\n",
      "X_test has shape: (22544, 122) \n",
      "y_test has shape: (22544, 1)\n",
      "Counter({0: 67343, 1: 45927, 2: 11656, 3: 995, 4: 52})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n",
    "\n",
    "\n",
    "df.drop(['difficulty'],axis=1,inplace=True)\n",
    "df_test.drop(['difficulty'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('Label distribution Training set:')\n",
    "print(df['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(df_test['label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "\n",
    "\n",
    "# protocol type\n",
    "unique_protocol=sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(df_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n",
    "\n",
    "df_cat_data.head()\n",
    "\n",
    "\n",
    "trainservice=df['service'].tolist()\n",
    "testservice= df_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference\n",
    "\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "\n",
    "newdf=df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "\n",
    "\n",
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())\n",
    "\n",
    "\n",
    "# Specify your selected features. Note that you'll need to modify this list according to your final processed dataframe\n",
    "#Uncomment the below lines to use these top 20 features from shap analysis\n",
    "#selected_features = [\"root_shell\",\"service_telnet\",\"num_shells\",\"service_uucp\",\"dst_host_same_src_port_rate\"\n",
    "#                     ,\"dst_host_rerror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_srv_count\",\"service_private\",\"logged_in\",\n",
    "#                    \"dst_host_serror_rate\",\"serror_rate\",\"srv_serror_rate\",\"flag_S0\",\"diff_srv_rate\",\"dst_host_srv_diff_host_rate\",\"num_file_creations\",\"flag_RSTR\"#,\"dst_host_same_srv_rate\",\"service_Idap\",\"label\"]\n",
    "                     \n",
    "\n",
    "# Select those features from your dataframe\n",
    "#newdf = newdf[selected_features]\n",
    "#newdf_test = newdf_test[selected_features]\n",
    "\n",
    "# Now your dataframe only contains your selected features.\n",
    "\n",
    "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
    "multi_data = newdf.copy()\n",
    "multi_label = pd.DataFrame(multi_data.label)\n",
    "\n",
    "multi_data_test=newdf_test.copy()\n",
    "multi_label_test = pd.DataFrame(multi_data_test.label)\n",
    "\n",
    "\n",
    "# using standard scaler for normalizing\n",
    "std_scaler = StandardScaler()\n",
    "def standardization(df,col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "    return df\n",
    "\n",
    "numeric_col = multi_data.select_dtypes(include='number').columns\n",
    "data = standardization(multi_data,numeric_col)\n",
    "numeric_col_test = multi_data_test.select_dtypes(include='number').columns\n",
    "data_test = standardization(multi_data_test,numeric_col_test)\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2_test = preprocessing.LabelEncoder()\n",
    "enc_label = multi_label.apply(le2.fit_transform)\n",
    "enc_label_test = multi_label_test.apply(le2_test.fit_transform)\n",
    "multi_data = multi_data.copy()\n",
    "multi_data_test = multi_data_test.copy()\n",
    "\n",
    "multi_data['intrusion'] = enc_label\n",
    "multi_data_test['intrusion'] = enc_label_test\n",
    "\n",
    "#y_mul = multi_data['intrusion']\n",
    "multi_data\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "\n",
    "multi_data.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data\n",
    "multi_data_test.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "y_train_multi= multi_data[['intrusion']]\n",
    "X_train_multi= multi_data.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_train has shape:',X_train_multi.shape,'\\ny_train has shape:',y_train_multi.shape)\n",
    "\n",
    "y_test_multi= multi_data_test[['intrusion']]\n",
    "X_test_multi= multi_data_test.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_test has shape:',X_test_multi.shape,'\\ny_test has shape:',y_test_multi.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train_multi['intrusion'])\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train_multi = LabelBinarizer().fit_transform(y_train_multi)\n",
    "\n",
    "y_test_multi = LabelBinarizer().fit_transform(y_test_multi)\n",
    "\n",
    "\n",
    "Y_train=y_train_multi.copy()\n",
    "X_train=X_train_multi.copy()\n",
    "\n",
    "Y_test=y_test_multi.copy()\n",
    "X_test=X_test_multi.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\n\\n# Number of best features you want to select\\nk = 15\\n\\n# Initialize a dataframe to store the scores for each feature against each class\\nfeature_scores = pd.DataFrame(index=X_train.columns)\\n\\n# Loop through each class\\nfor class_index in range(Y_train.shape[1]):\\n    \\n    # Get the current class labels\\n    y_train_current_class = Y_train[:, class_index]\\n    \\n    # Select K best features for the current class\\n    best_features = SelectKBest(score_func=f_classif, k=\\'all\\')\\n    fit = best_features.fit(X_train, y_train_current_class)\\n\\n    # Get the scores\\n    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\\n    \\n    # Concatenate the scores to the main dataframe\\n    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\\n\\n# Get the sum of the scores for each feature\\nfeature_scores[\\'total\\'] = feature_scores.sum(axis=1)\\n\\n# Get the top k features in a list\\ntop_k_features = feature_scores.nlargest(k, \\'total\\').index.tolist()\\n\\nprint(top_k_features)\\n\\n'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# In[24]:\n",
    "\n",
    "'''\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Number of best features you want to select\n",
    "k = 15\n",
    "\n",
    "# Initialize a dataframe to store the scores for each feature against each class\n",
    "feature_scores = pd.DataFrame(index=X_train.columns)\n",
    "\n",
    "# Loop through each class\n",
    "for class_index in range(Y_train.shape[1]):\n",
    "    \n",
    "    # Get the current class labels\n",
    "    y_train_current_class = Y_train[:, class_index]\n",
    "    \n",
    "    # Select K best features for the current class\n",
    "    best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "    fit = best_features.fit(X_train, y_train_current_class)\n",
    "\n",
    "    # Get the scores\n",
    "    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\n",
    "    \n",
    "    # Concatenate the scores to the main dataframe\n",
    "    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\n",
    "\n",
    "# Get the sum of the scores for each feature\n",
    "feature_scores['total'] = feature_scores.sum(axis=1)\n",
    "\n",
    "# Get the top k features in a list\n",
    "top_k_features = feature_scores.nlargest(k, 'total').index.tolist()\n",
    "\n",
    "print(top_k_features)\n",
    "\n",
    "'''\n",
    "# In[32]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.10249223e-01, -7.67859947e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.73736981e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.76224074e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02, -1.21190076e+00, -4.64315895e-02],\n",
       "       ...,\n",
       "       [-9.29714678e-02, -7.36430591e-03, -3.87394518e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-8.68282658e-02, -7.36430591e-03, -3.87568593e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [ 1.61587463e-01, -7.46804833e-03,  1.06953862e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Assuming you have features X and labels Y\n",
    "# X, Y = make_classification()\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=100)\n",
    "\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "X_train.values\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_class_train = np.argmax(y_train_multi, axis=1)\n",
    "single_class_test = np.argmax(y_test_multi, axis=1)\n",
    "\n",
    "\n",
    "df1 = X_train_multi.assign(Label = single_class_train)\n",
    "df2 =  X_test_multi.assign(Label = single_class_test)\n",
    "\n",
    "frames = [df1,  df2]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection = [\n",
    "                    'dst_host_same_srv_rate',\n",
    "                    'dst_host_srv_count',\n",
    "                    'dst_host_same_src_port_rate',\n",
    "                    'logged_in',\n",
    "                    'dst_host_serror_rate',\n",
    "                    'count',\n",
    "                    'srv_count',\n",
    "                    'dst_host_rerror_rate',\n",
    "                    'Label'\n",
    "                    ]\n",
    "\n",
    "# df.pop('dst host same srv rate')\n",
    "# df.pop('dst host srv count')\n",
    "# df.pop('dst host same src port rate')\n",
    "# df.pop('logged in')\n",
    "# df.pop('dst host serror rate')\n",
    "# df.pop('count')\n",
    "# df.pop('srv count')\n",
    "# df.pop('dst host rerror rate')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "\n",
    "y1, y2 = pd.factorize(y)\n",
    "\n",
    "y_0 = pd.DataFrame(y1)\n",
    "y_1 = pd.DataFrame(y1)\n",
    "y_2 = pd.DataFrame(y1)\n",
    "y_3 = pd.DataFrame(y1)\n",
    "y_4 = pd.DataFrame(y1)\n",
    "\n",
    "\n",
    "# y_0 = y_0.replace(0, 0)\n",
    "# y_0 = y_0.replace(1, 1)\n",
    "y_0 = y_0.replace(2, 1)\n",
    "y_0 = y_0.replace(3, 1)\n",
    "y_0 = y_0.replace(4, 1)\n",
    "\n",
    "\n",
    "y_1 = y_1.replace(1, 999)\n",
    "y_1 = y_1.replace(0, 1)\n",
    "# y_1 = y_1.replace(1, 0)\n",
    "y_1 = y_1.replace(2, 1)\n",
    "y_1 = y_1.replace(3, 1)\n",
    "y_1 = y_1.replace(4, 1)\n",
    "y_1 = y_1.replace(999, 1)\n",
    "\n",
    "\n",
    "y_2 = y_2.replace(0, 1)\n",
    "y_2 = y_2.replace(1, 1)\n",
    "y_2 = y_2.replace(2, 0)\n",
    "y_2 = y_2.replace(3, 1)\n",
    "y_2 = y_2.replace(4, 1)\n",
    "\n",
    "\n",
    "y_3 = y_3.replace(0, 1)\n",
    "# y_3 = y_3.replace(1, 1)\n",
    "y_3 = y_3.replace(2, 1)\n",
    "y_3 = y_3.replace(3, 0)\n",
    "y_3 = y_3.replace(4, 1)\n",
    "\n",
    "\n",
    "y_4 = y_4.replace(0, 1)\n",
    "# y_4 = y_4.replace(1, 1)\n",
    "y_4 = y_4.replace(2, 1)\n",
    "y_4 = y_4.replace(3, 1)\n",
    "y_4 = y_4.replace(4, 0)\n",
    "\n",
    "\n",
    "\n",
    "df = df.assign(Label = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset between level 00 and level 01\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "split = 0.5 # 0.7\n",
    "\n",
    "# X_00,X_01, y_00, y_01 = sklearn.model_selection.train_test_split(X, y, train_size=split)\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 77054, 1: 53387, 2: 14077, 3: 3880, 4: 119})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts2 = Counter(y)\n",
    "print(label_counts2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base learner Split\n",
    "# split = 0.7\n",
    "\n",
    "# X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_00, y_00, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134548</th>\n",
       "      <td>-0.155534</td>\n",
       "      <td>-0.021988</td>\n",
       "      <td>-0.096896</td>\n",
       "      <td>-0.017624</td>\n",
       "      <td>-0.059104</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>-0.113521</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.890373</td>\n",
       "      <td>-0.016494</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203539</td>\n",
       "      <td>-0.18843</td>\n",
       "      <td>-0.009419</td>\n",
       "      <td>-0.174880</td>\n",
       "      <td>-0.313124</td>\n",
       "      <td>-0.030535</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>-1.392705</td>\n",
       "      <td>-0.056997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51479</th>\n",
       "      <td>-0.109865</td>\n",
       "      <td>-0.007598</td>\n",
       "      <td>-0.004838</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93592</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007751</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68245</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007707</td>\n",
       "      <td>-0.004452</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142312</th>\n",
       "      <td>-0.155534</td>\n",
       "      <td>-0.021988</td>\n",
       "      <td>-0.096896</td>\n",
       "      <td>-0.017624</td>\n",
       "      <td>-0.059104</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>-0.113521</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.890373</td>\n",
       "      <td>-0.016494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453815</td>\n",
       "      <td>-0.18843</td>\n",
       "      <td>-0.009419</td>\n",
       "      <td>-0.174880</td>\n",
       "      <td>3.193619</td>\n",
       "      <td>-0.030535</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>-1.392705</td>\n",
       "      <td>-0.056997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14209</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007720</td>\n",
       "      <td>-0.004585</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196020</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-1.211901</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134732</th>\n",
       "      <td>-0.155534</td>\n",
       "      <td>-0.021988</td>\n",
       "      <td>-0.096896</td>\n",
       "      <td>-0.017624</td>\n",
       "      <td>-0.059104</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>-0.113521</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.890373</td>\n",
       "      <td>-0.016494</td>\n",
       "      <td>...</td>\n",
       "      <td>2.203539</td>\n",
       "      <td>-0.18843</td>\n",
       "      <td>-0.009419</td>\n",
       "      <td>-0.174880</td>\n",
       "      <td>-0.313124</td>\n",
       "      <td>-0.030535</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>-1.392705</td>\n",
       "      <td>-0.056997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46836</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007750</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101905</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>-0.004887</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74258 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
       "134548 -0.155534  -0.021988  -0.096896 -0.017624       -0.059104 -0.019459   \n",
       "51479  -0.109865  -0.007598  -0.004838 -0.014089       -0.089486 -0.007736   \n",
       "93592  -0.110249  -0.007751  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "68245  -0.110249  -0.007707  -0.004452 -0.014089       -0.089486 -0.007736   \n",
       "142312 -0.155534  -0.021988  -0.096896 -0.017624       -0.059104 -0.019459   \n",
       "...          ...        ...        ...       ...             ...       ...   \n",
       "14209  -0.110249  -0.007720  -0.004585 -0.014089       -0.089486 -0.007736   \n",
       "4220   -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "134732 -0.155534  -0.021988  -0.096896 -0.017624       -0.059104 -0.019459   \n",
       "46836  -0.110249  -0.007750  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "101905 -0.110249  -0.007755  -0.004887 -0.014089       -0.089486 -0.007736   \n",
       "\n",
       "             hot  num_failed_logins  logged_in  num_compromised  ...  \\\n",
       "134548 -0.113521          -0.143999  -0.890373        -0.016494  ...   \n",
       "51479  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "93592  -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "68245  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "142312 -0.113521          -0.143999  -0.890373        -0.016494  ...   \n",
       "...          ...                ...        ...              ...  ...   \n",
       "14209  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "4220   -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "134732 -0.113521          -0.143999  -0.890373        -0.016494  ...   \n",
       "46836  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "101905 -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "\n",
       "        flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1  \\\n",
       "134548  2.203539   -0.18843    -0.009419  -0.174880 -0.313124 -0.030535   \n",
       "51479  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "93592  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "68245  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "142312 -0.453815   -0.18843    -0.009419  -0.174880  3.193619 -0.030535   \n",
       "...          ...        ...          ...        ...       ...       ...   \n",
       "14209  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "4220    3.196020   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "134732  2.203539   -0.18843    -0.009419  -0.174880 -0.313124 -0.030535   \n",
       "46836  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "101905 -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "\n",
       "         flag_S2   flag_S3   flag_SF   flag_SH  \n",
       "134548 -0.025803 -0.105681 -1.392705 -0.056997  \n",
       "51479  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "93592  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "68245  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "142312 -0.025803 -0.105681 -1.392705 -0.056997  \n",
       "...          ...       ...       ...       ...  \n",
       "14209  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "4220   -0.031767 -0.019726 -1.211901 -0.046432  \n",
       "134732 -0.025803 -0.105681 -1.392705 -0.056997  \n",
       "46836  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "101905 -0.031767 -0.019726  0.825150 -0.046432  \n",
       "\n",
       "[74258 rows x 122 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134548    1\n",
       "51479     0\n",
       "93592     0\n",
       "68245     0\n",
       "142312    1\n",
       "         ..\n",
       "14209     0\n",
       "4220      2\n",
       "134732    2\n",
       "46836     0\n",
       "101905    0\n",
       "Name: Label, Length: 74258, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEVEL 0 - Weak models - Base Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 3)                 369       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 437\n",
      "Trainable params: 437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------START of WEAK LEARNERS (BASE MODELS) - STACK 00 -----------------', file = f)\n",
    "\n",
    "#Defining Basemodels\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "\n",
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "\n",
    "#DNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# #Model Parameters\n",
    "# dropout_rate = 0.01\n",
    "# nodes = 70\n",
    "# out_layer = 5\n",
    "# optimizer='adam'\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "# epochs=1\n",
    "# batch_size=2*256\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "dnn = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "dnn.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn.summary()\n",
    "\n",
    "\n",
    "\n",
    "# dnn = Sequential()\n",
    "# dnn.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "# dnn.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "# dnn.add(Dense(5))  # Output layer\n",
    "\n",
    "# dnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # summary of model layers\n",
    "# dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SVM\n",
    "# # Wrap SGDClassifier with MultiOutputClassifier\n",
    "# multi_target_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# multi_target_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.81093455 0.78709938 0.88176677 0.62911588 0.8131439 ]\n",
      "Mean accuracy: 0.7844120961457108\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.95300296 0.94485591 0.94929976 0.94491953 0.94175476]\n",
      "Mean accuracy: 0.9467665859722331\n",
      "---------------------------------------------------------------------------------\n",
      "Training SVM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.96956639 0.96983571 0.96747913 0.96983368 0.96525486]\n",
      "Mean accuracy: 0.9683939548686537\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.98969836 0.98841907 0.98774576 0.98841829 0.98653289]\n",
      "Mean accuracy: 0.9881628730296491\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.98309992 0.98471586 0.97569351 0.97447983 0.94727628]\n",
      "Mean accuracy: 0.9730530804755009\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.99245893 0.99266092 0.99165096 0.99286243 0.99138105]\n",
      "Mean accuracy: 0.9922028581131872\n",
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 4.9450 - accuracy: 0.4354 - val_loss: 3.2082 - val_accuracy: 0.6155\n",
      "Epoch 2/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 3.4443 - accuracy: 0.5341 - val_loss: 2.3652 - val_accuracy: 0.6965\n",
      "Epoch 3/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 2.5949 - accuracy: 0.5393 - val_loss: 1.2063 - val_accuracy: 0.7063\n",
      "Epoch 4/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 2.1064 - accuracy: 0.5414 - val_loss: 1.1003 - val_accuracy: 0.6964\n",
      "Epoch 5/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.7280 - accuracy: 0.5435 - val_loss: 1.0428 - val_accuracy: 0.6910\n",
      "Epoch 6/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.4767 - accuracy: 0.5450 - val_loss: 0.9898 - val_accuracy: 0.6893\n",
      "Epoch 7/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.3572 - accuracy: 0.5640 - val_loss: 0.9292 - val_accuracy: 0.6877\n",
      "Epoch 8/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.2470 - accuracy: 0.5907 - val_loss: 0.8983 - val_accuracy: 0.7063\n",
      "Epoch 9/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.2463 - accuracy: 0.6060 - val_loss: 0.9378 - val_accuracy: 0.6877\n",
      "Epoch 10/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.2149 - accuracy: 0.5989 - val_loss: 0.8530 - val_accuracy: 0.7010\n",
      "Epoch 11/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.3133 - accuracy: 0.5863 - val_loss: 0.8772 - val_accuracy: 0.6971\n",
      "Epoch 12/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.1805 - accuracy: 0.5924 - val_loss: 0.8385 - val_accuracy: 0.7035\n",
      "Epoch 13/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.1158 - accuracy: 0.6090 - val_loss: 0.8140 - val_accuracy: 0.7157\n",
      "Epoch 14/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.0917 - accuracy: 0.6087 - val_loss: 0.8168 - val_accuracy: 0.7247\n",
      "Epoch 15/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.1442 - accuracy: 0.6060 - val_loss: 0.8075 - val_accuracy: 0.7159\n",
      "Epoch 16/100\n",
      "465/465 [==============================] - 2s 3ms/step - loss: 1.1245 - accuracy: 0.6045 - val_loss: 0.7999 - val_accuracy: 0.7189\n",
      "Epoch 17/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.1083 - accuracy: 0.5927 - val_loss: 0.8334 - val_accuracy: 0.7129\n",
      "Epoch 18/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0608 - accuracy: 0.5997 - val_loss: 0.8269 - val_accuracy: 0.7144\n",
      "Epoch 19/100\n",
      "465/465 [==============================] - 1s 2ms/step - loss: 1.0462 - accuracy: 0.6023 - val_loss: 0.8142 - val_accuracy: 0.7138\n",
      "Epoch 20/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0418 - accuracy: 0.6114 - val_loss: 0.8128 - val_accuracy: 0.7124\n",
      "Epoch 21/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0197 - accuracy: 0.6108 - val_loss: 0.8037 - val_accuracy: 0.7165\n",
      "Epoch 22/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0591 - accuracy: 0.5942 - val_loss: 0.8159 - val_accuracy: 0.7008\n",
      "Epoch 23/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0417 - accuracy: 0.5965 - val_loss: 0.8134 - val_accuracy: 0.7022\n",
      "Epoch 24/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0454 - accuracy: 0.5984 - val_loss: 0.8110 - val_accuracy: 0.7054\n"
     ]
    }
   ],
   "source": [
    "#Training Basemodels\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "n_splits = 5  # You can adjust the number of folds as needed\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training Model')\n",
    "with open(output_file_name, \"a\") as f: print('Training weak models - level 0', file = f)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1 and load_model_ada == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_rf == 1 and load_model_rf == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf = rf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_rf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf, 'rf_base_model.joblib')\n",
    "\n",
    "if use_model_svm == 1 and load_model_svm == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    # clf.score(X_train, y_train)\n",
    "    time_taken = end - start\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(clf, 'svm_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_knn == 1 and load_model_knn == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf, 'knn_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_lgbm == 1 and load_model_lgbm == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_base_model.joblib')\n",
    "\n",
    "if use_model_mlp == 1 and load_model_mlp == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_dnn == 1 and load_model_dnn == 0:\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    # Convert Y_test back to its original format\n",
    "    # y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    # dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # End the timer\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(dnn, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    dnn.save(\"DNN_base_model.h5\")\n",
    "\n",
    "    # Calculate the time taken and print it out\n",
    "    # print(f'Time taken for training: {time_taken} seconds')\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Define your Keras model as a function\n",
    "# def create_model(optimizer='adam', hidden_layer_size=16):\n",
    "#     # model = Sequential()\n",
    "#     # model.add(Dense(hidden_layer_size, input_dim=input_size, activation='relu'))\n",
    "#     # model.add(Dense(1, activation='sigmoid'))\n",
    "#     # model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "#     dnn = tf.keras.Sequential()\n",
    "\n",
    "#     # Input layer\n",
    "#     dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "#     # Dense layers with dropout\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "#     dnn.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "#     dnn.summary()\n",
    "#     return dnn\n",
    "\n",
    "# # Create a KerasClassifier\n",
    "# dnn = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam', 'sgd'],\n",
    "#     'hidden_layer_size': [8, 16, 32]\n",
    "# }\n",
    "\n",
    "# # Create the StratifiedKFold\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Create GridSearchCV\n",
    "# grid = GridSearchCV(estimator=dnn, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best accuracy\n",
    "# print(\"Best Parameters: \", grid_result.best_params_)\n",
    "# print(\"Best Accuracy: \", grid_result.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if load_model_ada == 1:\n",
    "    ada = joblib.load('ada_base_model.joblib')\n",
    "\n",
    "if load_model_svm == 1:\n",
    "    clf =  joblib.load('svm_base_model.joblib')\n",
    "\n",
    "if load_model_dnn == 1:\n",
    "    dnn = load_model(\"DNN_base_model.h5\")\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf = joblib.load('knn_base_model.joblib')\n",
    "\n",
    "if load_model_mlp == 1:\n",
    "    MLP = joblib.load('mlp_base_model.joblib')\n",
    "\n",
    "if load_model_rf == 1:\n",
    "    rf = joblib.load('rf_base_model.joblib')\n",
    "\n",
    "if load_model_lgbm == 1:\n",
    "    lgbm = joblib.load('lgbm_base_model.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "# preds_svm = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# y_scores = y_pred\n",
    "# y_true = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base leaners predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction SVM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction DNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction MLP\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction KNN\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('Generating Predictions', file = f)\n",
    "\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf = rf.predict(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    print('Prediction SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "    start = time.time()\n",
    "    preds_svm = clf.predict(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm = lgbm.predict(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    print('Prediction DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #DNN\n",
    "    start = time.time()\n",
    "    pred_dnn = dnn.predict(X_test)\n",
    "    preds_dnn = np.argmax(pred_dnn,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada = ada.predict(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test)\n",
    "    preds_mlp = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf.predict(X_test)\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRICS - Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# >>> \n",
    "# >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "# 0.99...\n",
    "# >>> roc_auc_score(y, clf.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your features and labels as X and y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb = model.predict(dtest)\n",
    "\n",
    "# Convert predicted probabilities to class labels (if necessary)\n",
    "# y_pred_labels = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0      1.0     2.0     3.0   4.0\n",
      "0.0  38214.0     38.0    92.0   119.0   5.0\n",
      "1.0     24.0  26775.0    19.0     2.0   0.0\n",
      "2.0     73.0      9.0  6826.0    11.0   0.0\n",
      "3.0    131.0      1.0     3.0  1850.0   4.0\n",
      "4.0     22.0      1.0     0.0    10.0  30.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9924049610148264\n",
      "Precision total:  0.9346385618670787\n",
      "Recall total:  0.8769168240779491\n",
      "F1 total:  0.8988820677016778\n",
      "BACC total:  0.8769168240779491\n",
      "MCC total:  0.9871661407783556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    xgb_acc_00 = Acc\n",
    "    xgb_pre_00 = Precision\n",
    "    xgb_rec_00 = Recall\n",
    "    xgb_f1_00 = F1\n",
    "    xgb_bacc_00 = BACC\n",
    "    xgb_mcc_00 = MCC\n",
    "\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2939535\ttest: 1.2946361\tbest: 1.2946361 (0)\ttotal: 28.9ms\tremaining: 2.86s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:\tlearn: 0.4203759\ttest: 0.4208851\tbest: 0.4208851 (10)\ttotal: 251ms\tremaining: 2.03s\n",
      "20:\tlearn: 0.2101778\ttest: 0.2105840\tbest: 0.2105840 (20)\ttotal: 466ms\tremaining: 1.75s\n",
      "30:\tlearn: 0.1311956\ttest: 0.1318065\tbest: 0.1318065 (30)\ttotal: 666ms\tremaining: 1.48s\n",
      "40:\tlearn: 0.0943015\ttest: 0.0949814\tbest: 0.0949814 (40)\ttotal: 853ms\tremaining: 1.23s\n",
      "50:\tlearn: 0.0735534\ttest: 0.0744095\tbest: 0.0744095 (50)\ttotal: 1.04s\tremaining: 997ms\n",
      "60:\tlearn: 0.0612645\ttest: 0.0624434\tbest: 0.0624434 (60)\ttotal: 1.22s\tremaining: 780ms\n",
      "70:\tlearn: 0.0532660\ttest: 0.0544890\tbest: 0.0544890 (70)\ttotal: 1.39s\tremaining: 569ms\n",
      "80:\tlearn: 0.0482388\ttest: 0.0495674\tbest: 0.0495674 (80)\ttotal: 1.57s\tremaining: 367ms\n",
      "90:\tlearn: 0.0436600\ttest: 0.0451158\tbest: 0.0451158 (90)\ttotal: 1.74s\tremaining: 172ms\n",
      "99:\tlearn: 0.0402154\ttest: 0.0417733\tbest: 0.0417733 (99)\ttotal: 1.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.04177330737\n",
      "bestIteration = 99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "model = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = model.predict(X_test)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38287.0     35.0    52.0    89.0   5.0\n",
      "1     42.0  26769.0     7.0     2.0   0.0\n",
      "2    105.0     27.0  6780.0     7.0   0.0\n",
      "3    191.0      2.0     5.0  1789.0   2.0\n",
      "4     36.0      0.0     0.0    11.0  16.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9916777764311396\n",
      "Precision total:  0.923362618106846\n",
      "Recall total:  0.8253437656787028\n",
      "F1 total:  0.8537016873966621\n",
      "BACC total:  0.8253437656787028\n",
      "MCC total:  0.9859174555821406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Catboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_cat\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    cat_acc_00 = Acc\n",
    "    cat_pre_00 = Precision\n",
    "    cat_rec_00 = Recall\n",
    "    cat_f1_00 = F1\n",
    "    cat_bacc_00 = BACC\n",
    "    cat_mcc_00 = MCC\n",
    "\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "# pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2      3    4\n",
      "0  38280.0    127.0    61.0    0.0  0.0\n",
      "1   1473.0  25267.0    80.0    0.0  0.0\n",
      "2    714.0    255.0  5950.0    0.0  0.0\n",
      "3   1774.0     41.0    27.0  147.0  0.0\n",
      "4     59.0      4.0     0.0    0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9378526508571352\n",
      "Precision total:  0.7721771837874601\n",
      "Recall total:  0.5742131235647939\n",
      "F1 total:  0.5921246121434538\n",
      "BACC total:  0.5742131235647939\n",
      "MCC total:  0.8948555711561794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('RF base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_rf\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    # rf_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    rf_acc_00 = Acc\n",
    "    rf_pre_00 = Precision\n",
    "    rf_rec_00 = Recall\n",
    "    rf_f1_00 = F1\n",
    "    rf_bacc_00 = BACC\n",
    "    rf_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1    2       3     4\n",
      "0  38250.0     67.0  6.0   135.0  10.0\n",
      "1   9740.0  15412.0  7.0  1657.0   4.0\n",
      "2   6641.0    151.0  7.0   117.0   3.0\n",
      "3   1781.0    145.0  0.0    61.0   2.0\n",
      "4     33.0      5.0  2.0    22.0   1.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.7235621271495711\n",
      "Precision total:  0.4106268919206144\n",
      "Recall total:  0.3233064278569907\n",
      "F1 total:  0.31726566258400357\n",
      "BACC total:  0.3233064278569907\n",
      "MCC total:  0.5346225034422339\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('DNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_dnn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    dnn_acc_00 = Acc\n",
    "    dnn_pre_00 = Precision\n",
    "    dnn_rec_00 = Recall\n",
    "    dnn_f1_00 = F1\n",
    "    dnn_bacc_00 = BACC\n",
    "    dnn_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2      3     4\n",
      "0  37657.0    121.0   319.0  353.0  18.0\n",
      "1   3037.0  19901.0  3864.0   18.0   0.0\n",
      "2   2985.0    338.0  3595.0    1.0   0.0\n",
      "3    907.0     39.0    53.0  988.0   2.0\n",
      "4     22.0      0.0     0.0   18.0  23.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8371241196353304\n",
      "Precision total:  0.7061401478981317\n",
      "Recall total:  0.6204667135788681\n",
      "F1 total:  0.6515566716001219\n",
      "BACC total:  0.6204667135788681\n",
      "MCC total:  0.72839833087544\n"
     ]
    }
   ],
   "source": [
    "#ADA\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    ada_acc_00 = Acc\n",
    "    ada_pre_00 = Precision\n",
    "    ada_rec_00 = Recall\n",
    "    ada_f1_00 = F1\n",
    "    ada_bacc_00 = BACC\n",
    "    ada_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0        1       2       3     4\n",
      "0  37876.0    207.0   266.0   114.0   5.0\n",
      "1    558.0  26221.0    38.0     3.0   0.0\n",
      "2    278.0    112.0  6493.0    36.0   0.0\n",
      "3    796.0      8.0     5.0  1178.0   2.0\n",
      "4     28.0      2.0     0.0    13.0  20.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.966724572105738\n",
      "Precision total:  0.9034844514210691\n",
      "Recall total:  0.76208492972839\n",
      "F1 total:  0.8102978494437603\n",
      "BACC total:  0.76208492972839\n",
      "MCC total:  0.9435089308967697\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('SVM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_svm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "\n",
    "    svm_acc_00 = Acc\n",
    "    svm_pre_00 = Precision\n",
    "    svm_rec_00 = Recall\n",
    "    svm_f1_00 = F1\n",
    "    svm_bacc_00 = BACC\n",
    "    svm_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0        1       2       3     4\n",
      "0  38187.0     66.0    71.0   131.0  13.0\n",
      "1     36.0  26780.0     4.0     0.0   0.0\n",
      "2    101.0    193.0  6617.0     8.0   0.0\n",
      "3    144.0      8.0     4.0  1832.0   1.0\n",
      "4     20.0      0.0     1.0    10.0  32.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9890787648635182\n",
      "Precision total:  0.9181599328544454\n",
      "Recall total:  0.8753116494139175\n",
      "F1 total:  0.8937521326445188\n",
      "BACC total:  0.8753116494139175\n",
      "MCC total:  0.9815325712570225\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    knn_acc_00 = Acc\n",
    "    knn_pre_00 = Precision\n",
    "    knn_rec_00 = Recall\n",
    "    knn_f1_00 = F1\n",
    "    knn_bacc_00 = BACC\n",
    "    knn_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38213.0     48.0    59.0   141.0   7.0\n",
      "1     28.0  26784.0     7.0     1.0   0.0\n",
      "2     63.0     10.0  6840.0     6.0   0.0\n",
      "3     77.0      2.0     9.0  1896.0   5.0\n",
      "4     13.0      0.0     2.0    14.0  34.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9933745404597423\n",
      "Precision total:  0.9284663650723207\n",
      "Recall total:  0.8947072744209563\n",
      "F1 total:  0.9084214453110192\n",
      "BACC total:  0.8947072744209563\n",
      "MCC total:  0.9888137337493722\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    mlp_acc_00 = Acc\n",
    "    mlp_pre_00 = Precision\n",
    "    mlp_rec_00 = Recall\n",
    "    mlp_f1_00 = F1\n",
    "    mlp_bacc_00 = BACC\n",
    "    mlp_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0        1       2       3     4\n",
      "0  38359.0     13.0    15.0    59.0  22.0\n",
      "1      9.0  26808.0     3.0     0.0   0.0\n",
      "2     25.0      1.0  6892.0     1.0   0.0\n",
      "3     65.0      0.0     2.0  1916.0   6.0\n",
      "4     23.0      0.0     0.0    13.0  27.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9965391400368979\n",
      "Precision total:  0.88952427135883\n",
      "Recall total:  0.8769372638118347\n",
      "F1 total:  0.8828080823417052\n",
      "BACC total:  0.8769372638118347\n",
      "MCC total:  0.9941512096322088\n"
     ]
    }
   ],
   "source": [
    "#lgbm\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_lgbm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    lgbm_acc_00 = Acc\n",
    "    lgbm_pre_00 = Precision\n",
    "    lgbm_rec_00 = Recall\n",
    "    lgbm_f1_00 = F1\n",
    "    lgbm_bacc_00 = BACC\n",
    "    lgbm_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the stronger model - STACK level 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('------------START of STRONGER LEARNER - STACK 01 -----------------', file = f)\n",
    "\n",
    "\n",
    "# Stack the vectors horizontally to create a matrix\n",
    "column_features = ['dnn','rf','lgbm','ada','knn','mlp','svm','cat','xgb','label']\n",
    "training_matrix = np.column_stack((\n",
    "                          preds_dnn,\n",
    "                          preds_rf,\n",
    "                          preds_lgbm,\n",
    "                          preds_ada,\n",
    "                          preds_knn, \n",
    "                          preds_mlp,\n",
    "                          preds_svm,\n",
    "                          preds_cat,\n",
    "                          preds_xgb,\n",
    "                        #   preds\n",
    "                          y_test\n",
    "                          ))\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_level_01 = pd.DataFrame(training_matrix, columns=column_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_level_01.to_csv('models7dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_01 = df_level_01.pop('label')\n",
    "X_01 = df_level_01\n",
    "df_level_01 = df_level_01.assign(label = y_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  ada  knn  mlp  svm  cat  xgb\n",
       "0      0.0  1.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
       "1      0.0  0.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0\n",
       "2      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "...    ...  ...   ...  ...  ...  ...  ...  ...  ...\n",
       "74254  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "74255  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0\n",
       "74256  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "74257  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "74258  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[74259 rows x 9 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.0\n",
       "1        1.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "74254    0.0\n",
       "74255    1.0\n",
       "74256    0.0\n",
       "74257    0.0\n",
       "74258    0.0\n",
       "Name: label, Length: 74259, dtype: float64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  ada  knn  mlp  svm  cat  xgb  label\n",
       "0      0.0  1.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "1      0.0  0.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "2      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "3      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "4      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "...    ...  ...   ...  ...  ...  ...  ...  ...  ...    ...\n",
       "74254  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74255  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "74256  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74257  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74258  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "\n",
       "[74259 rows x 10 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "\n",
    "X_train_01,X_test_01, y_train_01, y_test_01 = sklearn.model_selection.train_test_split(X_01, y_01, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define EarlyStopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Compile the model\n",
    "# # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # Train the model with EarlyStopping callback\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# # Save the trained model\n",
    "# # model.save(\"CNN_CIC_1.h5\")\n",
    "# model = load_model(\"CNN_CIC_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 3)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train_01.shape[1]\n",
    "\n",
    "dnn_01 = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# # Dense layers with dropout\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(3*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn_01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "119/325 [=========>....................] - ETA: 0s - loss: 3.4274 - accuracy: 0.4702"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 1s 3ms/step - loss: 3.0340 - accuracy: 0.5012 - val_loss: 1.2505 - val_accuracy: 0.5006\n",
      "Epoch 2/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 2.0349 - accuracy: 0.5183 - val_loss: 0.8114 - val_accuracy: 0.5007\n",
      "Epoch 3/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.5711 - accuracy: 0.5192 - val_loss: 0.7563 - val_accuracy: 0.5007\n",
      "Epoch 4/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.5217 - accuracy: 0.5165 - val_loss: 0.7593 - val_accuracy: 0.5034\n",
      "Epoch 5/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.5448 - accuracy: 0.5157 - val_loss: 0.7604 - val_accuracy: 0.5047\n",
      "Epoch 6/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.5165 - accuracy: 0.5098 - val_loss: 0.7575 - val_accuracy: 0.5039\n",
      "Epoch 7/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.4519 - accuracy: 0.5161 - val_loss: 0.7576 - val_accuracy: 0.5039\n",
      "Epoch 8/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.4659 - accuracy: 0.5017 - val_loss: 0.7623 - val_accuracy: 0.5038\n",
      "Epoch 9/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.4637 - accuracy: 0.4921 - val_loss: 0.7584 - val_accuracy: 0.5039\n",
      "Epoch 10/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.4691 - accuracy: 0.4925 - val_loss: 0.8558 - val_accuracy: 0.5050\n",
      "Epoch 11/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.4403 - accuracy: 0.5172 - val_loss: 0.8963 - val_accuracy: 0.5122\n",
      "Epoch 12/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.4493 - accuracy: 0.5177 - val_loss: 0.8646 - val_accuracy: 0.5122\n",
      "Epoch 13/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.4074 - accuracy: 0.5163 - val_loss: 0.8349 - val_accuracy: 0.5122\n",
      "Epoch 14/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.3936 - accuracy: 0.5165 - val_loss: 0.7705 - val_accuracy: 0.5122\n",
      "Epoch 15/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.3617 - accuracy: 0.5154 - val_loss: 0.7593 - val_accuracy: 0.5122\n",
      "Epoch 16/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.3468 - accuracy: 0.5164 - val_loss: 0.7459 - val_accuracy: 0.5122\n",
      "Epoch 17/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.3388 - accuracy: 0.5167 - val_loss: 0.7286 - val_accuracy: 0.5124\n",
      "Epoch 18/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.3298 - accuracy: 0.5164 - val_loss: 0.7404 - val_accuracy: 0.5124\n",
      "Epoch 19/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.3073 - accuracy: 0.5159 - val_loss: 0.7340 - val_accuracy: 0.5124\n",
      "Epoch 20/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.3695 - accuracy: 0.5158 - val_loss: 0.7391 - val_accuracy: 0.5122\n",
      "Epoch 21/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.2949 - accuracy: 0.5141 - val_loss: 0.7368 - val_accuracy: 0.5085\n",
      "Epoch 22/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 1.2877 - accuracy: 0.5147 - val_loss: 0.7350 - val_accuracy: 0.5122\n",
      "Epoch 23/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.3116 - accuracy: 0.5152 - val_loss: 0.7552 - val_accuracy: 0.5122\n",
      "Epoch 24/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.2748 - accuracy: 0.5156 - val_loss: 0.7467 - val_accuracy: 0.5122\n",
      "Epoch 25/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.2537 - accuracy: 0.5167 - val_loss: 0.7411 - val_accuracy: 0.5122\n",
      "Epoch 26/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.2271 - accuracy: 0.5179 - val_loss: 0.7355 - val_accuracy: 0.5122\n",
      "Epoch 27/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.2435 - accuracy: 0.5188 - val_loss: 0.7294 - val_accuracy: 0.5050\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training DNN')\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# Convert Y_test back to its original format\n",
    "# y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Start the timer\n",
    "start = time.time()\n",
    "# dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size)\n",
    "dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# End the timer\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "# joblib.dump(dnn_01, 'dnn_level_01.joblib')\n",
    "dnn_01.save(\"dnn_level_01.h5\")\n",
    "\n",
    "# Calculate the time taken and print it out\n",
    "# print(f'Time taken for training: {time_taken} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_01 = load_model(\"dnn_level_01.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "start = time.time()\n",
    "pred_dnn = dnn_01.predict(X_test_01)\n",
    "preds_dnn_01 = np.argmax(pred_dnn,axis = 1)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_test_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "with open(output_file_name, \"a\") as f: print('Stack model - Strong learner - level 01', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0   1.0  2.0  3.0  4.0\n",
      "0.0  11609.0   6.0  0.0  0.0  0.0\n",
      "1.0   7959.0   0.0  0.0  0.0  0.0\n",
      "2.0   2094.0   0.0  0.0  0.0  0.0\n",
      "3.0    585.0  10.0  0.0  0.0  0.0\n",
      "4.0     14.0   1.0  0.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.5215459197414489\n",
      "Precision total:  0.22194605707158654\n",
      "Recall total:  0.2032580298585211\n",
      "F1 total:  0.14361234398116315\n",
      "BACC total:  0.2032580298585211\n",
      "MCC total:  0.018554098155110756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('DNN', file = f)\n",
    "pred_label = preds_dnn_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "dnn_acc_01 = Acc\n",
    "dnn_pre_01 = Precision\n",
    "dnn_rec_01 = Recall\n",
    "dnn_f1_01 = F1\n",
    "dnn_bacc_01 = BACC\n",
    "dnn_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "\n",
    "\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "clf.fit(X_train_01, y_train_01)\n",
    "end = time.time()\n",
    "clf.score(X_train_01, y_train_01)\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "joblib.dump(clf, 'svm_level_01.joblib')\n",
    "\n",
    "\n",
    "clf = loaded_model = joblib.load('svm_level_01.joblib')\n",
    "\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "preds_svm_01 = clf.predict(X_test_01)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11418.0   176.0     8.0   13.0  0.0\n",
      "1.0    270.0  7686.0     3.0    0.0  0.0\n",
      "2.0      7.0   238.0  1848.0    1.0  0.0\n",
      "3.0     16.0    26.0    15.0  538.0  0.0\n",
      "4.0      4.0     5.0     0.0    6.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.964628781757788\n",
      "Precision total:  0.7740405423209356\n",
      "Recall total:  0.7470923105138906\n",
      "F1 total:  0.7597741724657565\n",
      "BACC total:  0.7470923105138906\n",
      "MCC total:  0.939897769553311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('SVM', file = f)\n",
    "pred_label = preds_svm_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "svm_acc_01 = Acc\n",
    "svm_pre_01 = Precision\n",
    "svm_rec_01 = Recall\n",
    "svm_f1_01 = F1\n",
    "svm_bacc_01 = BACC\n",
    "svm_mcc_01 = MCC\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11558.0     8.0     5.0   42.0  2.0\n",
      "1.0      2.0  7956.0     0.0    1.0  0.0\n",
      "2.0      7.0     2.0  2084.0    1.0  0.0\n",
      "3.0      8.0     0.0     0.0  586.0  1.0\n",
      "4.0      5.0     0.0     0.0    5.0  5.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9960050273812731\n",
      "Precision total:  0.9084571986996753\n",
      "Recall total:  0.8616294709366388\n",
      "F1 total:  0.8759639922143867\n",
      "BACC total:  0.8616294709366388\n",
      "MCC total:  0.9932536668955528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if True == True:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf_01 = rf.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(model_rf_01, X_train_01, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf_01, 'rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    model_rf_01  = joblib.load('rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf_01 = model_rf_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('RF', file = f)\n",
    "pred_label = preds_rf_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "rf_acc_01 = Acc\n",
    "rf_pre_01 = Precision\n",
    "rf_rec_01 = Recall\n",
    "rf_f1_01 = F1\n",
    "rf_bacc_01 = BACC\n",
    "rf_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9960050273812731"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11563.0    10.0     3.0   36.0  3.0\n",
      "1.0      2.0  7957.0     0.0    0.0  0.0\n",
      "2.0      7.0     0.0  2087.0    0.0  0.0\n",
      "3.0     12.0     0.0     0.0  580.0  3.0\n",
      "4.0      4.0     0.0     0.0    3.0  8.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9962743513780411\n",
      "Precision total:  0.9007151472768932\n",
      "Recall total:  0.9000104215163407\n",
      "F1 total:  0.9001562009880514\n",
      "BACC total:  0.9000104215163407\n",
      "MCC total:  0.9937039003342494\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    lgbm = joblib.load('lgbm_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm_01 = lgbm.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM', file = f)\n",
    "    pred_label = preds_lgbm_01\n",
    "\n",
    "    # pred_label = ypred\n",
    "    #pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    lgbm_acc_01 = Acc\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    lgbm_acc_01 = Acc\n",
    "    lgbm_pre_01 = Precision\n",
    "    lgbm_rec_01 = Recall\n",
    "    lgbm_f1_01 = F1\n",
    "    lgbm_bacc_01 = BACC\n",
    "    lgbm_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1       2      3    4\n",
      "0  11574.0    10.0     2.0   26.0  3.0\n",
      "1      1.0  7957.0     1.0    0.0  0.0\n",
      "2      7.0     0.0  2087.0    0.0  0.0\n",
      "3     21.0     0.0     2.0  570.0  2.0\n",
      "4      4.0     0.0     0.0    5.0  6.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9962294640452465\n",
      "Precision total:  0.8974770999470078\n",
      "Recall total:  0.8701718205572314\n",
      "F1 total:  0.8815817804624496\n",
      "BACC total:  0.8701718205572314\n",
      "MCC total:  0.9936209878817929\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    MLP = joblib.load('mlp_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test_01)\n",
    "    preds_mlp_01 = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    mlp_acc_01 = Acc\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    mlp_acc_01 = Acc\n",
    "    mlp_pre_01 = Precision\n",
    "    mlp_rec_01 = Recall\n",
    "    mlp_f1_01 = F1\n",
    "    mlp_bacc_01 = BACC\n",
    "    mlp_mcc_01 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11560.0     8.0     4.0   42.0  1.0\n",
      "1.0      8.0  7170.0   781.0    0.0  0.0\n",
      "2.0     52.0    20.0  2022.0    0.0  0.0\n",
      "3.0    200.0     0.0     0.0  394.0  1.0\n",
      "4.0     12.0     0.0     0.0    2.0  1.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9492324266092109\n",
      "Precision total:  0.78526804785664\n",
      "Recall total:  0.7181198546821024\n",
      "F1 total:  0.7262453444043158\n",
      "BACC total:  0.7181198546821024\n",
      "MCC total:  0.9165304396908468\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_01.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    ada = joblib.load('ada_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada_01 = ada.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    ada_acc_01 = Acc\n",
    "    ada_pre_01 = Precision\n",
    "    ada_rec_01 = Recall\n",
    "    ada_f1_01 = F1\n",
    "    ada_bacc_01 = BACC\n",
    "    ada_mcc_01 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11564.0     9.0    10.0   31.0  1.0\n",
      "1.0      2.0  7957.0     0.0    0.0  0.0\n",
      "2.0      9.0     7.0  2074.0    4.0  0.0\n",
      "3.0     14.0     1.0     9.0  571.0  0.0\n",
      "4.0      5.0     0.0     0.0    7.0  3.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9951072807253794\n",
      "Precision total:  0.9335374299506691\n",
      "Recall total:  0.8290941210899355\n",
      "F1 total:  0.8494313296173044\n",
      "BACC total:  0.8290941210899355\n",
      "MCC total:  0.9917264477689346\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf_01, 'knn_01.joblib')\n",
    "\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf_01 = joblib.load('knn_01.joblib')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf_01.predict(X_test_01)\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    knn_acc_01 = Acc\n",
    "    knn_pre_01 = Precision\n",
    "    knn_rec_01 = Recall\n",
    "    knn_f1_01 = F1\n",
    "    knn_bacc_01 = BACC\n",
    "    knn_mcc_01 = MCC    \n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    knn_acc_01 = Acc\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Logistic Regression Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LR \n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11538.0    47.0    10.0   15.0  5.0\n",
      "1.0      5.0  7954.0     0.0    0.0  0.0\n",
      "2.0      7.0    27.0  2040.0   20.0  0.0\n",
      "3.0      8.0    19.0    35.0  531.0  2.0\n",
      "4.0      5.0     1.0     0.0    7.0  2.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.990438998114732\n",
      "Precision total:  0.8226998139890925\n",
      "Recall total:  0.7985449528587178\n",
      "F1 total:  0.8083275690140148\n",
      "BACC total:  0.7985449528587178\n",
      "MCC total:  0.983837069525688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Logistic Regression Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "logreg_01 = LogisticRegression()\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LR ')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LR', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    logreg_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(logreg_01, 'logreg_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    logreg_01 = joblib.load('logreg_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #lR\n",
    "    start = time.time()\n",
    "    preds_logreg =logreg_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#LR\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LR 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_logreg\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    lr_acc_01 = Acc\n",
    "    lr_pre_01 = Precision\n",
    "    lr_rec_01 = Recall\n",
    "    lr_f1_01 = F1\n",
    "    lr_bacc_01 = BACC\n",
    "    lr_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "# # model1 = LogisticRegression(random_state=1)\n",
    "# # model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "# model_voting = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard')\n",
    "# model_voting.fit(X_train_01,y_train_01)\n",
    "# model_voting.score(x_test_01,y_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from scipy.stats import mode\n",
    "\n",
    "# # Sample data\n",
    "# data = {\n",
    "#     'dnn': [0, 2, 0, 0, 4],\n",
    "#     'rf': [0, 1, 0, 1, 1],\n",
    "#     'lgbm': [0, 1, 3, 1, 1],\n",
    "#     'ada': [2, 1, 3, 1, 1],\n",
    "#     'knn': [0, 1, 1, 1, 1],\n",
    "#     'mlp': [0, 1, 2, 1, 1],\n",
    "#     'svm': [0, 3, 0, 3, 1],\n",
    "#     'label': [0, 1, 1, 1, 1],\n",
    "# }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Extract predictions columns\n",
    "# predictions = df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm']]\n",
    "\n",
    "# # Use the mode function along axis 1 to get the most common prediction for each row\n",
    "# ensemble_predictions, _ = mode(predictions.values, axis=1)\n",
    "\n",
    "# # Add the ensemble predictions to the DataFrame\n",
    "# df['ensemble'] = ensemble_predictions.astype(int)\n",
    "\n",
    "# # Display the DataFrame with ensemble predictions\n",
    "# print(df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm', 'ensemble', 'label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dnn   rf  lgbm  ada  knn  mlp  svm  cat  xgb  ensemble\n",
      "29188  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0         0\n",
      "35840  0.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0         1\n",
      "4205   0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0         0\n",
      "68957  0.0  2.0   2.0  0.0  2.0  2.0  2.0  2.0  2.0         2\n",
      "36797  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0         1\n",
      "...    ...  ...   ...  ...  ...  ...  ...  ...  ...       ...\n",
      "60661  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0         0\n",
      "52998  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0         1\n",
      "36272  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0         1\n",
      "70664  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0         0\n",
      "37317  3.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0         1\n",
      "\n",
      "[22278 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Voting start\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Assuming 'df' is your original DataFrame with columns 'dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm', 'label'\n",
    "df = X_test_01\n",
    "# Extract predictions columns\n",
    "predictions = df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm','cat','xgb']]\n",
    "\n",
    "# Use the mode function along axis 1 to get the most common prediction for each row\n",
    "ensemble_predictions, _ = mode(predictions.values, axis=1)\n",
    "\n",
    "# Add the ensemble predictions to the DataFrame\n",
    "df['ensemble'] = ensemble_predictions.astype(int)\n",
    "\n",
    "# Display the DataFrame with ensemble predictions\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ensemble']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1       2      3    4\n",
      "0  11587.0     7.0     5.0   15.0  1.0\n",
      "1      9.0  7950.0     0.0    0.0  0.0\n",
      "2     36.0     8.0  2049.0    1.0  0.0\n",
      "3     77.0     0.0     0.0  517.0  1.0\n",
      "4      8.0     0.0     0.0    3.0  4.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9923242660921088\n",
      "Precision total:  0.9231612803059775\n",
      "Recall total:  0.8221085574337865\n",
      "F1 total:  0.8549709189166815\n",
      "BACC total:  0.8221085574337865\n",
      "MCC total:  0.9869963091845585\n"
     ]
    }
   ],
   "source": [
    "pred_label = df ['ensemble'].values\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Voting', file = f)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "voting_acc_01 = Acc\n",
    "voting_pre_01 = Precision\n",
    "voting_rec_01 = Recall\n",
    "voting_f1_01 = F1\n",
    "voting_bacc_01 = BACC\n",
    "voting_mcc_01 = MCC\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  ada  knn  mlp  svm  cat  xgb  label\n",
       "0      0.0  1.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "1      0.0  0.0   1.0  0.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "2      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "3      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "4      0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "...    ...  ...   ...  ...  ...  ...  ...  ...  ...    ...\n",
       "74254  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74255  1.0  1.0   1.0  1.0  1.0  1.0  1.0  1.0  1.0    1.0\n",
       "74256  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74257  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "74258  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0\n",
       "\n",
       "[74259 rows x 10 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29188    0\n",
       "35840    1\n",
       "4205     0\n",
       "68957    2\n",
       "36797    1\n",
       "        ..\n",
       "60661    0\n",
       "52998    1\n",
       "36272    1\n",
       "70664    0\n",
       "37317    1\n",
       "Name: ensemble, Length: 22278, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_01.pop('ensemble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2362891\ttest: 1.2365986\tbest: 1.2365986 (0)\ttotal: 14.1ms\tremaining: 1.4s\n",
      "10:\tlearn: 0.2974902\ttest: 0.2984363\tbest: 0.2984363 (10)\ttotal: 87.1ms\tremaining: 705ms\n",
      "20:\tlearn: 0.1072990\ttest: 0.1083719\tbest: 0.1083719 (20)\ttotal: 132ms\tremaining: 495ms\n",
      "30:\tlearn: 0.0463146\ttest: 0.0473780\tbest: 0.0473780 (30)\ttotal: 173ms\tremaining: 386ms\n",
      "40:\tlearn: 0.0249767\ttest: 0.0261287\tbest: 0.0261287 (40)\ttotal: 216ms\tremaining: 311ms\n",
      "50:\tlearn: 0.0177232\ttest: 0.0191420\tbest: 0.0191420 (50)\ttotal: 258ms\tremaining: 248ms\n",
      "60:\tlearn: 0.0150523\ttest: 0.0167258\tbest: 0.0167258 (60)\ttotal: 297ms\tremaining: 190ms\n",
      "70:\tlearn: 0.0138309\ttest: 0.0157216\tbest: 0.0157216 (70)\ttotal: 337ms\tremaining: 138ms\n",
      "80:\tlearn: 0.0132669\ttest: 0.0153726\tbest: 0.0153726 (80)\ttotal: 374ms\tremaining: 87.8ms\n",
      "90:\tlearn: 0.0128609\ttest: 0.0151321\tbest: 0.0151321 (90)\ttotal: 414ms\tremaining: 40.9ms\n",
      "99:\tlearn: 0.0125673\ttest: 0.0150051\tbest: 0.0150051 (99)\ttotal: 449ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.01500508785\n",
      "bestIteration = 99\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11563.0     9.0     4.0   38.0  1.0\n",
      "1.0      1.0  7958.0     0.0    0.0  0.0\n",
      "2.0      7.0     0.0  2087.0    0.0  0.0\n",
      "3.0     11.0     0.0     0.0  583.0  1.0\n",
      "4.0      4.0     0.0     0.0    4.0  7.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9964090133764252\n",
      "Precision total:  0.9411100006256058\n",
      "Recall total:  0.8877106203293753\n",
      "F1 total:  0.9065164160049909\n",
      "BACC total:  0.8877106203293753\n",
      "MCC total:  0.993932768638464\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "model_cat_01 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "model_cat_01.fit(X_train_01, y_train_01, eval_set=(X_test_01, y_test_01), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat_01 = model_cat_01.predict(X_test_01)\n",
    "preds_cat_01 = np.squeeze(preds_cat_01)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('catboost 01 ', file = f)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "pred_label = preds_cat_01\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "cat_acc_01 = Acc\n",
    "cat_pre_01 = Precision\n",
    "cat_rec_01 = Recall\n",
    "cat_f1_01 = F1\n",
    "cat_bacc_01 = BACC\n",
    "cat_mcc_01 = MCC\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_xgb_01.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11574.0     9.0     4.0   26.0  2.0\n",
      "1.0      1.0  7958.0     0.0    0.0  0.0\n",
      "2.0      7.0     0.0  2087.0    0.0  0.0\n",
      "3.0     19.0     0.0     0.0  575.0  1.0\n",
      "4.0      5.0     0.0     0.0    2.0  8.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9965885627076039\n",
      "Precision total:  0.9350076836739113\n",
      "Recall total:  0.8985442882778288\n",
      "F1 total:  0.913783558470102\n",
      "BACC total:  0.8985442882778288\n",
      "MCC total:  0.9942288794380022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_01, label=y_train_01)\n",
    "dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb_01 = model.predict(dtest)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    xgb_acc_01 = Acc\n",
    "    xgb_pre_01 = Precision\n",
    "    xgb_rec_01 = Recall\n",
    "    xgb_f1_01 = F1\n",
    "    xgb_bacc_01 = BACC\n",
    "    xgb_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy cat: ', cat_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision cat: ', cat_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgb: ', xgb_pre_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall cat: ', cat_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgb: ', xgb_rec_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 cat: ', cat_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgb: ', xgb_f1_00, file = f)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voting_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision LR: ', lr_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision Voting: ', voting_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision catboosting: ', cat_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgboost: ', xgb_pre_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall LR: ', lr_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall Voting: ', voting_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall catboosting: ', cat_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgboost: ', xgb_rec_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 LR: ', lr_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 Voting: ', voting_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 catboosting: ', cat_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgboost: ', xgb_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc_00 = 0 \n",
    "voting_acc_00 = 0\n",
    "\n",
    "lr_pre_00 = 0 \n",
    "voting_pre_00 = 0\n",
    "\n",
    "lr_rec_00 = 0 \n",
    "voting_rec_00 = 0\n",
    "\n",
    "lr_f1_00 = 0 \n",
    "voting_f1_00 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY WITH LINEAR REGRESSION AND VOTING\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# split = 0.7\n",
    "\n",
    "# #AUC ROC\n",
    "# #---------------------------------------------------------------------\n",
    "\n",
    "# #AUCROC\n",
    "# aucroc =[]\n",
    "# y_array = [y_0,y_1,y_2,y_3,y_4]\n",
    "# for j in range(0,len(y_array)):\n",
    "#     # print(j)\n",
    "#     #------------------------------------------------------------------------------------------------------------\n",
    "#     X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y_array[j], train_size=split)\n",
    "    \n",
    "#     # evaluate the model\n",
    "\n",
    "#     knn_clf.fit(X_train,y_train)\n",
    "#     y_pred=knn_clf.predict(X_test) #These are the predicted output value\n",
    "#     # y_pred = knn_clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "\n",
    "#     # model = LGBMClassifier()\n",
    "#     # model.fit(X_train, y_train)\n",
    "#     # y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "    \n",
    "#     # Calculate AUC-ROC score\n",
    "#     auc_roc_score= roc_auc_score(y_true, y_scores,  average='weighted')  # Use 'micro' or 'macro' for different averaging strategies\n",
    "#     # print(\"AUC-ROC Score class:\", auc_roc_score)\n",
    "#     aucroc.append(auc_roc_score)\n",
    "#     #-------------------------------------------------------------------------------------------------------    -----\n",
    "#     # Calculate the average\n",
    "# average = sum(aucroc) / len(aucroc)\n",
    "\n",
    "# # Display the result\n",
    "# # with open(output_file_name, \"a\") as f:print(\"AUC ROC Average:\", average, file = f)\n",
    "# print(\"AUC ROC Average:\", average)\n",
    "\n",
    "# #End AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "| Accuracy   | Level 00           | Level 01           |\n",
      "+============+====================+====================+\n",
      "| ADA        | 0.8371241196353304 | 0.9492324266092109 |\n",
      "+------------+--------------------+--------------------+\n",
      "| SVM        | 0.966724572105738  | 0.964628781757788  |\n",
      "+------------+--------------------+--------------------+\n",
      "| DNN        | 0.7235621271495711 | 0.5215459197414489 |\n",
      "+------------+--------------------+--------------------+\n",
      "| MLP        | 0.9933745404597423 | 0.9962294640452465 |\n",
      "+------------+--------------------+--------------------+\n",
      "| KNN        | 0.9890787648635182 | 0.9951072807253794 |\n",
      "+------------+--------------------+--------------------+\n",
      "| CAT        | 0.9916777764311396 | 0.9964090133764252 |\n",
      "+------------+--------------------+--------------------+\n",
      "| XGB        | 0.9924049610148264 | 0.9965885627076039 |\n",
      "+------------+--------------------+--------------------+\n",
      "| LGBM       | 0.9965391400368979 | 0.9962743513780411 |\n",
      "+------------+--------------------+--------------------+\n",
      "| RF         | 0.9378526508571352 | 0.9960050273812731 |\n",
      "+------------+--------------------+--------------------+\n",
      "| LR         | 0                  | 0.990438998114732  |\n",
      "+------------+--------------------+--------------------+\n",
      "| VOTING     | 0                  | 0.9923242660921088 |\n",
      "+------------+--------------------+--------------------+\n",
      "|            |                    |                    |\n",
      "+------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_acc = [ada_acc_00,\n",
    "                svm_acc_00,\n",
    "                dnn_acc_00,\n",
    "                mlp_acc_00,\n",
    "                knn_acc_00,\n",
    "                cat_acc_00,\n",
    "                xgb_acc_00,\n",
    "                lgbm_acc_00,\n",
    "                rf_acc_00,\n",
    "                lr_acc_00,\n",
    "                voting_acc_00]  \n",
    "level_01_acc = [ada_acc_01,\n",
    "                svm_acc_01,\n",
    "                dnn_acc_01,\n",
    "                mlp_acc_01,\n",
    "                knn_acc_01,\n",
    "                cat_acc_01,\n",
    "                xgb_acc_01,\n",
    "                lgbm_acc_01,\n",
    "                rf_acc_01,\n",
    "                lr_acc_01,\n",
    "                voting_acc_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Accuracy\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+---------------------+\n",
      "| Precision   | Level 00           | Level 01            |\n",
      "+=============+====================+=====================+\n",
      "| ADA         | 0.7061401478981317 | 0.78526804785664    |\n",
      "+-------------+--------------------+---------------------+\n",
      "| SVM         | 0.9034844514210691 | 0.7740405423209356  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| DNN         | 0.4106268919206144 | 0.22194605707158654 |\n",
      "+-------------+--------------------+---------------------+\n",
      "| MLP         | 0.9284663650723207 | 0.8974770999470078  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| KNN         | 0.9181599328544454 | 0.9335374299506691  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| CAT         | 0.923362618106846  | 0.9411100006256058  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| XGB         | 0.9346385618670787 | 0.9350076836739113  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| LGBM        | 0.88952427135883   | 0.9007151472768932  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| RF          | 0.7721771837874601 | 0.9084571986996753  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| LR          | 0                  | 0.8226998139890925  |\n",
      "+-------------+--------------------+---------------------+\n",
      "| VOTING      | 0                  | 0.9231612803059775  |\n",
      "+-------------+--------------------+---------------------+\n",
      "|             |                    |                     |\n",
      "+-------------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_pre = [ada_pre_00,\n",
    "                svm_pre_00,\n",
    "                dnn_pre_00,\n",
    "                mlp_pre_00,\n",
    "                knn_pre_00,\n",
    "                cat_pre_00,\n",
    "                xgb_pre_00,\n",
    "                lgbm_pre_00,\n",
    "                rf_pre_00,\n",
    "                lr_pre_00,\n",
    "                voting_pre_00]  \n",
    "level_01_pre = [ada_pre_01,\n",
    "                svm_pre_01,\n",
    "                dnn_pre_01,\n",
    "                mlp_pre_01,\n",
    "                knn_pre_01,\n",
    "                cat_pre_01,\n",
    "                xgb_pre_01,\n",
    "                lgbm_pre_01,\n",
    "                rf_pre_01,\n",
    "                lr_pre_01,\n",
    "                voting_pre_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_pre[i]\n",
    "    data[i][2] = level_01_pre[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Precision\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "| Recall   | Level 00           | Level 01           |\n",
      "+==========+====================+====================+\n",
      "| ADA      | 0.6204667135788681 | 0.7181198546821024 |\n",
      "+----------+--------------------+--------------------+\n",
      "| SVM      | 0.76208492972839   | 0.7470923105138906 |\n",
      "+----------+--------------------+--------------------+\n",
      "| DNN      | 0.3233064278569907 | 0.2032580298585211 |\n",
      "+----------+--------------------+--------------------+\n",
      "| MLP      | 0.8947072744209563 | 0.8701718205572314 |\n",
      "+----------+--------------------+--------------------+\n",
      "| KNN      | 0.8753116494139175 | 0.8290941210899355 |\n",
      "+----------+--------------------+--------------------+\n",
      "| CAT      | 0.8253437656787028 | 0.8877106203293753 |\n",
      "+----------+--------------------+--------------------+\n",
      "| XGB      | 0.8769168240779491 | 0.8985442882778288 |\n",
      "+----------+--------------------+--------------------+\n",
      "| LGBM     | 0.8769372638118347 | 0.9000104215163407 |\n",
      "+----------+--------------------+--------------------+\n",
      "| RF       | 0.5742131235647939 | 0.8616294709366388 |\n",
      "+----------+--------------------+--------------------+\n",
      "| LR       | 0                  | 0.7985449528587178 |\n",
      "+----------+--------------------+--------------------+\n",
      "| VOTING   | 0                  | 0.8221085574337865 |\n",
      "+----------+--------------------+--------------------+\n",
      "|          |                    |                    |\n",
      "+----------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_rec = [ada_rec_00,\n",
    "                svm_rec_00,\n",
    "                dnn_rec_00,\n",
    "                mlp_rec_00,\n",
    "                knn_rec_00,\n",
    "                cat_rec_00,\n",
    "                xgb_rec_00,\n",
    "                lgbm_rec_00,\n",
    "                rf_rec_00,\n",
    "                lr_rec_00,\n",
    "                voting_rec_00]  \n",
    "level_01_rec = [ada_rec_01,\n",
    "                svm_rec_01,\n",
    "                dnn_rec_01,\n",
    "                mlp_rec_01,\n",
    "                knn_rec_01,\n",
    "                cat_rec_01,\n",
    "                xgb_rec_01,\n",
    "                lgbm_rec_01,\n",
    "                rf_rec_01,\n",
    "                lr_rec_01,\n",
    "                voting_rec_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_rec[i]\n",
    "    data[i][2] = level_01_rec[i]\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Recall\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+---------------------+\n",
      "| F1     | Level 00            | Level 01            |\n",
      "+========+=====================+=====================+\n",
      "| ADA    | 0.6515566716001219  | 0.7262453444043158  |\n",
      "+--------+---------------------+---------------------+\n",
      "| SVM    | 0.8102978494437603  | 0.7597741724657565  |\n",
      "+--------+---------------------+---------------------+\n",
      "| DNN    | 0.31726566258400357 | 0.14361234398116315 |\n",
      "+--------+---------------------+---------------------+\n",
      "| MLP    | 0.9084214453110192  | 0.8815817804624496  |\n",
      "+--------+---------------------+---------------------+\n",
      "| KNN    | 0.8937521326445188  | 0.8494313296173044  |\n",
      "+--------+---------------------+---------------------+\n",
      "| CAT    | 0.8537016873966621  | 0.9065164160049909  |\n",
      "+--------+---------------------+---------------------+\n",
      "| XGB    | 0.8988820677016778  | 0.913783558470102   |\n",
      "+--------+---------------------+---------------------+\n",
      "| LGBM   | 0.8828080823417052  | 0.9001562009880514  |\n",
      "+--------+---------------------+---------------------+\n",
      "| RF     | 0.5921246121434538  | 0.8759639922143867  |\n",
      "+--------+---------------------+---------------------+\n",
      "| LR     | 0                   | 0.8083275690140148  |\n",
      "+--------+---------------------+---------------------+\n",
      "| VOTING | 0                   | 0.8549709189166815  |\n",
      "+--------+---------------------+---------------------+\n",
      "|        |                     |                     |\n",
      "+--------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_f1[i]\n",
    "    data[i][2] = level_01_f1[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"F1\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ada_f1_00' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa34ea8292e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;34m'VOTING'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 ]\n\u001b[0;32m---> 28\u001b[0;31m level_00_f1 = [ada_f1_00,\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0msvm_f1_00\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mdnn_f1_00\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ada_f1_00' is not defined"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(9)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "    data[i][3] = level_00_pre[i] \n",
    "    data[i][4] = level_01_pre[i]\n",
    "\n",
    "    data[i][5] = level_00_rec[i] \n",
    "    data[i][6] = level_01_rec[i]\n",
    "\n",
    "    data[i][7] = level_00_f1[i]\n",
    "    data[i][8] = level_01_f1[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Models\", \"ACC-00\", \" ACC-01\",\"PRE-00\", \" PRE-01\",\"REC-00\", \" REC-01\",\"F1-00\", \" F1-01\",]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
