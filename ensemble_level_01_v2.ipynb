{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensemble with NSL-KDD\n",
    "# Parameters\n",
    "# Few parameters are not fully implemented yet\n",
    "\n",
    "#----------------------------------------------\n",
    "# 0 for not using it as base learner\n",
    "# 1 for using it as base learner\n",
    "# not implemented but in the code in someparts\n",
    "use_model_ada = 1 \n",
    "use_model_dnn = 1 \n",
    "use_model_mlp = 1 \n",
    "use_model_lgbm = 1 \n",
    "use_model_rf = 1 \n",
    "use_model_svm = 1\n",
    "use_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "# 0 for training the model\n",
    "# 1 for using the saved version of the model\n",
    "\n",
    "# load_model_ada = 0 \n",
    "# load_model_dnn = 0 \n",
    "# load_model_mlp = 0 \n",
    "# load_model_lgbm = 0 \n",
    "# load_model_rf = 0 \n",
    "# load_model_svm = 0\n",
    "# load_model_knn = 0 \n",
    "#----------------------------------------------\n",
    "# not implemented but in the code in someparts\n",
    "load_model_ada = 1\n",
    "load_model_dnn = 1 \n",
    "load_model_mlp = 1 \n",
    "load_model_lgbm = 1 \n",
    "load_model_rf = 1                               \n",
    "load_model_svm = 1\n",
    "load_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "\n",
    "# Implemented\n",
    "#----------------------------------------------\n",
    "# feature_selection_bit = 0 # OFF\n",
    "feature_selection_bit = 1 # On\n",
    "# pick_prob = 1 # set equal one to choose the dataset with probabilities, set to 0 to choose one with the classes.\n",
    "pick_prob = 0\n",
    "generate_feature_importance = 0 # Generate Shap graphs\n",
    "\n",
    "\n",
    "column_features = [\n",
    "                    # 'dnn',\n",
    "                #    'rf',\n",
    "                   'lgbm',\n",
    "                #    'ada',\n",
    "                   'knn',\n",
    "                   'mlp',\n",
    "                   'svm',\n",
    "                #    'cat',\n",
    "                #    'xgb',\n",
    "                   'lr',\n",
    "                   'dt',\n",
    "                   'label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the output text file\n",
    "if feature_selection_bit == 0:\n",
    "\n",
    "    if pick_prob == 0:\n",
    "        output_file_name = \"ensemble_level_01_all_features_classes.txt\"\n",
    "        with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "        with open(output_file_name, \"a\") as f: print('----ensemble_level_01_all_features_classes--', file = f)\n",
    "\n",
    "    elif pick_prob == 1:\n",
    "        output_file_name = \"ensemble_level_01_all_features_probabilites.txt\"\n",
    "        with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "        with open(output_file_name, \"a\") as f: print('----ensemble_level_01_all_features_probabilites--', file = f)\n",
    "\n",
    "elif feature_selection_bit == 1:\n",
    "    if pick_prob == 0:\n",
    "        output_file_name = \"ensemble_level_01_feature_selection_classes.txt\"\n",
    "        with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "        with open(output_file_name, \"a\") as f: print('----ensemble_level_01_feature_selection_classes--', file = f)\n",
    "    elif pick_prob == 1:\n",
    "        output_file_name = \"ensemble_level_01_feature_selection_probabilites.txt\"\n",
    "        with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "        with open(output_file_name, \"a\") as f: print('----ensemble_level_01_feature_selection_probabilites--', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from tabulate import tabulate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "import time\n",
    "start_program = time.time()\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from tabulate import tabulate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def confusion_metrics (name_model,predictions,true_labels):\n",
    "\n",
    "    name = name_model\n",
    "    pred_label = predictions\n",
    "    y_test_01 = true_labels \n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(name, file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    # voting_acc_01 = Acc\n",
    "    # voting_pre_01 = Precision\n",
    "    # weighed_avg_rec_01 = Recall\n",
    "    # weighed_avg_f1_01 = F1\n",
    "    # weighed_avg_bacc_01 = BACC\n",
    "    # weighed_avg_mcc_01 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "    return Acc, Precision, Recall, F1, BACC, MCC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your DataFrame\n",
    "# if feature_selection_bit == 1:\n",
    "#     df_level_00_1=pd.read_csv('base_models_prob_feature_selection.csv',names=column_features)\n",
    "#     df_level_00_0=pd.read_csv('base_models_class_feature_selection.csv',names=column_features)\n",
    "\n",
    "# if feature_selection_bit == 0:\n",
    "\n",
    "#     df_level_00_1=pd.read_csv('base_models_prob_feature_selection.csv')\n",
    "#     df_level_00_0=pd.read_csv('base_models_class_feature_selection.csv')\n",
    "\n",
    "df_level_00_1=pd.read_csv('base_models_prob_feature_selection.csv')\n",
    "df_level_00_0=pd.read_csv('base_models_class_feature_selection.csv')\n",
    "\n",
    "if feature_selection_bit == 1:\n",
    "    df_level_00_0 = df_level_00_0[column_features]\n",
    "    df_level_00_1 = df_level_00_1[column_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.184959</td>\n",
       "      <td>0.995664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.263317</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.999541</td>\n",
       "      <td>0.994737</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.156495</td>\n",
       "      <td>0.952385</td>\n",
       "      <td>0.997448</td>\n",
       "      <td>0.957163</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.361415</td>\n",
       "      <td>0.773010</td>\n",
       "      <td>0.908986</td>\n",
       "      <td>0.795008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.176913</td>\n",
       "      <td>0.995534</td>\n",
       "      <td>0.999906</td>\n",
       "      <td>0.995351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44551</th>\n",
       "      <td>2.543204</td>\n",
       "      <td>0.982695</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.993117</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44552</th>\n",
       "      <td>2.522557</td>\n",
       "      <td>0.991618</td>\n",
       "      <td>0.999552</td>\n",
       "      <td>0.993117</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>0.189286</td>\n",
       "      <td>0.995664</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.995351</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44554</th>\n",
       "      <td>0.030160</td>\n",
       "      <td>0.791277</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986277</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44555</th>\n",
       "      <td>2.552334</td>\n",
       "      <td>0.982695</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.992599</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44556 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dnn        rf      lgbm       xgb  label\n",
       "0      0.184959  0.995664  1.000000  0.995351    1.0\n",
       "1      1.263317  0.901483  0.999541  0.994737    0.0\n",
       "2      2.156495  0.952385  0.997448  0.957163    0.0\n",
       "3      2.361415  0.773010  0.908986  0.795008    0.0\n",
       "4      0.176913  0.995534  0.999906  0.995351    1.0\n",
       "...         ...       ...       ...       ...    ...\n",
       "44551  2.543204  0.982695  0.999375  0.993117    0.0\n",
       "44552  2.522557  0.991618  0.999552  0.993117    0.0\n",
       "44553  0.189286  0.995664  1.000000  0.995351    1.0\n",
       "44554  0.030160  0.791277  1.000000  0.986277    2.0\n",
       "44555  2.552334  0.982695  0.999449  0.992599    0.0\n",
       "\n",
       "[44556 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_00_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44551</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44552</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44553</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44554</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44555</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44556 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  xgb  label\n",
       "0      1.0  1.0   1.0  1.0    1.0\n",
       "1      0.0  0.0   0.0  0.0    0.0\n",
       "2      0.0  0.0   0.0  0.0    0.0\n",
       "3      0.0  0.0   0.0  0.0    0.0\n",
       "4      1.0  1.0   1.0  1.0    1.0\n",
       "...    ...  ...   ...  ...    ...\n",
       "44551  0.0  0.0   0.0  0.0    0.0\n",
       "44552  0.0  0.0   0.0  0.0    0.0\n",
       "44553  1.0  1.0   1.0  1.0    1.0\n",
       "44554  1.0  2.0   2.0  2.0    2.0\n",
       "44555  0.0  0.0   0.0  0.0    0.0\n",
       "\n",
       "[44556 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_00_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pick_prob == 1:\n",
    "    df_level_01 = df_level_00_1\n",
    "else: \n",
    "    df_level_01 = df_level_00_0\n",
    "\n",
    "\n",
    "y_01 = df_level_01.pop('label')\n",
    "X_01 = df_level_01\n",
    "df_level_01 = df_level_01.assign(label = y_01)\n",
    "\n",
    "\n",
    "split = 0.7\n",
    "X_train_01,X_test_01, y_train_01, y_test_01 = sklearn.model_selection.train_test_split(X_01, y_01, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_level_02 = pd.read_csv('base_models_class_feature_selection.csv')\n",
    "\n",
    "# df_level_02\n",
    "\n",
    "# y_02 = df_level_02.pop('label')\n",
    "# X_02 = df_level_02\n",
    "# df_level_02 = df_level_02.assign(label = y_01)\n",
    "\n",
    "\n",
    "# split = 0.7\n",
    "# X_train_02,X_test_02, y_train_02, y_test_02 = sklearn.model_selection.train_test_split(X_02, y_02, train_size=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the stronger model - STACK level 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "with open(output_file_name, \"a\") as f: print('Stack model - Strong learner - level 01', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19464</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11947</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23727</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3986</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34708</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7738</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36443</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16723</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10901</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13367 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  xgb\n",
       "19464  0.0  0.0   0.0  0.0\n",
       "11947  1.0  0.0   2.0  0.0\n",
       "23727  1.0  1.0   1.0  1.0\n",
       "3986   1.0  1.0   1.0  1.0\n",
       "34708  1.0  2.0   2.0  2.0\n",
       "...    ...  ...   ...  ...\n",
       "12997  0.0  0.0   0.0  0.0\n",
       "7738   1.0  1.0   1.0  1.0\n",
       "36443  0.0  2.0   2.0  2.0\n",
       "16723  0.0  0.0   0.0  0.0\n",
       "10901  0.0  0.0   0.0  2.0\n",
       "\n",
       "[13367 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dnn   rf  lgbm  xgb  ensemble\n",
      "19464  0.0  0.0   0.0  0.0         0\n",
      "11947  1.0  0.0   2.0  0.0         0\n",
      "23727  1.0  1.0   1.0  1.0         1\n",
      "3986   1.0  1.0   1.0  1.0         1\n",
      "34708  1.0  2.0   2.0  2.0         2\n",
      "...    ...  ...   ...  ...       ...\n",
      "12997  0.0  0.0   0.0  0.0         0\n",
      "7738   1.0  1.0   1.0  1.0         1\n",
      "36443  0.0  2.0   2.0  2.0         2\n",
      "16723  0.0  0.0   0.0  0.0         0\n",
      "10901  0.0  0.0   0.0  2.0         0\n",
      "\n",
      "[13367 rows x 5 columns]\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0   3.0  4.0\n",
      "0.0  6890.0    25.0    28.0   4.0  0.0\n",
      "1.0   298.0  4498.0     8.0   0.0  0.0\n",
      "2.0    97.0    89.0  1084.0   3.0  0.0\n",
      "3.0   304.0     0.0     5.0  26.0  0.0\n",
      "4.0     7.0     0.0     0.0   1.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9349891523902147\n",
      "Precision total:  0.722119931345455\n",
      "Recall total:  0.571448371021708\n",
      "F1 total:  0.589587623378901\n",
      "BACC total:  0.571448371021708\n",
      "MCC total:  0.8894884988293867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "    \n",
    "if pick_prob == 0:\n",
    "    # Voting start\n",
    "\n",
    "    import pandas as pd\n",
    "    from scipy.stats import mode\n",
    "\n",
    "    # Assuming 'df' is your original DataFrame with columns 'dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm', 'label'\n",
    "    df = X_test_01\n",
    "    # Extract predictions columns\n",
    "    \n",
    "    # predictions = df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm','cat','xgb']]\n",
    "        # selected_columns = df.loc[:, ~df.columns.isin(['rf'])]\n",
    "    predictions = df.loc[:, ~df.columns.isin(['label'])] #df[column_features]\n",
    "\n",
    "    # Use the mode function along axis 1 to get the most common prediction for each row\n",
    "    ensemble_predictions, _ = mode(predictions.values, axis=1)\n",
    "\n",
    "    # Add the ensemble predictions to the DataFrame\n",
    "    df['ensemble'] = ensemble_predictions.astype(int)\n",
    "\n",
    "    # Display the DataFrame with ensemble predictions\n",
    "    print(df)\n",
    "\n",
    "    pred_label = df ['ensemble'].values\n",
    "    df.pop('ensemble')\n",
    "\n",
    "    #testing metrics def\n",
    "    name = 'voting'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "   \n",
    "else:\n",
    "    name = 'voting'\n",
    "    globals()[f\"{name}_acc_01\"] = 0\n",
    "    globals()[f\"{name}_pre_01\"] = 0\n",
    "    globals()[f\"{name}_rec_01\"] = 0\n",
    "    globals()[f\"{name}_f1_01\"] = 0\n",
    "    globals()[f\"{name}_bacc_01\"] = 0\n",
    "    globals()[f\"{name}_mcc_01\"] = 0\n",
    "    globals()[f\"{name}_time_01\"] = 9999\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9349891523902147"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_acc_01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dnn   rf  lgbm  xgb  results\n",
      "19464  0.0  0.0   0.0  0.0        0\n",
      "11947  1.0  0.0   2.0  0.0        1\n",
      "23727  1.0  1.0   1.0  1.0        1\n",
      "3986   1.0  1.0   1.0  1.0        1\n",
      "34708  1.0  2.0   2.0  2.0        2\n",
      "...    ...  ...   ...  ...      ...\n",
      "12997  0.0  0.0   0.0  0.0        0\n",
      "7738   1.0  1.0   1.0  1.0        1\n",
      "36443  0.0  2.0   2.0  2.0        2\n",
      "16723  0.0  0.0   0.0  0.0        0\n",
      "10901  0.0  0.0   0.0  2.0        0\n",
      "\n",
      "[13367 rows x 5 columns]\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0  3.0  4.0\n",
      "0.0  6769.0   125.0    53.0  0.0  0.0\n",
      "1.0   280.0  4506.0    18.0  0.0  0.0\n",
      "2.0    57.0    73.0  1143.0  0.0  0.0\n",
      "3.0    34.0    97.0   204.0  0.0  0.0\n",
      "4.0     3.0     0.0     5.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9290042642328121\n",
      "Precision total:  0.5378856244332432\n",
      "Recall total:  0.562044962945871\n",
      "F1 total:  0.549401489962783\n",
      "BACC total:  0.562044962945871\n",
      "MCC total:  0.8790589599857936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# if pick_prob == 0:\n",
    "if 0 == 0:\n",
    "    # Average start\n",
    "\n",
    "    import pandas as pd\n",
    "    from scipy.stats import mode\n",
    "\n",
    "    # Assuming 'df' is your original DataFrame with columns 'dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm', 'label'\n",
    "    df = X_test_01\n",
    "    predictions = df.loc[:, ~df.columns.isin(['label'])] #df[column_features]\n",
    "\n",
    "   \n",
    "\n",
    "    column_sums = df.sum(axis=1)\n",
    "    row_average = df.mean(axis=1)\n",
    "\n",
    "    # Approximate the result to the closest integer\n",
    "    rounded_average = row_average.round().astype(int)\n",
    "\n",
    "    # print(rounded_average)\n",
    "\n",
    "    df['results'] = rounded_average\n",
    "    print(df)\n",
    " \n",
    "    pred_label = df ['results'].values\n",
    "\n",
    "    # pred_label = df ['ensemble'].values\n",
    "    # df.pop('ensemble')\n",
    "    df.pop('results')\n",
    "\n",
    "    # df.pop('column_sums')\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    name = 'avg'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    \n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighed Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_features\n",
    "# move this up with column_features\n",
    "\n",
    "#important update this as you need to select the important features,\n",
    "#  the left is the least important while the right is the most important\n",
    "# needs automation\n",
    "feature_selection_columns_in_order_of_importance = column_features[:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n",
      "[0.0, 0.3333333333333333, 0.6666666666666666, 1.0]\n",
      "       xgb  lgbm   rf  dnn\n",
      "19464  0.0   0.0  0.0  0.0\n",
      "11947  0.0   2.0  0.0  1.0\n",
      "23727  1.0   1.0  1.0  1.0\n",
      "3986   1.0   1.0  1.0  1.0\n",
      "34708  2.0   2.0  2.0  1.0\n",
      "...    ...   ...  ...  ...\n",
      "12997  0.0   0.0  0.0  0.0\n",
      "7738   1.0   1.0  1.0  1.0\n",
      "36443  2.0   2.0  2.0  0.0\n",
      "16723  0.0   0.0  0.0  0.0\n",
      "10901  2.0   0.0  0.0  0.0\n",
      "\n",
      "[13367 rows x 4 columns]\n",
      "19464    0.000000\n",
      "11947    0.833333\n",
      "23727    1.000000\n",
      "3986     1.000000\n",
      "34708    1.500000\n",
      "           ...   \n",
      "12997    0.000000\n",
      "7738     1.000000\n",
      "36443    1.000000\n",
      "16723    0.000000\n",
      "10901    0.000000\n",
      "Length: 13367, dtype: float64\n",
      "19464    0\n",
      "11947    1\n",
      "23727    1\n",
      "3986     1\n",
      "34708    2\n",
      "        ..\n",
      "12997    0\n",
      "7738     1\n",
      "36443    1\n",
      "16723    0\n",
      "10901    0\n",
      "Length: 13367, dtype: int64\n",
      "       xgb  lgbm   rf  dnn  results\n",
      "19464  0.0   0.0  0.0  0.0        0\n",
      "11947  0.0   2.0  0.0  1.0        1\n",
      "23727  1.0   1.0  1.0  1.0        1\n",
      "3986   1.0   1.0  1.0  1.0        1\n",
      "34708  2.0   2.0  2.0  1.0        2\n",
      "...    ...   ...  ...  ...      ...\n",
      "12997  0.0   0.0  0.0  0.0        0\n",
      "7738   1.0   1.0  1.0  1.0        1\n",
      "36443  2.0   2.0  2.0  0.0        1\n",
      "16723  0.0   0.0  0.0  0.0        0\n",
      "10901  2.0   0.0  0.0  0.0        0\n",
      "\n",
      "[13367 rows x 5 columns]\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0    2.0  3.0  4.0\n",
      "0.0  6857.0    84.0    6.0  0.0  0.0\n",
      "1.0   411.0  4392.0    1.0  0.0  0.0\n",
      "2.0    89.0   608.0  576.0  0.0  0.0\n",
      "3.0   304.0    28.0    3.0  0.0  0.0\n",
      "4.0     6.0     2.0    0.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8846412807660656\n",
      "Precision total:  0.5472213002950794\n",
      "Recall total:  0.47075147443392507\n",
      "F1 total:  0.48875313090583\n",
      "BACC total:  0.47075147443392507\n",
      "MCC total:  0.8003783517601386\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "# if pick_prob == 0:\n",
    "if 0 == 0:\n",
    "    # Average start\n",
    "\n",
    "    import pandas as pd\n",
    "    from scipy.stats import mode\n",
    "\n",
    "    # Assuming 'df' is your original DataFrame with columns 'dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm', 'label'\n",
    "    # df = X_test_01\n",
    "    df = X_test_01[feature_selection_columns_in_order_of_importance]\n",
    "    # Extract predictions columns\n",
    "    \n",
    "    # predictions = df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm','cat','xgb']]\n",
    "        # selected_columns = df.loc[:, ~df.columns.isin(['rf'])]\n",
    "    predictions = df.loc[:, ~df.columns.isin(['label'])] #df[column_features]\n",
    "\n",
    "    # weight\n",
    "    weights_values = []\n",
    "\n",
    "    # linear weight distribution\n",
    "    for i in range(0,len(~df.columns.isin(['label']))):\n",
    "        weights_values.append(i/(len(~df.columns.isin(['label']))-1))\n",
    "    print(weights_values)\n",
    "    # weights_values = [10,3,2,2.3]\n",
    "    print(weights_values)\n",
    "    print(df)\n",
    "    weighted_average = df.multiply(weights_values).sum(axis=1) / sum(weights_values)\n",
    "    print(weighted_average)\n",
    "    # Approximate the result to the closest integer\n",
    "    rounded_weighted_average = weighted_average.round().astype(int)\n",
    "\n",
    "    print(rounded_weighted_average)\n",
    "\n",
    "    # print(rounded_average)\n",
    "\n",
    "    df['results'] = rounded_weighted_average\n",
    "    print(df)\n",
    " \n",
    "    pred_label = df ['results'].values\n",
    "\n",
    "    # pred_label = df ['ensemble'].values\n",
    "    # df.pop('ensemble')\n",
    "    df.pop('results')\n",
    "\n",
    "    # df.pop('column_sums')\n",
    "\n",
    "    #testing metrics def\n",
    "    name = 'weighed_avg'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging  with DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6810.0    43.0    39.0   53.0  2.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    33.0     8.0  1228.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9768085583900651\n",
      "Precision total:  0.7478320419040972\n",
      "Recall total:  0.7584424852612487\n",
      "F1 total:  0.7529902813154326\n",
      "BACC total:  0.7584424852612487\n",
      "MCC total:  0.9608099817231377\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "start = time.time()\n",
    "base_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# accuracy = accuracy_score(y_test_01, y_pred)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_dt'\n",
    "pred_label = y_pred\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging  with SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6775.0    68.0    30.0   74.0  0.0\n",
      "1.0   404.0  4368.0    22.0   10.0  0.0\n",
      "2.0    36.0    83.0  1097.0   57.0  0.0\n",
      "3.0    41.0     0.0    31.0  263.0  0.0\n",
      "4.0     2.0     1.0     1.0    4.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9353632079000523\n",
      "Precision total:  0.6946612204175527\n",
      "Recall total:  0.7062603896481012\n",
      "F1 total:  0.6985635445680366\n",
      "BACC total:  0.7062603896481012\n",
      "MCC total:  0.8907428560226828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "## bagging  with SVM\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "svm_01 = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "# # Define the base classifier (Decision Tree in this case)\n",
    "base_classifier = svm_01\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_svm'\n",
    "pred_label = y_pred\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging with DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# #Model Parameters\n",
    "# dropout_rate = 0.2\n",
    "# nodes = 3\n",
    "# out_layer = 5\n",
    "# optimizer='adam'\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "# epochs=100\n",
    "# batch_size=128\n",
    "\n",
    "\n",
    "# num_columns = X_train_01.shape[1]\n",
    "\n",
    "# dnn_01 = tf.keras.Sequential()\n",
    "\n",
    "# # Input layer\n",
    "# dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# # Dense layers with dropout\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# # Output layer\n",
    "# # dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "\n",
    "\n",
    "# dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "# base_classifier = dnn_01\n",
    "\n",
    "# # Define the BaggingClassifier\n",
    "# bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# # Train the BaggingClassifier\n",
    "# bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# # Evaluate accuracy\n",
    "# # accuracy = accuracy_score(y_test_01, y_pred)\n",
    "# # print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Bagging with DNN', file = f)\n",
    "\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('CONFUSION MATRIX')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = y_pred\n",
    "\n",
    "# confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "# all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "# z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "# rows, cols = confusion_matrix.shape\n",
    "# z[:rows, :cols] = confusion_matrix\n",
    "# confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "# print(confusion_matrix)\n",
    "# with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "# FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "# FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "# TP = np.diag(confusion_matrix)\n",
    "# TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "# TP_total = sum(TP)\n",
    "# TN_total = sum(TN)\n",
    "# FP_total = sum(FP)\n",
    "# FN_total = sum(FN)\n",
    "\n",
    "# TP_total = np.array(TP_total,dtype=np.float64)\n",
    "# TN_total = np.array(TN_total,dtype=np.float64)\n",
    "# FP_total = np.array(FP_total,dtype=np.float64)\n",
    "# FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "# #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('METRICS')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# Acc = accuracy_score(y_test_01, pred_label)\n",
    "# Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "# Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "# F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "# BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "# MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "# bag_dnn_acc_01 = Acc\n",
    "# bag_dnn_pre_01 = Precision\n",
    "# bag_dnn_rec_01 = Recall\n",
    "# bag_dnn_f1_01 = F1\n",
    "# bag_dnn_bacc_01 = BACC\n",
    "# bag_dnn_mcc_01 = MCC\n",
    "# # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "# print('Accuracy total: ', Acc)\n",
    "# print('Precision total: ', Precision )\n",
    "# print('Recall total: ', Recall )\n",
    "# print('F1 total: ', F1 )\n",
    "# print('BACC total: ', BACC)\n",
    "# print('MCC total: ', MCC)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6818.0    37.0    38.0   53.0  1.0\n",
      "1.0    60.0  4727.0    14.0    3.0  0.0\n",
      "2.0    35.0     7.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     6.0  288.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.7483051148529558\n",
      "Recall total:  0.7577937804689114\n",
      "F1 total:  0.7529280869485394\n",
      "BACC total:  0.7577937804689114\n",
      "MCC total:  0.9611740958290594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "start = time.time()\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp_01 = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "base_classifier = mlp_01\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# accuracy = accuracy_score(y_test_01, y_pred)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_mlp'\n",
    "pred_label = y_pred\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6814.0    43.0    38.0   52.0  0.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    34.0     8.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.7484076614985394\n",
      "Recall total:  0.7584005336923563\n",
      "F1 total:  0.7532717233487052\n",
      "BACC total:  0.7584005336923563\n",
      "MCC total:  0.9611770004973139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "start = time.time()\n",
    "\n",
    "base_classifier = knn_01\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# accuracy = accuracy_score(y_test_01, y_pred)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_knn'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bagging LogRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining baggin Logistic Regression Model\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6761.0    92.0    29.0   65.0  0.0\n",
      "1.0   279.0  4501.0    16.0    8.0  0.0\n",
      "2.0    35.0   117.0  1094.0   27.0  0.0\n",
      "3.0    41.0     0.0    31.0  263.0  0.0\n",
      "4.0     2.0     1.0     1.0    4.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9440412957282861\n",
      "Precision total:  0.7112268856558526\n",
      "Recall total:  0.7109230628547418\n",
      "F1 total:  0.7104040181117334\n",
      "BACC total:  0.7109230628547418\n",
      "MCC total:  0.9049903386357704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "start = time.time()\n",
    "\n",
    "#Logistic Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining baggin Logistic Regression Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "logreg_01 = LogisticRegression()\n",
    "\n",
    "\n",
    "base_classifier = logreg_01\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# Evaluate accuracy\n",
    "# accuracy = accuracy_score(y_test_01, y_pred)\n",
    "# print(f'Accuracy: {accuracy}')\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_lr'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6790.0    36.0    68.0   52.0  1.0\n",
      "1.0    65.0  4710.0    26.0    3.0  0.0\n",
      "2.0    34.0    24.0  1212.0    3.0  0.0\n",
      "3.0    41.0     0.0     7.0  287.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9724695144759482\n",
      "Precision total:  0.7419744630181018\n",
      "Recall total:  0.7533262807792135\n",
      "F1 total:  0.7475224258184201\n",
      "BACC total:  0.7533262807792135\n",
      "MCC total:  0.9535264883346009\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "ada = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "base_classifier = ada\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_ada'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging CAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2542044\ttotal: 6.92ms\tremaining: 685ms\n",
      "1:\tlearn: 1.0394500\ttotal: 13.5ms\tremaining: 660ms\n",
      "2:\tlearn: 0.8879125\ttotal: 19.4ms\tremaining: 626ms\n",
      "3:\tlearn: 0.7677547\ttotal: 26.8ms\tremaining: 643ms\n",
      "4:\tlearn: 0.6740711\ttotal: 32.4ms\tremaining: 616ms\n",
      "5:\tlearn: 0.5961516\ttotal: 39ms\tremaining: 611ms\n",
      "6:\tlearn: 0.5318419\ttotal: 45ms\tremaining: 598ms\n",
      "7:\tlearn: 0.4784518\ttotal: 50.6ms\tremaining: 582ms\n",
      "8:\tlearn: 0.4324336\ttotal: 56.1ms\tremaining: 568ms\n",
      "9:\tlearn: 0.3942625\ttotal: 61.4ms\tremaining: 553ms\n",
      "10:\tlearn: 0.3598592\ttotal: 67ms\tremaining: 542ms\n",
      "11:\tlearn: 0.3301636\ttotal: 72.5ms\tremaining: 532ms\n",
      "12:\tlearn: 0.3043098\ttotal: 77.8ms\tremaining: 521ms\n",
      "13:\tlearn: 0.2815426\ttotal: 82.7ms\tremaining: 508ms\n",
      "14:\tlearn: 0.2617480\ttotal: 87.7ms\tremaining: 497ms\n",
      "15:\tlearn: 0.2443553\ttotal: 92.6ms\tremaining: 486ms\n",
      "16:\tlearn: 0.2287907\ttotal: 97.3ms\tremaining: 475ms\n",
      "17:\tlearn: 0.2150106\ttotal: 102ms\tremaining: 464ms\n",
      "18:\tlearn: 0.2037176\ttotal: 106ms\tremaining: 452ms\n",
      "19:\tlearn: 0.1929293\ttotal: 111ms\tremaining: 443ms\n",
      "20:\tlearn: 0.1834133\ttotal: 115ms\tremaining: 432ms\n",
      "21:\tlearn: 0.1752160\ttotal: 119ms\tremaining: 420ms\n",
      "22:\tlearn: 0.1680078\ttotal: 123ms\tremaining: 411ms\n",
      "23:\tlearn: 0.1612412\ttotal: 127ms\tremaining: 402ms\n",
      "24:\tlearn: 0.1553502\ttotal: 131ms\tremaining: 392ms\n",
      "25:\tlearn: 0.1502473\ttotal: 135ms\tremaining: 384ms\n",
      "26:\tlearn: 0.1455656\ttotal: 139ms\tremaining: 376ms\n",
      "27:\tlearn: 0.1412137\ttotal: 143ms\tremaining: 368ms\n",
      "28:\tlearn: 0.1374743\ttotal: 147ms\tremaining: 360ms\n",
      "29:\tlearn: 0.1342804\ttotal: 151ms\tremaining: 352ms\n",
      "30:\tlearn: 0.1312628\ttotal: 155ms\tremaining: 344ms\n",
      "31:\tlearn: 0.1284881\ttotal: 159ms\tremaining: 337ms\n",
      "32:\tlearn: 0.1261027\ttotal: 162ms\tremaining: 330ms\n",
      "33:\tlearn: 0.1239845\ttotal: 166ms\tremaining: 322ms\n",
      "34:\tlearn: 0.1226299\ttotal: 168ms\tremaining: 312ms\n",
      "35:\tlearn: 0.1209281\ttotal: 172ms\tremaining: 305ms\n",
      "36:\tlearn: 0.1193788\ttotal: 175ms\tremaining: 298ms\n",
      "37:\tlearn: 0.1180061\ttotal: 178ms\tremaining: 290ms\n",
      "38:\tlearn: 0.1167754\ttotal: 181ms\tremaining: 283ms\n",
      "39:\tlearn: 0.1159668\ttotal: 183ms\tremaining: 275ms\n",
      "40:\tlearn: 0.1148530\ttotal: 186ms\tremaining: 268ms\n",
      "41:\tlearn: 0.1142267\ttotal: 213ms\tremaining: 294ms\n",
      "42:\tlearn: 0.1133654\ttotal: 216ms\tremaining: 287ms\n",
      "43:\tlearn: 0.1126323\ttotal: 219ms\tremaining: 279ms\n",
      "44:\tlearn: 0.1119101\ttotal: 222ms\tremaining: 272ms\n",
      "45:\tlearn: 0.1112982\ttotal: 225ms\tremaining: 264ms\n",
      "46:\tlearn: 0.1107062\ttotal: 228ms\tremaining: 257ms\n",
      "47:\tlearn: 0.1101659\ttotal: 231ms\tremaining: 251ms\n",
      "48:\tlearn: 0.1097031\ttotal: 234ms\tremaining: 244ms\n",
      "49:\tlearn: 0.1092582\ttotal: 237ms\tremaining: 237ms\n",
      "50:\tlearn: 0.1090640\ttotal: 239ms\tremaining: 230ms\n",
      "51:\tlearn: 0.1086756\ttotal: 242ms\tremaining: 224ms\n",
      "52:\tlearn: 0.1084051\ttotal: 245ms\tremaining: 217ms\n",
      "53:\tlearn: 0.1080764\ttotal: 248ms\tremaining: 211ms\n",
      "54:\tlearn: 0.1078502\ttotal: 251ms\tremaining: 205ms\n",
      "55:\tlearn: 0.1075945\ttotal: 254ms\tremaining: 200ms\n",
      "56:\tlearn: 0.1074309\ttotal: 257ms\tremaining: 194ms\n",
      "57:\tlearn: 0.1072563\ttotal: 260ms\tremaining: 188ms\n",
      "58:\tlearn: 0.1070542\ttotal: 263ms\tremaining: 183ms\n",
      "59:\tlearn: 0.1069125\ttotal: 266ms\tremaining: 177ms\n",
      "60:\tlearn: 0.1067526\ttotal: 269ms\tremaining: 172ms\n",
      "61:\tlearn: 0.1066946\ttotal: 271ms\tremaining: 166ms\n",
      "62:\tlearn: 0.1065476\ttotal: 274ms\tremaining: 161ms\n",
      "63:\tlearn: 0.1064073\ttotal: 277ms\tremaining: 156ms\n",
      "64:\tlearn: 0.1062880\ttotal: 280ms\tremaining: 151ms\n",
      "65:\tlearn: 0.1062509\ttotal: 282ms\tremaining: 145ms\n",
      "66:\tlearn: 0.1061794\ttotal: 285ms\tremaining: 140ms\n",
      "67:\tlearn: 0.1060606\ttotal: 288ms\tremaining: 135ms\n",
      "68:\tlearn: 0.1059757\ttotal: 290ms\tremaining: 131ms\n",
      "69:\tlearn: 0.1059302\ttotal: 293ms\tremaining: 126ms\n",
      "70:\tlearn: 0.1058841\ttotal: 296ms\tremaining: 121ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71:\tlearn: 0.1058036\ttotal: 299ms\tremaining: 116ms\n",
      "72:\tlearn: 0.1057439\ttotal: 302ms\tremaining: 112ms\n",
      "73:\tlearn: 0.1056752\ttotal: 305ms\tremaining: 107ms\n",
      "74:\tlearn: 0.1056171\ttotal: 308ms\tremaining: 103ms\n",
      "75:\tlearn: 0.1055786\ttotal: 311ms\tremaining: 98.1ms\n",
      "76:\tlearn: 0.1055275\ttotal: 314ms\tremaining: 93.7ms\n",
      "77:\tlearn: 0.1054714\ttotal: 317ms\tremaining: 89.3ms\n",
      "78:\tlearn: 0.1054406\ttotal: 320ms\tremaining: 84.9ms\n",
      "79:\tlearn: 0.1053652\ttotal: 323ms\tremaining: 80.6ms\n",
      "80:\tlearn: 0.1053070\ttotal: 325ms\tremaining: 76.3ms\n",
      "81:\tlearn: 0.1052612\ttotal: 328ms\tremaining: 72.1ms\n",
      "82:\tlearn: 0.1051895\ttotal: 332ms\tremaining: 67.9ms\n",
      "83:\tlearn: 0.1051132\ttotal: 335ms\tremaining: 63.8ms\n",
      "84:\tlearn: 0.1050997\ttotal: 337ms\tremaining: 59.5ms\n",
      "85:\tlearn: 0.1050641\ttotal: 340ms\tremaining: 55.3ms\n",
      "86:\tlearn: 0.1050314\ttotal: 343ms\tremaining: 51.3ms\n",
      "87:\tlearn: 0.1049898\ttotal: 346ms\tremaining: 47.2ms\n",
      "88:\tlearn: 0.1049309\ttotal: 349ms\tremaining: 43.1ms\n",
      "89:\tlearn: 0.1048891\ttotal: 352ms\tremaining: 39.1ms\n",
      "90:\tlearn: 0.1048439\ttotal: 357ms\tremaining: 35.3ms\n",
      "91:\tlearn: 0.1048180\ttotal: 360ms\tremaining: 31.3ms\n",
      "92:\tlearn: 0.1047835\ttotal: 363ms\tremaining: 27.3ms\n",
      "93:\tlearn: 0.1047481\ttotal: 366ms\tremaining: 23.4ms\n",
      "94:\tlearn: 0.1047155\ttotal: 369ms\tremaining: 19.4ms\n",
      "95:\tlearn: 0.1046683\ttotal: 372ms\tremaining: 15.5ms\n",
      "96:\tlearn: 0.1046353\ttotal: 375ms\tremaining: 11.6ms\n",
      "97:\tlearn: 0.1046044\ttotal: 378ms\tremaining: 7.72ms\n",
      "98:\tlearn: 0.1045869\ttotal: 381ms\tremaining: 3.85ms\n",
      "99:\tlearn: 0.1045585\ttotal: 385ms\tremaining: 0us\n",
      "0:\tlearn: 1.2529738\ttotal: 7.62ms\tremaining: 755ms\n",
      "1:\tlearn: 1.0371723\ttotal: 14.5ms\tremaining: 709ms\n",
      "2:\tlearn: 0.8848716\ttotal: 20.3ms\tremaining: 656ms\n",
      "3:\tlearn: 0.7643997\ttotal: 27.1ms\tremaining: 651ms\n",
      "4:\tlearn: 0.6703715\ttotal: 32.8ms\tremaining: 622ms\n",
      "5:\tlearn: 0.5923038\ttotal: 39ms\tremaining: 611ms\n",
      "6:\tlearn: 0.5278061\ttotal: 44.9ms\tremaining: 597ms\n",
      "7:\tlearn: 0.4740594\ttotal: 50.7ms\tremaining: 583ms\n",
      "8:\tlearn: 0.4276724\ttotal: 56.2ms\tremaining: 568ms\n",
      "9:\tlearn: 0.3890495\ttotal: 61.6ms\tremaining: 554ms\n",
      "10:\tlearn: 0.3545555\ttotal: 67ms\tremaining: 542ms\n",
      "11:\tlearn: 0.3246662\ttotal: 72.3ms\tremaining: 530ms\n",
      "12:\tlearn: 0.2985292\ttotal: 77.2ms\tremaining: 516ms\n",
      "13:\tlearn: 0.2757118\ttotal: 81.8ms\tremaining: 502ms\n",
      "14:\tlearn: 0.2557286\ttotal: 86.2ms\tremaining: 489ms\n",
      "15:\tlearn: 0.2380467\ttotal: 91.2ms\tremaining: 479ms\n",
      "16:\tlearn: 0.2224015\ttotal: 95.6ms\tremaining: 467ms\n",
      "17:\tlearn: 0.2085476\ttotal: 100ms\tremaining: 455ms\n",
      "18:\tlearn: 0.1970638\ttotal: 104ms\tremaining: 442ms\n",
      "19:\tlearn: 0.1861232\ttotal: 108ms\tremaining: 433ms\n",
      "20:\tlearn: 0.1764640\ttotal: 112ms\tremaining: 422ms\n",
      "21:\tlearn: 0.1679643\ttotal: 116ms\tremaining: 411ms\n",
      "22:\tlearn: 0.1609213\ttotal: 119ms\tremaining: 399ms\n",
      "23:\tlearn: 0.1542255\ttotal: 123ms\tremaining: 390ms\n",
      "24:\tlearn: 0.1480034\ttotal: 127ms\tremaining: 381ms\n",
      "25:\tlearn: 0.1426241\ttotal: 131ms\tremaining: 372ms\n",
      "26:\tlearn: 0.1380601\ttotal: 134ms\tremaining: 363ms\n",
      "27:\tlearn: 0.1337025\ttotal: 138ms\tremaining: 354ms\n",
      "28:\tlearn: 0.1297830\ttotal: 141ms\tremaining: 345ms\n",
      "29:\tlearn: 0.1262980\ttotal: 145ms\tremaining: 337ms\n",
      "30:\tlearn: 0.1232599\ttotal: 148ms\tremaining: 329ms\n",
      "31:\tlearn: 0.1204249\ttotal: 151ms\tremaining: 321ms\n",
      "32:\tlearn: 0.1179029\ttotal: 154ms\tremaining: 313ms\n",
      "33:\tlearn: 0.1156140\ttotal: 158ms\tremaining: 306ms\n",
      "34:\tlearn: 0.1140907\ttotal: 160ms\tremaining: 297ms\n",
      "35:\tlearn: 0.1125259\ttotal: 163ms\tremaining: 289ms\n",
      "36:\tlearn: 0.1109329\ttotal: 166ms\tremaining: 282ms\n",
      "37:\tlearn: 0.1096064\ttotal: 168ms\tremaining: 275ms\n",
      "38:\tlearn: 0.1082285\ttotal: 172ms\tremaining: 269ms\n",
      "39:\tlearn: 0.1070741\ttotal: 175ms\tremaining: 262ms\n",
      "40:\tlearn: 0.1060404\ttotal: 178ms\tremaining: 256ms\n",
      "41:\tlearn: 0.1050943\ttotal: 181ms\tremaining: 250ms\n",
      "42:\tlearn: 0.1042883\ttotal: 184ms\tremaining: 244ms\n",
      "43:\tlearn: 0.1035210\ttotal: 187ms\tremaining: 238ms\n",
      "44:\tlearn: 0.1028542\ttotal: 190ms\tremaining: 232ms\n",
      "45:\tlearn: 0.1022381\ttotal: 193ms\tremaining: 227ms\n",
      "46:\tlearn: 0.1017162\ttotal: 196ms\tremaining: 221ms\n",
      "47:\tlearn: 0.1011992\ttotal: 199ms\tremaining: 216ms\n",
      "48:\tlearn: 0.1007944\ttotal: 202ms\tremaining: 211ms\n",
      "49:\tlearn: 0.1004548\ttotal: 205ms\tremaining: 205ms\n",
      "50:\tlearn: 0.1000988\ttotal: 208ms\tremaining: 200ms\n",
      "51:\tlearn: 0.0997881\ttotal: 212ms\tremaining: 195ms\n",
      "52:\tlearn: 0.0995169\ttotal: 215ms\tremaining: 190ms\n",
      "53:\tlearn: 0.0992723\ttotal: 217ms\tremaining: 185ms\n",
      "54:\tlearn: 0.0989924\ttotal: 220ms\tremaining: 180ms\n",
      "55:\tlearn: 0.0987911\ttotal: 223ms\tremaining: 175ms\n",
      "56:\tlearn: 0.0986006\ttotal: 226ms\tremaining: 171ms\n",
      "57:\tlearn: 0.0984050\ttotal: 229ms\tremaining: 166ms\n",
      "58:\tlearn: 0.0982306\ttotal: 232ms\tremaining: 161ms\n",
      "59:\tlearn: 0.0980762\ttotal: 235ms\tremaining: 157ms\n",
      "60:\tlearn: 0.0979526\ttotal: 238ms\tremaining: 152ms\n",
      "61:\tlearn: 0.0978081\ttotal: 241ms\tremaining: 148ms\n",
      "62:\tlearn: 0.0976965\ttotal: 244ms\tremaining: 143ms\n",
      "63:\tlearn: 0.0975453\ttotal: 247ms\tremaining: 139ms\n",
      "64:\tlearn: 0.0974469\ttotal: 250ms\tremaining: 135ms\n",
      "65:\tlearn: 0.0973258\ttotal: 253ms\tremaining: 130ms\n",
      "66:\tlearn: 0.0972059\ttotal: 256ms\tremaining: 126ms\n",
      "67:\tlearn: 0.0971355\ttotal: 259ms\tremaining: 122ms\n",
      "68:\tlearn: 0.0970813\ttotal: 262ms\tremaining: 118ms\n",
      "69:\tlearn: 0.0970001\ttotal: 264ms\tremaining: 113ms\n",
      "70:\tlearn: 0.0969406\ttotal: 267ms\tremaining: 109ms\n",
      "71:\tlearn: 0.0968811\ttotal: 270ms\tremaining: 105ms\n",
      "72:\tlearn: 0.0968006\ttotal: 273ms\tremaining: 101ms\n",
      "73:\tlearn: 0.0967736\ttotal: 276ms\tremaining: 96.8ms\n",
      "74:\tlearn: 0.0967109\ttotal: 278ms\tremaining: 92.8ms\n",
      "75:\tlearn: 0.0966584\ttotal: 282ms\tremaining: 88.9ms\n",
      "76:\tlearn: 0.0966002\ttotal: 285ms\tremaining: 85ms\n",
      "77:\tlearn: 0.0965212\ttotal: 287ms\tremaining: 81.1ms\n",
      "78:\tlearn: 0.0964776\ttotal: 291ms\tremaining: 77.2ms\n",
      "79:\tlearn: 0.0964293\ttotal: 294ms\tremaining: 73.4ms\n",
      "80:\tlearn: 0.0963755\ttotal: 297ms\tremaining: 69.6ms\n",
      "81:\tlearn: 0.0963144\ttotal: 300ms\tremaining: 65.8ms\n",
      "82:\tlearn: 0.0962799\ttotal: 303ms\tremaining: 62ms\n",
      "83:\tlearn: 0.0962524\ttotal: 306ms\tremaining: 58.3ms\n",
      "84:\tlearn: 0.0962454\ttotal: 308ms\tremaining: 54.4ms\n",
      "85:\tlearn: 0.0962046\ttotal: 311ms\tremaining: 50.7ms\n",
      "86:\tlearn: 0.0961741\ttotal: 314ms\tremaining: 47ms\n",
      "87:\tlearn: 0.0961553\ttotal: 317ms\tremaining: 43.2ms\n",
      "88:\tlearn: 0.0961325\ttotal: 320ms\tremaining: 39.5ms\n",
      "89:\tlearn: 0.0960936\ttotal: 323ms\tremaining: 35.9ms\n",
      "90:\tlearn: 0.0960515\ttotal: 326ms\tremaining: 32.2ms\n",
      "91:\tlearn: 0.0960216\ttotal: 329ms\tremaining: 28.6ms\n",
      "92:\tlearn: 0.0959800\ttotal: 332ms\tremaining: 25ms\n",
      "93:\tlearn: 0.0959635\ttotal: 335ms\tremaining: 21.4ms\n",
      "94:\tlearn: 0.0959444\ttotal: 338ms\tremaining: 17.8ms\n",
      "95:\tlearn: 0.0959079\ttotal: 341ms\tremaining: 14.2ms\n",
      "96:\tlearn: 0.0958850\ttotal: 344ms\tremaining: 10.6ms\n",
      "97:\tlearn: 0.0958625\ttotal: 347ms\tremaining: 7.08ms\n",
      "98:\tlearn: 0.0958373\ttotal: 350ms\tremaining: 3.53ms\n",
      "99:\tlearn: 0.0958074\ttotal: 353ms\tremaining: 0us\n",
      "0:\tlearn: 1.2534536\ttotal: 7.91ms\tremaining: 783ms\n",
      "1:\tlearn: 1.0378547\ttotal: 14.6ms\tremaining: 716ms\n",
      "2:\tlearn: 0.8855017\ttotal: 20.6ms\tremaining: 666ms\n",
      "3:\tlearn: 0.7651865\ttotal: 27.7ms\tremaining: 664ms\n",
      "4:\tlearn: 0.6712808\ttotal: 33.6ms\tremaining: 638ms\n",
      "5:\tlearn: 0.5937471\ttotal: 40.3ms\tremaining: 632ms\n",
      "6:\tlearn: 0.5292418\ttotal: 46.5ms\tremaining: 617ms\n",
      "7:\tlearn: 0.4756182\ttotal: 52.1ms\tremaining: 599ms\n",
      "8:\tlearn: 0.4300939\ttotal: 58.2ms\tremaining: 588ms\n",
      "9:\tlearn: 0.3906972\ttotal: 63.7ms\tremaining: 573ms\n",
      "10:\tlearn: 0.3563947\ttotal: 69ms\tremaining: 559ms\n",
      "11:\tlearn: 0.3265939\ttotal: 74.5ms\tremaining: 546ms\n",
      "12:\tlearn: 0.3004627\ttotal: 79.8ms\tremaining: 534ms\n",
      "13:\tlearn: 0.2777782\ttotal: 84.5ms\tremaining: 519ms\n",
      "14:\tlearn: 0.2578484\ttotal: 89.3ms\tremaining: 506ms\n",
      "15:\tlearn: 0.2402876\ttotal: 94ms\tremaining: 494ms\n",
      "16:\tlearn: 0.2247230\ttotal: 98.3ms\tremaining: 480ms\n",
      "17:\tlearn: 0.2111019\ttotal: 103ms\tremaining: 468ms\n",
      "18:\tlearn: 0.1997373\ttotal: 106ms\tremaining: 453ms\n",
      "19:\tlearn: 0.1889159\ttotal: 111ms\tremaining: 443ms\n",
      "20:\tlearn: 0.1791773\ttotal: 115ms\tremaining: 431ms\n",
      "21:\tlearn: 0.1707347\ttotal: 118ms\tremaining: 420ms\n",
      "22:\tlearn: 0.1636704\ttotal: 122ms\tremaining: 408ms\n",
      "23:\tlearn: 0.1570482\ttotal: 126ms\tremaining: 398ms\n",
      "24:\tlearn: 0.1510456\ttotal: 130ms\tremaining: 389ms\n",
      "25:\tlearn: 0.1463210\ttotal: 132ms\tremaining: 377ms\n",
      "26:\tlearn: 0.1416971\ttotal: 136ms\tremaining: 367ms\n",
      "27:\tlearn: 0.1374712\ttotal: 139ms\tremaining: 357ms\n",
      "28:\tlearn: 0.1334468\ttotal: 143ms\tremaining: 349ms\n",
      "29:\tlearn: 0.1299580\ttotal: 146ms\tremaining: 341ms\n",
      "30:\tlearn: 0.1267611\ttotal: 150ms\tremaining: 333ms\n",
      "31:\tlearn: 0.1240529\ttotal: 153ms\tremaining: 325ms\n",
      "32:\tlearn: 0.1216650\ttotal: 156ms\tremaining: 318ms\n",
      "33:\tlearn: 0.1196753\ttotal: 160ms\tremaining: 310ms\n",
      "34:\tlearn: 0.1176936\ttotal: 163ms\tremaining: 302ms\n",
      "35:\tlearn: 0.1160058\ttotal: 166ms\tremaining: 295ms\n",
      "36:\tlearn: 0.1146635\ttotal: 169ms\tremaining: 288ms\n",
      "37:\tlearn: 0.1132467\ttotal: 172ms\tremaining: 281ms\n",
      "38:\tlearn: 0.1119995\ttotal: 175ms\tremaining: 274ms\n",
      "39:\tlearn: 0.1110324\ttotal: 178ms\tremaining: 267ms\n",
      "40:\tlearn: 0.1099479\ttotal: 182ms\tremaining: 261ms\n",
      "41:\tlearn: 0.1093424\ttotal: 184ms\tremaining: 254ms\n",
      "42:\tlearn: 0.1085482\ttotal: 187ms\tremaining: 248ms\n",
      "43:\tlearn: 0.1077845\ttotal: 190ms\tremaining: 242ms\n",
      "44:\tlearn: 0.1071128\ttotal: 193ms\tremaining: 236ms\n",
      "45:\tlearn: 0.1065688\ttotal: 196ms\tremaining: 230ms\n",
      "46:\tlearn: 0.1059890\ttotal: 199ms\tremaining: 224ms\n",
      "47:\tlearn: 0.1055844\ttotal: 202ms\tremaining: 219ms\n",
      "48:\tlearn: 0.1051631\ttotal: 205ms\tremaining: 213ms\n",
      "49:\tlearn: 0.1047056\ttotal: 208ms\tremaining: 208ms\n",
      "50:\tlearn: 0.1045345\ttotal: 210ms\tremaining: 202ms\n",
      "51:\tlearn: 0.1041650\ttotal: 213ms\tremaining: 197ms\n",
      "52:\tlearn: 0.1038884\ttotal: 216ms\tremaining: 192ms\n",
      "53:\tlearn: 0.1035710\ttotal: 219ms\tremaining: 187ms\n",
      "54:\tlearn: 0.1034612\ttotal: 222ms\tremaining: 181ms\n",
      "55:\tlearn: 0.1032147\ttotal: 224ms\tremaining: 176ms\n",
      "56:\tlearn: 0.1029936\ttotal: 227ms\tremaining: 171ms\n",
      "57:\tlearn: 0.1028177\ttotal: 230ms\tremaining: 167ms\n",
      "58:\tlearn: 0.1026537\ttotal: 233ms\tremaining: 162ms\n",
      "59:\tlearn: 0.1025189\ttotal: 236ms\tremaining: 157ms\n",
      "60:\tlearn: 0.1024137\ttotal: 239ms\tremaining: 153ms\n",
      "61:\tlearn: 0.1022739\ttotal: 242ms\tremaining: 148ms\n",
      "62:\tlearn: 0.1021339\ttotal: 244ms\tremaining: 144ms\n",
      "63:\tlearn: 0.1020285\ttotal: 247ms\tremaining: 139ms\n",
      "64:\tlearn: 0.1019264\ttotal: 250ms\tremaining: 135ms\n",
      "65:\tlearn: 0.1018955\ttotal: 252ms\tremaining: 130ms\n",
      "66:\tlearn: 0.1018061\ttotal: 255ms\tremaining: 126ms\n",
      "67:\tlearn: 0.1017290\ttotal: 258ms\tremaining: 122ms\n",
      "68:\tlearn: 0.1016468\ttotal: 261ms\tremaining: 117ms\n",
      "69:\tlearn: 0.1015548\ttotal: 264ms\tremaining: 113ms\n",
      "70:\tlearn: 0.1015101\ttotal: 267ms\tremaining: 109ms\n",
      "71:\tlearn: 0.1014253\ttotal: 270ms\tremaining: 105ms\n",
      "72:\tlearn: 0.1013659\ttotal: 272ms\tremaining: 101ms\n",
      "73:\tlearn: 0.1013031\ttotal: 275ms\tremaining: 96.7ms\n",
      "74:\tlearn: 0.1012495\ttotal: 279ms\tremaining: 92.9ms\n",
      "75:\tlearn: 0.1011932\ttotal: 281ms\tremaining: 88.9ms\n",
      "76:\tlearn: 0.1011434\ttotal: 284ms\tremaining: 84.9ms\n",
      "77:\tlearn: 0.1010806\ttotal: 287ms\tremaining: 81ms\n",
      "78:\tlearn: 0.1010293\ttotal: 290ms\tremaining: 77.1ms\n",
      "79:\tlearn: 0.1009764\ttotal: 293ms\tremaining: 73.3ms\n",
      "80:\tlearn: 0.1009264\ttotal: 296ms\tremaining: 69.4ms\n",
      "81:\tlearn: 0.1008798\ttotal: 299ms\tremaining: 65.6ms\n",
      "82:\tlearn: 0.1008586\ttotal: 301ms\tremaining: 61.7ms\n",
      "83:\tlearn: 0.1008172\ttotal: 304ms\tremaining: 58ms\n",
      "84:\tlearn: 0.1007689\ttotal: 307ms\tremaining: 54.2ms\n",
      "85:\tlearn: 0.1007325\ttotal: 310ms\tremaining: 50.5ms\n",
      "86:\tlearn: 0.1006989\ttotal: 313ms\tremaining: 46.8ms\n",
      "87:\tlearn: 0.1006534\ttotal: 316ms\tremaining: 43.1ms\n",
      "88:\tlearn: 0.1006255\ttotal: 319ms\tremaining: 39.4ms\n",
      "89:\tlearn: 0.1005973\ttotal: 322ms\tremaining: 35.8ms\n",
      "90:\tlearn: 0.1005600\ttotal: 325ms\tremaining: 32.1ms\n",
      "91:\tlearn: 0.1005343\ttotal: 328ms\tremaining: 28.5ms\n",
      "92:\tlearn: 0.1004932\ttotal: 331ms\tremaining: 24.9ms\n",
      "93:\tlearn: 0.1004755\ttotal: 334ms\tremaining: 21.3ms\n",
      "94:\tlearn: 0.1004521\ttotal: 336ms\tremaining: 17.7ms\n",
      "95:\tlearn: 0.1004306\ttotal: 339ms\tremaining: 14.1ms\n",
      "96:\tlearn: 0.1004026\ttotal: 342ms\tremaining: 10.6ms\n",
      "97:\tlearn: 0.1003762\ttotal: 345ms\tremaining: 7.04ms\n",
      "98:\tlearn: 0.1003516\ttotal: 348ms\tremaining: 3.51ms\n",
      "99:\tlearn: 0.1003167\ttotal: 351ms\tremaining: 0us\n",
      "0:\tlearn: 1.2537372\ttotal: 8.58ms\tremaining: 849ms\n",
      "1:\tlearn: 1.0380672\ttotal: 15.3ms\tremaining: 749ms\n",
      "2:\tlearn: 0.8865193\ttotal: 21ms\tremaining: 680ms\n",
      "3:\tlearn: 0.7663379\ttotal: 27.9ms\tremaining: 670ms\n",
      "4:\tlearn: 0.6727309\ttotal: 33.4ms\tremaining: 635ms\n",
      "5:\tlearn: 0.5946663\ttotal: 39.6ms\tremaining: 620ms\n",
      "6:\tlearn: 0.5302597\ttotal: 45.1ms\tremaining: 600ms\n",
      "7:\tlearn: 0.4765250\ttotal: 50.2ms\tremaining: 577ms\n",
      "8:\tlearn: 0.4305138\ttotal: 55.3ms\tremaining: 560ms\n",
      "9:\tlearn: 0.3909575\ttotal: 60.1ms\tremaining: 541ms\n",
      "10:\tlearn: 0.3567128\ttotal: 65.1ms\tremaining: 527ms\n",
      "11:\tlearn: 0.3270975\ttotal: 69.7ms\tremaining: 511ms\n",
      "12:\tlearn: 0.3011732\ttotal: 74.1ms\tremaining: 496ms\n",
      "13:\tlearn: 0.2785079\ttotal: 77.9ms\tremaining: 478ms\n",
      "14:\tlearn: 0.2587007\ttotal: 82ms\tremaining: 465ms\n",
      "15:\tlearn: 0.2409673\ttotal: 86.3ms\tremaining: 453ms\n",
      "16:\tlearn: 0.2254501\ttotal: 90.2ms\tremaining: 440ms\n",
      "17:\tlearn: 0.2117962\ttotal: 93.9ms\tremaining: 428ms\n",
      "18:\tlearn: 0.2004429\ttotal: 97.2ms\tremaining: 414ms\n",
      "19:\tlearn: 0.1895929\ttotal: 101ms\tremaining: 405ms\n",
      "20:\tlearn: 0.1799711\ttotal: 105ms\tremaining: 396ms\n",
      "21:\tlearn: 0.1715925\ttotal: 109ms\tremaining: 385ms\n",
      "22:\tlearn: 0.1643501\ttotal: 112ms\tremaining: 375ms\n",
      "23:\tlearn: 0.1577327\ttotal: 115ms\tremaining: 366ms\n",
      "24:\tlearn: 0.1516969\ttotal: 118ms\tremaining: 355ms\n",
      "25:\tlearn: 0.1464866\ttotal: 122ms\tremaining: 347ms\n",
      "26:\tlearn: 0.1417208\ttotal: 125ms\tremaining: 338ms\n",
      "27:\tlearn: 0.1374305\ttotal: 128ms\tremaining: 330ms\n",
      "28:\tlearn: 0.1336869\ttotal: 131ms\tremaining: 322ms\n",
      "29:\tlearn: 0.1303958\ttotal: 134ms\tremaining: 314ms\n",
      "30:\tlearn: 0.1272840\ttotal: 138ms\tremaining: 306ms\n",
      "31:\tlearn: 0.1245293\ttotal: 141ms\tremaining: 299ms\n",
      "32:\tlearn: 0.1221513\ttotal: 144ms\tremaining: 292ms\n",
      "33:\tlearn: 0.1200550\ttotal: 147ms\tremaining: 284ms\n",
      "34:\tlearn: 0.1181350\ttotal: 150ms\tremaining: 278ms\n",
      "35:\tlearn: 0.1164440\ttotal: 153ms\tremaining: 272ms\n",
      "36:\tlearn: 0.1149280\ttotal: 156ms\tremaining: 265ms\n",
      "37:\tlearn: 0.1136249\ttotal: 159ms\tremaining: 259ms\n",
      "38:\tlearn: 0.1123868\ttotal: 162ms\tremaining: 253ms\n",
      "39:\tlearn: 0.1114087\ttotal: 165ms\tremaining: 247ms\n",
      "40:\tlearn: 0.1103565\ttotal: 168ms\tremaining: 241ms\n",
      "41:\tlearn: 0.1094932\ttotal: 171ms\tremaining: 236ms\n",
      "42:\tlearn: 0.1087759\ttotal: 174ms\tremaining: 230ms\n",
      "43:\tlearn: 0.1080657\ttotal: 177ms\tremaining: 225ms\n",
      "44:\tlearn: 0.1074448\ttotal: 180ms\tremaining: 220ms\n",
      "45:\tlearn: 0.1071049\ttotal: 182ms\tremaining: 214ms\n",
      "46:\tlearn: 0.1065333\ttotal: 185ms\tremaining: 208ms\n",
      "47:\tlearn: 0.1061026\ttotal: 188ms\tremaining: 203ms\n",
      "48:\tlearn: 0.1058300\ttotal: 190ms\tremaining: 198ms\n",
      "49:\tlearn: 0.1054165\ttotal: 193ms\tremaining: 193ms\n",
      "50:\tlearn: 0.1049881\ttotal: 196ms\tremaining: 189ms\n",
      "51:\tlearn: 0.1048266\ttotal: 198ms\tremaining: 183ms\n",
      "52:\tlearn: 0.1045393\ttotal: 201ms\tremaining: 178ms\n",
      "53:\tlearn: 0.1043321\ttotal: 205ms\tremaining: 174ms\n",
      "54:\tlearn: 0.1040396\ttotal: 208ms\tremaining: 170ms\n",
      "55:\tlearn: 0.1037505\ttotal: 211ms\tremaining: 166ms\n",
      "56:\tlearn: 0.1036241\ttotal: 213ms\tremaining: 161ms\n",
      "57:\tlearn: 0.1035322\ttotal: 216ms\tremaining: 156ms\n",
      "58:\tlearn: 0.1032793\ttotal: 218ms\tremaining: 152ms\n",
      "59:\tlearn: 0.1031089\ttotal: 222ms\tremaining: 148ms\n",
      "60:\tlearn: 0.1029573\ttotal: 224ms\tremaining: 143ms\n",
      "61:\tlearn: 0.1028133\ttotal: 227ms\tremaining: 139ms\n",
      "62:\tlearn: 0.1026503\ttotal: 230ms\tremaining: 135ms\n",
      "63:\tlearn: 0.1024753\ttotal: 233ms\tremaining: 131ms\n",
      "64:\tlearn: 0.1023396\ttotal: 236ms\tremaining: 127ms\n",
      "65:\tlearn: 0.1023124\ttotal: 238ms\tremaining: 123ms\n",
      "66:\tlearn: 0.1022060\ttotal: 241ms\tremaining: 119ms\n",
      "67:\tlearn: 0.1020746\ttotal: 244ms\tremaining: 115ms\n",
      "68:\tlearn: 0.1020017\ttotal: 247ms\tremaining: 111ms\n",
      "69:\tlearn: 0.1019190\ttotal: 250ms\tremaining: 107ms\n",
      "70:\tlearn: 0.1018420\ttotal: 253ms\tremaining: 103ms\n",
      "71:\tlearn: 0.1017769\ttotal: 256ms\tremaining: 99.5ms\n",
      "72:\tlearn: 0.1016918\ttotal: 259ms\tremaining: 95.7ms\n",
      "73:\tlearn: 0.1016180\ttotal: 262ms\tremaining: 91.9ms\n",
      "74:\tlearn: 0.1015285\ttotal: 264ms\tremaining: 88.1ms\n",
      "75:\tlearn: 0.1014471\ttotal: 267ms\tremaining: 84.4ms\n",
      "76:\tlearn: 0.1014128\ttotal: 270ms\tremaining: 80.5ms\n",
      "77:\tlearn: 0.1013643\ttotal: 273ms\tremaining: 76.9ms\n",
      "78:\tlearn: 0.1013013\ttotal: 276ms\tremaining: 73.2ms\n",
      "79:\tlearn: 0.1012495\ttotal: 278ms\tremaining: 69.6ms\n",
      "80:\tlearn: 0.1011797\ttotal: 281ms\tremaining: 66ms\n",
      "81:\tlearn: 0.1011313\ttotal: 284ms\tremaining: 62.4ms\n",
      "82:\tlearn: 0.1010894\ttotal: 287ms\tremaining: 58.9ms\n",
      "83:\tlearn: 0.1010534\ttotal: 290ms\tremaining: 55.3ms\n",
      "84:\tlearn: 0.1010069\ttotal: 293ms\tremaining: 51.8ms\n",
      "85:\tlearn: 0.1009646\ttotal: 296ms\tremaining: 48.2ms\n",
      "86:\tlearn: 0.1009172\ttotal: 299ms\tremaining: 44.7ms\n",
      "87:\tlearn: 0.1009054\ttotal: 302ms\tremaining: 41.1ms\n",
      "88:\tlearn: 0.1008581\ttotal: 305ms\tremaining: 37.7ms\n",
      "89:\tlearn: 0.1008181\ttotal: 307ms\tremaining: 34.2ms\n",
      "90:\tlearn: 0.1007942\ttotal: 310ms\tremaining: 30.7ms\n",
      "91:\tlearn: 0.1007639\ttotal: 313ms\tremaining: 27.2ms\n",
      "92:\tlearn: 0.1007559\ttotal: 315ms\tremaining: 23.7ms\n",
      "93:\tlearn: 0.1007338\ttotal: 318ms\tremaining: 20.3ms\n",
      "94:\tlearn: 0.1007046\ttotal: 321ms\tremaining: 16.9ms\n",
      "95:\tlearn: 0.1006746\ttotal: 323ms\tremaining: 13.5ms\n",
      "96:\tlearn: 0.1006338\ttotal: 326ms\tremaining: 10.1ms\n",
      "97:\tlearn: 0.1005992\ttotal: 329ms\tremaining: 6.72ms\n",
      "98:\tlearn: 0.1005760\ttotal: 332ms\tremaining: 3.35ms\n",
      "99:\tlearn: 0.1005421\ttotal: 335ms\tremaining: 0us\n",
      "0:\tlearn: 1.2538464\ttotal: 8.08ms\tremaining: 800ms\n",
      "1:\tlearn: 1.0384075\ttotal: 15ms\tremaining: 733ms\n",
      "2:\tlearn: 0.8864392\ttotal: 21.4ms\tremaining: 691ms\n",
      "3:\tlearn: 0.7661195\ttotal: 28.1ms\tremaining: 675ms\n",
      "4:\tlearn: 0.6722122\ttotal: 33.9ms\tremaining: 644ms\n",
      "5:\tlearn: 0.5940422\ttotal: 40.7ms\tremaining: 638ms\n",
      "6:\tlearn: 0.5295637\ttotal: 47.3ms\tremaining: 628ms\n",
      "7:\tlearn: 0.4758853\ttotal: 53.1ms\tremaining: 611ms\n",
      "8:\tlearn: 0.4295799\ttotal: 58.6ms\tremaining: 592ms\n",
      "9:\tlearn: 0.3901929\ttotal: 63.8ms\tremaining: 574ms\n",
      "10:\tlearn: 0.3559216\ttotal: 69ms\tremaining: 558ms\n",
      "11:\tlearn: 0.3261665\ttotal: 74.2ms\tremaining: 544ms\n",
      "12:\tlearn: 0.3000109\ttotal: 79.5ms\tremaining: 532ms\n",
      "13:\tlearn: 0.2778327\ttotal: 83.8ms\tremaining: 515ms\n",
      "14:\tlearn: 0.2578274\ttotal: 88.5ms\tremaining: 502ms\n",
      "15:\tlearn: 0.2401533\ttotal: 93.4ms\tremaining: 491ms\n",
      "16:\tlearn: 0.2245647\ttotal: 97.7ms\tremaining: 477ms\n",
      "17:\tlearn: 0.2107180\ttotal: 102ms\tremaining: 464ms\n",
      "18:\tlearn: 0.1993061\ttotal: 105ms\tremaining: 450ms\n",
      "19:\tlearn: 0.1883962\ttotal: 110ms\tremaining: 439ms\n",
      "20:\tlearn: 0.1787994\ttotal: 114ms\tremaining: 428ms\n",
      "21:\tlearn: 0.1703164\ttotal: 117ms\tremaining: 416ms\n",
      "22:\tlearn: 0.1631312\ttotal: 121ms\tremaining: 405ms\n",
      "23:\tlearn: 0.1564193\ttotal: 125ms\tremaining: 395ms\n",
      "24:\tlearn: 0.1503844\ttotal: 128ms\tremaining: 385ms\n",
      "25:\tlearn: 0.1450301\ttotal: 132ms\tremaining: 376ms\n",
      "26:\tlearn: 0.1403059\ttotal: 136ms\tremaining: 367ms\n",
      "27:\tlearn: 0.1360080\ttotal: 140ms\tremaining: 359ms\n",
      "28:\tlearn: 0.1322058\ttotal: 143ms\tremaining: 351ms\n",
      "29:\tlearn: 0.1286471\ttotal: 147ms\tremaining: 342ms\n",
      "30:\tlearn: 0.1255689\ttotal: 150ms\tremaining: 334ms\n",
      "31:\tlearn: 0.1227248\ttotal: 154ms\tremaining: 327ms\n",
      "32:\tlearn: 0.1202412\ttotal: 157ms\tremaining: 319ms\n",
      "33:\tlearn: 0.1178899\ttotal: 161ms\tremaining: 312ms\n",
      "34:\tlearn: 0.1163176\ttotal: 163ms\tremaining: 303ms\n",
      "35:\tlearn: 0.1145770\ttotal: 166ms\tremaining: 295ms\n",
      "36:\tlearn: 0.1129842\ttotal: 169ms\tremaining: 288ms\n",
      "37:\tlearn: 0.1115182\ttotal: 172ms\tremaining: 281ms\n",
      "38:\tlearn: 0.1102669\ttotal: 175ms\tremaining: 274ms\n",
      "39:\tlearn: 0.1091077\ttotal: 178ms\tremaining: 267ms\n",
      "40:\tlearn: 0.1082878\ttotal: 181ms\tremaining: 261ms\n",
      "41:\tlearn: 0.1074090\ttotal: 184ms\tremaining: 254ms\n",
      "42:\tlearn: 0.1065401\ttotal: 187ms\tremaining: 248ms\n",
      "43:\tlearn: 0.1059212\ttotal: 190ms\tremaining: 242ms\n",
      "44:\tlearn: 0.1051989\ttotal: 193ms\tremaining: 236ms\n",
      "45:\tlearn: 0.1045733\ttotal: 196ms\tremaining: 230ms\n",
      "46:\tlearn: 0.1042058\ttotal: 199ms\tremaining: 224ms\n",
      "47:\tlearn: 0.1036799\ttotal: 201ms\tremaining: 218ms\n",
      "48:\tlearn: 0.1031772\ttotal: 205ms\tremaining: 213ms\n",
      "49:\tlearn: 0.1027206\ttotal: 208ms\tremaining: 208ms\n",
      "50:\tlearn: 0.1023324\ttotal: 211ms\tremaining: 203ms\n",
      "51:\tlearn: 0.1019907\ttotal: 214ms\tremaining: 197ms\n",
      "52:\tlearn: 0.1017005\ttotal: 217ms\tremaining: 192ms\n",
      "53:\tlearn: 0.1014086\ttotal: 220ms\tremaining: 187ms\n",
      "54:\tlearn: 0.1011609\ttotal: 222ms\tremaining: 182ms\n",
      "55:\tlearn: 0.1010235\ttotal: 225ms\tremaining: 176ms\n",
      "56:\tlearn: 0.1007938\ttotal: 228ms\tremaining: 172ms\n",
      "57:\tlearn: 0.1005356\ttotal: 231ms\tremaining: 167ms\n",
      "58:\tlearn: 0.1003277\ttotal: 234ms\tremaining: 162ms\n",
      "59:\tlearn: 0.1001150\ttotal: 237ms\tremaining: 158ms\n",
      "60:\tlearn: 0.0999536\ttotal: 240ms\tremaining: 153ms\n",
      "61:\tlearn: 0.0997934\ttotal: 243ms\tremaining: 149ms\n",
      "62:\tlearn: 0.0996594\ttotal: 246ms\tremaining: 144ms\n",
      "63:\tlearn: 0.0995216\ttotal: 249ms\tremaining: 140ms\n",
      "64:\tlearn: 0.0993929\ttotal: 252ms\tremaining: 136ms\n",
      "65:\tlearn: 0.0992710\ttotal: 255ms\tremaining: 131ms\n",
      "66:\tlearn: 0.0991642\ttotal: 258ms\tremaining: 127ms\n",
      "67:\tlearn: 0.0990914\ttotal: 261ms\tremaining: 123ms\n",
      "68:\tlearn: 0.0990473\ttotal: 263ms\tremaining: 118ms\n",
      "69:\tlearn: 0.0989493\ttotal: 266ms\tremaining: 114ms\n",
      "70:\tlearn: 0.0989313\ttotal: 268ms\tremaining: 109ms\n",
      "71:\tlearn: 0.0988575\ttotal: 271ms\tremaining: 105ms\n",
      "72:\tlearn: 0.0987599\ttotal: 274ms\tremaining: 101ms\n",
      "73:\tlearn: 0.0986938\ttotal: 277ms\tremaining: 97.4ms\n",
      "74:\tlearn: 0.0986138\ttotal: 280ms\tremaining: 93.3ms\n",
      "75:\tlearn: 0.0985958\ttotal: 282ms\tremaining: 89.1ms\n",
      "76:\tlearn: 0.0985374\ttotal: 285ms\tremaining: 85.2ms\n",
      "77:\tlearn: 0.0984801\ttotal: 288ms\tremaining: 81.2ms\n",
      "78:\tlearn: 0.0984207\ttotal: 291ms\tremaining: 77.4ms\n",
      "79:\tlearn: 0.0983563\ttotal: 294ms\tremaining: 73.5ms\n",
      "80:\tlearn: 0.0983163\ttotal: 297ms\tremaining: 69.6ms\n",
      "81:\tlearn: 0.0983066\ttotal: 299ms\tremaining: 65.6ms\n",
      "82:\tlearn: 0.0982450\ttotal: 302ms\tremaining: 61.8ms\n",
      "83:\tlearn: 0.0981815\ttotal: 305ms\tremaining: 58ms\n",
      "84:\tlearn: 0.0981332\ttotal: 307ms\tremaining: 54.3ms\n",
      "85:\tlearn: 0.0981047\ttotal: 310ms\tremaining: 50.5ms\n",
      "86:\tlearn: 0.0980589\ttotal: 313ms\tremaining: 46.8ms\n",
      "87:\tlearn: 0.0980288\ttotal: 316ms\tremaining: 43ms\n",
      "88:\tlearn: 0.0979849\ttotal: 319ms\tremaining: 39.4ms\n",
      "89:\tlearn: 0.0979671\ttotal: 322ms\tremaining: 35.7ms\n",
      "90:\tlearn: 0.0979257\ttotal: 325ms\tremaining: 32.1ms\n",
      "91:\tlearn: 0.0978917\ttotal: 328ms\tremaining: 28.5ms\n",
      "92:\tlearn: 0.0978535\ttotal: 330ms\tremaining: 24.9ms\n",
      "93:\tlearn: 0.0978260\ttotal: 333ms\tremaining: 21.3ms\n",
      "94:\tlearn: 0.0977832\ttotal: 336ms\tremaining: 17.7ms\n",
      "95:\tlearn: 0.0977529\ttotal: 339ms\tremaining: 14.1ms\n",
      "96:\tlearn: 0.0977143\ttotal: 342ms\tremaining: 10.6ms\n",
      "97:\tlearn: 0.0976797\ttotal: 345ms\tremaining: 7.03ms\n",
      "98:\tlearn: 0.0976413\ttotal: 348ms\tremaining: 3.51ms\n",
      "99:\tlearn: 0.0976046\ttotal: 351ms\tremaining: 0us\n",
      "0:\tlearn: 1.2530546\ttotal: 7.82ms\tremaining: 774ms\n",
      "1:\tlearn: 1.0371061\ttotal: 14ms\tremaining: 688ms\n",
      "2:\tlearn: 0.8849478\ttotal: 20ms\tremaining: 648ms\n",
      "3:\tlearn: 0.7646463\ttotal: 26.7ms\tremaining: 641ms\n",
      "4:\tlearn: 0.6693568\ttotal: 33.4ms\tremaining: 634ms\n",
      "5:\tlearn: 0.5919917\ttotal: 40.1ms\tremaining: 628ms\n",
      "6:\tlearn: 0.5276217\ttotal: 46.4ms\tremaining: 616ms\n",
      "7:\tlearn: 0.4732894\ttotal: 52.4ms\tremaining: 603ms\n",
      "8:\tlearn: 0.4288340\ttotal: 57.2ms\tremaining: 578ms\n",
      "9:\tlearn: 0.3901187\ttotal: 62.7ms\tremaining: 564ms\n",
      "10:\tlearn: 0.3557503\ttotal: 68.3ms\tremaining: 553ms\n",
      "11:\tlearn: 0.3260273\ttotal: 73.9ms\tremaining: 542ms\n",
      "12:\tlearn: 0.2998321\ttotal: 79ms\tremaining: 529ms\n",
      "13:\tlearn: 0.2771246\ttotal: 83.6ms\tremaining: 513ms\n",
      "14:\tlearn: 0.2577503\ttotal: 88.2ms\tremaining: 500ms\n",
      "15:\tlearn: 0.2398782\ttotal: 93ms\tremaining: 488ms\n",
      "16:\tlearn: 0.2243896\ttotal: 97.8ms\tremaining: 477ms\n",
      "17:\tlearn: 0.2106648\ttotal: 102ms\tremaining: 465ms\n",
      "18:\tlearn: 0.1991913\ttotal: 106ms\tremaining: 452ms\n",
      "19:\tlearn: 0.1885387\ttotal: 110ms\tremaining: 439ms\n",
      "20:\tlearn: 0.1789820\ttotal: 114ms\tremaining: 429ms\n",
      "21:\tlearn: 0.1703133\ttotal: 118ms\tremaining: 419ms\n",
      "22:\tlearn: 0.1628586\ttotal: 122ms\tremaining: 409ms\n",
      "23:\tlearn: 0.1560868\ttotal: 126ms\tremaining: 399ms\n",
      "24:\tlearn: 0.1500966\ttotal: 130ms\tremaining: 390ms\n",
      "25:\tlearn: 0.1447517\ttotal: 134ms\tremaining: 380ms\n",
      "26:\tlearn: 0.1401073\ttotal: 137ms\tremaining: 371ms\n",
      "27:\tlearn: 0.1357850\ttotal: 141ms\tremaining: 362ms\n",
      "28:\tlearn: 0.1320281\ttotal: 144ms\tremaining: 353ms\n",
      "29:\tlearn: 0.1285925\ttotal: 148ms\tremaining: 345ms\n",
      "30:\tlearn: 0.1255912\ttotal: 151ms\tremaining: 336ms\n",
      "31:\tlearn: 0.1227286\ttotal: 154ms\tremaining: 327ms\n",
      "32:\tlearn: 0.1202879\ttotal: 157ms\tremaining: 319ms\n",
      "33:\tlearn: 0.1181730\ttotal: 161ms\tremaining: 312ms\n",
      "34:\tlearn: 0.1166942\ttotal: 163ms\tremaining: 303ms\n",
      "35:\tlearn: 0.1149846\ttotal: 166ms\tremaining: 295ms\n",
      "36:\tlearn: 0.1132615\ttotal: 169ms\tremaining: 288ms\n",
      "37:\tlearn: 0.1118820\ttotal: 172ms\tremaining: 281ms\n",
      "38:\tlearn: 0.1106818\ttotal: 175ms\tremaining: 274ms\n",
      "39:\tlearn: 0.1099037\ttotal: 177ms\tremaining: 266ms\n",
      "40:\tlearn: 0.1087816\ttotal: 180ms\tremaining: 259ms\n",
      "41:\tlearn: 0.1081741\ttotal: 183ms\tremaining: 252ms\n",
      "42:\tlearn: 0.1073017\ttotal: 186ms\tremaining: 246ms\n",
      "43:\tlearn: 0.1065713\ttotal: 188ms\tremaining: 240ms\n",
      "44:\tlearn: 0.1058641\ttotal: 191ms\tremaining: 234ms\n",
      "45:\tlearn: 0.1052881\ttotal: 194ms\tremaining: 228ms\n",
      "46:\tlearn: 0.1047267\ttotal: 197ms\tremaining: 222ms\n",
      "47:\tlearn: 0.1041802\ttotal: 200ms\tremaining: 217ms\n",
      "48:\tlearn: 0.1037534\ttotal: 203ms\tremaining: 212ms\n",
      "49:\tlearn: 0.1032882\ttotal: 207ms\tremaining: 207ms\n",
      "50:\tlearn: 0.1030936\ttotal: 209ms\tremaining: 200ms\n",
      "51:\tlearn: 0.1027888\ttotal: 212ms\tremaining: 195ms\n",
      "52:\tlearn: 0.1026443\ttotal: 214ms\tremaining: 190ms\n",
      "53:\tlearn: 0.1023537\ttotal: 217ms\tremaining: 185ms\n",
      "54:\tlearn: 0.1021163\ttotal: 220ms\tremaining: 180ms\n",
      "55:\tlearn: 0.1018913\ttotal: 223ms\tremaining: 175ms\n",
      "56:\tlearn: 0.1017099\ttotal: 226ms\tremaining: 170ms\n",
      "57:\tlearn: 0.1015234\ttotal: 229ms\tremaining: 166ms\n",
      "58:\tlearn: 0.1013293\ttotal: 232ms\tremaining: 161ms\n",
      "59:\tlearn: 0.1011639\ttotal: 235ms\tremaining: 156ms\n",
      "60:\tlearn: 0.1010070\ttotal: 238ms\tremaining: 152ms\n",
      "61:\tlearn: 0.1008616\ttotal: 241ms\tremaining: 147ms\n",
      "62:\tlearn: 0.1007369\ttotal: 244ms\tremaining: 143ms\n",
      "63:\tlearn: 0.1005675\ttotal: 247ms\tremaining: 139ms\n",
      "64:\tlearn: 0.1004620\ttotal: 250ms\tremaining: 134ms\n",
      "65:\tlearn: 0.1003893\ttotal: 252ms\tremaining: 130ms\n",
      "66:\tlearn: 0.1002951\ttotal: 255ms\tremaining: 126ms\n",
      "67:\tlearn: 0.1002026\ttotal: 258ms\tremaining: 121ms\n",
      "68:\tlearn: 0.1001329\ttotal: 261ms\tremaining: 117ms\n",
      "69:\tlearn: 0.1000979\ttotal: 263ms\tremaining: 113ms\n",
      "70:\tlearn: 0.0999924\ttotal: 266ms\tremaining: 109ms\n",
      "71:\tlearn: 0.0999268\ttotal: 269ms\tremaining: 105ms\n",
      "72:\tlearn: 0.0998481\ttotal: 272ms\tremaining: 100ms\n",
      "73:\tlearn: 0.0997752\ttotal: 275ms\tremaining: 96.6ms\n",
      "74:\tlearn: 0.0997237\ttotal: 278ms\tremaining: 92.5ms\n",
      "75:\tlearn: 0.0996933\ttotal: 280ms\tremaining: 88.4ms\n",
      "76:\tlearn: 0.0996213\ttotal: 283ms\tremaining: 84.5ms\n",
      "77:\tlearn: 0.0995808\ttotal: 286ms\tremaining: 80.5ms\n",
      "78:\tlearn: 0.0995246\ttotal: 288ms\tremaining: 76.7ms\n",
      "79:\tlearn: 0.0994845\ttotal: 291ms\tremaining: 72.8ms\n",
      "80:\tlearn: 0.0994480\ttotal: 294ms\tremaining: 68.9ms\n",
      "81:\tlearn: 0.0994374\ttotal: 296ms\tremaining: 65ms\n",
      "82:\tlearn: 0.0993975\ttotal: 299ms\tremaining: 61.2ms\n",
      "83:\tlearn: 0.0993648\ttotal: 302ms\tremaining: 57.4ms\n",
      "84:\tlearn: 0.0993407\ttotal: 304ms\tremaining: 53.6ms\n",
      "85:\tlearn: 0.0992948\ttotal: 307ms\tremaining: 49.9ms\n",
      "86:\tlearn: 0.0992388\ttotal: 310ms\tremaining: 46.3ms\n",
      "87:\tlearn: 0.0992102\ttotal: 313ms\tremaining: 42.6ms\n",
      "88:\tlearn: 0.0992065\ttotal: 315ms\tremaining: 38.9ms\n",
      "89:\tlearn: 0.0991643\ttotal: 317ms\tremaining: 35.3ms\n",
      "90:\tlearn: 0.0991270\ttotal: 320ms\tremaining: 31.7ms\n",
      "91:\tlearn: 0.0990954\ttotal: 323ms\tremaining: 28.1ms\n",
      "92:\tlearn: 0.0990551\ttotal: 326ms\tremaining: 24.5ms\n",
      "93:\tlearn: 0.0990525\ttotal: 328ms\tremaining: 20.9ms\n",
      "94:\tlearn: 0.0990296\ttotal: 331ms\tremaining: 17.4ms\n",
      "95:\tlearn: 0.0990023\ttotal: 334ms\tremaining: 13.9ms\n",
      "96:\tlearn: 0.0989736\ttotal: 336ms\tremaining: 10.4ms\n",
      "97:\tlearn: 0.0989352\ttotal: 340ms\tremaining: 6.93ms\n",
      "98:\tlearn: 0.0988856\ttotal: 343ms\tremaining: 3.46ms\n",
      "99:\tlearn: 0.0988729\ttotal: 345ms\tremaining: 0us\n",
      "0:\tlearn: 1.2537730\ttotal: 7.52ms\tremaining: 744ms\n",
      "1:\tlearn: 1.0385069\ttotal: 14.6ms\tremaining: 717ms\n",
      "2:\tlearn: 0.8866511\ttotal: 21ms\tremaining: 677ms\n",
      "3:\tlearn: 0.7664240\ttotal: 28.1ms\tremaining: 675ms\n",
      "4:\tlearn: 0.6711826\ttotal: 35ms\tremaining: 666ms\n",
      "5:\tlearn: 0.5949557\ttotal: 41.5ms\tremaining: 650ms\n",
      "6:\tlearn: 0.5304532\ttotal: 47.1ms\tremaining: 625ms\n",
      "7:\tlearn: 0.4761074\ttotal: 52.9ms\tremaining: 608ms\n",
      "8:\tlearn: 0.4316858\ttotal: 57.4ms\tremaining: 581ms\n",
      "9:\tlearn: 0.3932063\ttotal: 62.4ms\tremaining: 561ms\n",
      "10:\tlearn: 0.3588196\ttotal: 67.1ms\tremaining: 543ms\n",
      "11:\tlearn: 0.3289471\ttotal: 72.1ms\tremaining: 529ms\n",
      "12:\tlearn: 0.3031728\ttotal: 76ms\tremaining: 508ms\n",
      "13:\tlearn: 0.2804943\ttotal: 79.9ms\tremaining: 491ms\n",
      "14:\tlearn: 0.2614277\ttotal: 84.4ms\tremaining: 478ms\n",
      "15:\tlearn: 0.2436814\ttotal: 88.5ms\tremaining: 465ms\n",
      "16:\tlearn: 0.2278275\ttotal: 92.6ms\tremaining: 452ms\n",
      "17:\tlearn: 0.2140840\ttotal: 96.9ms\tremaining: 442ms\n",
      "18:\tlearn: 0.2024702\ttotal: 101ms\tremaining: 429ms\n",
      "19:\tlearn: 0.1916656\ttotal: 105ms\tremaining: 418ms\n",
      "20:\tlearn: 0.1818649\ttotal: 109ms\tremaining: 409ms\n",
      "21:\tlearn: 0.1732535\ttotal: 112ms\tremaining: 398ms\n",
      "22:\tlearn: 0.1659400\ttotal: 116ms\tremaining: 388ms\n",
      "23:\tlearn: 0.1590949\ttotal: 120ms\tremaining: 380ms\n",
      "24:\tlearn: 0.1530416\ttotal: 123ms\tremaining: 370ms\n",
      "25:\tlearn: 0.1477345\ttotal: 127ms\tremaining: 360ms\n",
      "26:\tlearn: 0.1428527\ttotal: 130ms\tremaining: 352ms\n",
      "27:\tlearn: 0.1384061\ttotal: 134ms\tremaining: 344ms\n",
      "28:\tlearn: 0.1347382\ttotal: 137ms\tremaining: 335ms\n",
      "29:\tlearn: 0.1313824\ttotal: 140ms\tremaining: 327ms\n",
      "30:\tlearn: 0.1281690\ttotal: 143ms\tremaining: 319ms\n",
      "31:\tlearn: 0.1253251\ttotal: 146ms\tremaining: 311ms\n",
      "32:\tlearn: 0.1231015\ttotal: 149ms\tremaining: 303ms\n",
      "33:\tlearn: 0.1209303\ttotal: 152ms\tremaining: 296ms\n",
      "34:\tlearn: 0.1194250\ttotal: 155ms\tremaining: 288ms\n",
      "35:\tlearn: 0.1177094\ttotal: 158ms\tremaining: 281ms\n",
      "36:\tlearn: 0.1159636\ttotal: 161ms\tremaining: 274ms\n",
      "37:\tlearn: 0.1144892\ttotal: 164ms\tremaining: 267ms\n",
      "38:\tlearn: 0.1133398\ttotal: 167ms\tremaining: 261ms\n",
      "39:\tlearn: 0.1125233\ttotal: 169ms\tremaining: 253ms\n",
      "40:\tlearn: 0.1114143\ttotal: 172ms\tremaining: 247ms\n",
      "41:\tlearn: 0.1107790\ttotal: 174ms\tremaining: 241ms\n",
      "42:\tlearn: 0.1099726\ttotal: 177ms\tremaining: 235ms\n",
      "43:\tlearn: 0.1091565\ttotal: 180ms\tremaining: 229ms\n",
      "44:\tlearn: 0.1086855\ttotal: 183ms\tremaining: 223ms\n",
      "45:\tlearn: 0.1080961\ttotal: 186ms\tremaining: 218ms\n",
      "46:\tlearn: 0.1074179\ttotal: 189ms\tremaining: 213ms\n",
      "47:\tlearn: 0.1068697\ttotal: 192ms\tremaining: 208ms\n",
      "48:\tlearn: 0.1064008\ttotal: 195ms\tremaining: 203ms\n",
      "49:\tlearn: 0.1059894\ttotal: 197ms\tremaining: 197ms\n",
      "50:\tlearn: 0.1055863\ttotal: 201ms\tremaining: 193ms\n",
      "51:\tlearn: 0.1052133\ttotal: 204ms\tremaining: 188ms\n",
      "52:\tlearn: 0.1049167\ttotal: 207ms\tremaining: 183ms\n",
      "53:\tlearn: 0.1046480\ttotal: 210ms\tremaining: 179ms\n",
      "54:\tlearn: 0.1043927\ttotal: 212ms\tremaining: 174ms\n",
      "55:\tlearn: 0.1041887\ttotal: 215ms\tremaining: 169ms\n",
      "56:\tlearn: 0.1040584\ttotal: 218ms\tremaining: 164ms\n",
      "57:\tlearn: 0.1038211\ttotal: 221ms\tremaining: 160ms\n",
      "58:\tlearn: 0.1036510\ttotal: 224ms\tremaining: 155ms\n",
      "59:\tlearn: 0.1034912\ttotal: 226ms\tremaining: 151ms\n",
      "60:\tlearn: 0.1033444\ttotal: 229ms\tremaining: 147ms\n",
      "61:\tlearn: 0.1032368\ttotal: 232ms\tremaining: 142ms\n",
      "62:\tlearn: 0.1031324\ttotal: 234ms\tremaining: 138ms\n",
      "63:\tlearn: 0.1029589\ttotal: 237ms\tremaining: 134ms\n",
      "64:\tlearn: 0.1028412\ttotal: 240ms\tremaining: 129ms\n",
      "65:\tlearn: 0.1027327\ttotal: 243ms\tremaining: 125ms\n",
      "66:\tlearn: 0.1026088\ttotal: 246ms\tremaining: 121ms\n",
      "67:\tlearn: 0.1025097\ttotal: 249ms\tremaining: 117ms\n",
      "68:\tlearn: 0.1024331\ttotal: 252ms\tremaining: 113ms\n",
      "69:\tlearn: 0.1023808\ttotal: 254ms\tremaining: 109ms\n",
      "70:\tlearn: 0.1023003\ttotal: 257ms\tremaining: 105ms\n",
      "71:\tlearn: 0.1022337\ttotal: 260ms\tremaining: 101ms\n",
      "72:\tlearn: 0.1021665\ttotal: 263ms\tremaining: 97.3ms\n",
      "73:\tlearn: 0.1020845\ttotal: 266ms\tremaining: 93.5ms\n",
      "74:\tlearn: 0.1020198\ttotal: 269ms\tremaining: 89.7ms\n",
      "75:\tlearn: 0.1019722\ttotal: 272ms\tremaining: 85.8ms\n",
      "76:\tlearn: 0.1019015\ttotal: 275ms\tremaining: 82.1ms\n",
      "77:\tlearn: 0.1018614\ttotal: 277ms\tremaining: 78.2ms\n",
      "78:\tlearn: 0.1018282\ttotal: 280ms\tremaining: 74.4ms\n",
      "79:\tlearn: 0.1017687\ttotal: 283ms\tremaining: 70.7ms\n",
      "80:\tlearn: 0.1017163\ttotal: 286ms\tremaining: 67ms\n",
      "81:\tlearn: 0.1016645\ttotal: 289ms\tremaining: 63.4ms\n",
      "82:\tlearn: 0.1016317\ttotal: 292ms\tremaining: 59.7ms\n",
      "83:\tlearn: 0.1015841\ttotal: 295ms\tremaining: 56.2ms\n",
      "84:\tlearn: 0.1015466\ttotal: 298ms\tremaining: 52.6ms\n",
      "85:\tlearn: 0.1015065\ttotal: 301ms\tremaining: 49ms\n",
      "86:\tlearn: 0.1014712\ttotal: 304ms\tremaining: 45.4ms\n",
      "87:\tlearn: 0.1014268\ttotal: 307ms\tremaining: 41.9ms\n",
      "88:\tlearn: 0.1013701\ttotal: 310ms\tremaining: 38.3ms\n",
      "89:\tlearn: 0.1013349\ttotal: 313ms\tremaining: 34.8ms\n",
      "90:\tlearn: 0.1013018\ttotal: 316ms\tremaining: 31.3ms\n",
      "91:\tlearn: 0.1012584\ttotal: 319ms\tremaining: 27.7ms\n",
      "92:\tlearn: 0.1012135\ttotal: 322ms\tremaining: 24.3ms\n",
      "93:\tlearn: 0.1011866\ttotal: 325ms\tremaining: 20.8ms\n",
      "94:\tlearn: 0.1011571\ttotal: 328ms\tremaining: 17.3ms\n",
      "95:\tlearn: 0.1011377\ttotal: 331ms\tremaining: 13.8ms\n",
      "96:\tlearn: 0.1011048\ttotal: 334ms\tremaining: 10.3ms\n",
      "97:\tlearn: 0.1010838\ttotal: 337ms\tremaining: 6.88ms\n",
      "98:\tlearn: 0.1010563\ttotal: 340ms\tremaining: 3.43ms\n",
      "99:\tlearn: 0.1010368\ttotal: 343ms\tremaining: 0us\n",
      "0:\tlearn: 1.2534523\ttotal: 7.53ms\tremaining: 746ms\n",
      "1:\tlearn: 1.0381333\ttotal: 14.2ms\tremaining: 693ms\n",
      "2:\tlearn: 0.8861304\ttotal: 20ms\tremaining: 648ms\n",
      "3:\tlearn: 0.7656794\ttotal: 27.3ms\tremaining: 655ms\n",
      "4:\tlearn: 0.6702623\ttotal: 34.1ms\tremaining: 647ms\n",
      "5:\tlearn: 0.5927199\ttotal: 40.4ms\tremaining: 633ms\n",
      "6:\tlearn: 0.5282312\ttotal: 46.7ms\tremaining: 620ms\n",
      "7:\tlearn: 0.4743054\ttotal: 52.3ms\tremaining: 601ms\n",
      "8:\tlearn: 0.4286584\ttotal: 57.1ms\tremaining: 577ms\n",
      "9:\tlearn: 0.3892076\ttotal: 62.3ms\tremaining: 561ms\n",
      "10:\tlearn: 0.3551071\ttotal: 67.5ms\tremaining: 546ms\n",
      "11:\tlearn: 0.3255079\ttotal: 72.6ms\tremaining: 533ms\n",
      "12:\tlearn: 0.2996016\ttotal: 77.3ms\tremaining: 518ms\n",
      "13:\tlearn: 0.2770149\ttotal: 81.9ms\tremaining: 503ms\n",
      "14:\tlearn: 0.2576257\ttotal: 86.8ms\tremaining: 492ms\n",
      "15:\tlearn: 0.2400511\ttotal: 91.6ms\tremaining: 481ms\n",
      "16:\tlearn: 0.2245468\ttotal: 95.7ms\tremaining: 467ms\n",
      "17:\tlearn: 0.2110563\ttotal: 99.7ms\tremaining: 454ms\n",
      "18:\tlearn: 0.1997119\ttotal: 104ms\tremaining: 441ms\n",
      "19:\tlearn: 0.1888606\ttotal: 108ms\tremaining: 432ms\n",
      "20:\tlearn: 0.1792510\ttotal: 112ms\tremaining: 422ms\n",
      "21:\tlearn: 0.1708251\ttotal: 116ms\tremaining: 410ms\n",
      "22:\tlearn: 0.1638311\ttotal: 119ms\tremaining: 398ms\n",
      "23:\tlearn: 0.1570753\ttotal: 123ms\tremaining: 390ms\n",
      "24:\tlearn: 0.1511922\ttotal: 127ms\tremaining: 380ms\n",
      "25:\tlearn: 0.1458436\ttotal: 130ms\tremaining: 371ms\n",
      "26:\tlearn: 0.1415904\ttotal: 133ms\tremaining: 360ms\n",
      "27:\tlearn: 0.1371631\ttotal: 137ms\tremaining: 352ms\n",
      "28:\tlearn: 0.1332681\ttotal: 141ms\tremaining: 344ms\n",
      "29:\tlearn: 0.1297899\ttotal: 144ms\tremaining: 336ms\n",
      "30:\tlearn: 0.1272004\ttotal: 147ms\tremaining: 326ms\n",
      "31:\tlearn: 0.1242841\ttotal: 150ms\tremaining: 319ms\n",
      "32:\tlearn: 0.1217710\ttotal: 154ms\tremaining: 312ms\n",
      "33:\tlearn: 0.1196226\ttotal: 157ms\tremaining: 304ms\n",
      "34:\tlearn: 0.1177320\ttotal: 160ms\tremaining: 297ms\n",
      "35:\tlearn: 0.1165245\ttotal: 162ms\tremaining: 288ms\n",
      "36:\tlearn: 0.1150130\ttotal: 165ms\tremaining: 281ms\n",
      "37:\tlearn: 0.1135602\ttotal: 168ms\tremaining: 274ms\n",
      "38:\tlearn: 0.1120987\ttotal: 171ms\tremaining: 268ms\n",
      "39:\tlearn: 0.1110045\ttotal: 174ms\tremaining: 261ms\n",
      "40:\tlearn: 0.1103072\ttotal: 176ms\tremaining: 254ms\n",
      "41:\tlearn: 0.1093170\ttotal: 179ms\tremaining: 248ms\n",
      "42:\tlearn: 0.1087720\ttotal: 182ms\tremaining: 241ms\n",
      "43:\tlearn: 0.1080065\ttotal: 184ms\tremaining: 235ms\n",
      "44:\tlearn: 0.1071974\ttotal: 188ms\tremaining: 229ms\n",
      "45:\tlearn: 0.1065875\ttotal: 190ms\tremaining: 224ms\n",
      "46:\tlearn: 0.1059105\ttotal: 194ms\tremaining: 218ms\n",
      "47:\tlearn: 0.1054170\ttotal: 197ms\tremaining: 213ms\n",
      "48:\tlearn: 0.1049617\ttotal: 199ms\tremaining: 208ms\n",
      "49:\tlearn: 0.1045238\ttotal: 203ms\tremaining: 203ms\n",
      "50:\tlearn: 0.1041002\ttotal: 206ms\tremaining: 198ms\n",
      "51:\tlearn: 0.1039343\ttotal: 209ms\tremaining: 193ms\n",
      "52:\tlearn: 0.1035511\ttotal: 213ms\tremaining: 189ms\n",
      "53:\tlearn: 0.1034121\ttotal: 215ms\tremaining: 183ms\n",
      "54:\tlearn: 0.1031542\ttotal: 218ms\tremaining: 178ms\n",
      "55:\tlearn: 0.1028676\ttotal: 221ms\tremaining: 174ms\n",
      "56:\tlearn: 0.1026665\ttotal: 224ms\tremaining: 169ms\n",
      "57:\tlearn: 0.1024516\ttotal: 227ms\tremaining: 164ms\n",
      "58:\tlearn: 0.1022419\ttotal: 230ms\tremaining: 160ms\n",
      "59:\tlearn: 0.1020485\ttotal: 233ms\tremaining: 155ms\n",
      "60:\tlearn: 0.1018956\ttotal: 235ms\tremaining: 151ms\n",
      "61:\tlearn: 0.1017507\ttotal: 238ms\tremaining: 146ms\n",
      "62:\tlearn: 0.1016006\ttotal: 241ms\tremaining: 142ms\n",
      "63:\tlearn: 0.1014662\ttotal: 244ms\tremaining: 137ms\n",
      "64:\tlearn: 0.1013227\ttotal: 247ms\tremaining: 133ms\n",
      "65:\tlearn: 0.1012820\ttotal: 249ms\tremaining: 128ms\n",
      "66:\tlearn: 0.1011752\ttotal: 252ms\tremaining: 124ms\n",
      "67:\tlearn: 0.1010577\ttotal: 255ms\tremaining: 120ms\n",
      "68:\tlearn: 0.1009780\ttotal: 258ms\tremaining: 116ms\n",
      "69:\tlearn: 0.1008701\ttotal: 261ms\tremaining: 112ms\n",
      "70:\tlearn: 0.1007714\ttotal: 264ms\tremaining: 108ms\n",
      "71:\tlearn: 0.1006929\ttotal: 267ms\tremaining: 104ms\n",
      "72:\tlearn: 0.1006511\ttotal: 269ms\tremaining: 99.6ms\n",
      "73:\tlearn: 0.1006040\ttotal: 272ms\tremaining: 95.5ms\n",
      "74:\tlearn: 0.1005557\ttotal: 275ms\tremaining: 91.5ms\n",
      "75:\tlearn: 0.1005028\ttotal: 278ms\tremaining: 87.7ms\n",
      "76:\tlearn: 0.1004490\ttotal: 281ms\tremaining: 83.8ms\n",
      "77:\tlearn: 0.1003915\ttotal: 283ms\tremaining: 80ms\n",
      "78:\tlearn: 0.1003359\ttotal: 287ms\tremaining: 76.2ms\n",
      "79:\tlearn: 0.1002797\ttotal: 290ms\tremaining: 72.4ms\n",
      "80:\tlearn: 0.1002568\ttotal: 292ms\tremaining: 68.5ms\n",
      "81:\tlearn: 0.1002085\ttotal: 295ms\tremaining: 64.8ms\n",
      "82:\tlearn: 0.1001765\ttotal: 298ms\tremaining: 61ms\n",
      "83:\tlearn: 0.1001346\ttotal: 301ms\tremaining: 57.3ms\n",
      "84:\tlearn: 0.1000949\ttotal: 304ms\tremaining: 53.7ms\n",
      "85:\tlearn: 0.1000534\ttotal: 307ms\tremaining: 50ms\n",
      "86:\tlearn: 0.1000238\ttotal: 310ms\tremaining: 46.3ms\n",
      "87:\tlearn: 0.0999735\ttotal: 313ms\tremaining: 42.6ms\n",
      "88:\tlearn: 0.0999384\ttotal: 316ms\tremaining: 39.1ms\n",
      "89:\tlearn: 0.0999165\ttotal: 319ms\tremaining: 35.4ms\n",
      "90:\tlearn: 0.0998903\ttotal: 322ms\tremaining: 31.8ms\n",
      "91:\tlearn: 0.0998613\ttotal: 325ms\tremaining: 28.3ms\n",
      "92:\tlearn: 0.0998363\ttotal: 328ms\tremaining: 24.7ms\n",
      "93:\tlearn: 0.0998029\ttotal: 331ms\tremaining: 21.1ms\n",
      "94:\tlearn: 0.0997811\ttotal: 334ms\tremaining: 17.6ms\n",
      "95:\tlearn: 0.0997759\ttotal: 336ms\tremaining: 14ms\n",
      "96:\tlearn: 0.0997508\ttotal: 339ms\tremaining: 10.5ms\n",
      "97:\tlearn: 0.0997216\ttotal: 342ms\tremaining: 6.98ms\n",
      "98:\tlearn: 0.0997006\ttotal: 345ms\tremaining: 3.49ms\n",
      "99:\tlearn: 0.0996861\ttotal: 348ms\tremaining: 0us\n",
      "0:\tlearn: 1.2539541\ttotal: 6.98ms\tremaining: 691ms\n",
      "1:\tlearn: 1.0386207\ttotal: 13.3ms\tremaining: 651ms\n",
      "2:\tlearn: 0.8867990\ttotal: 19.5ms\tremaining: 630ms\n",
      "3:\tlearn: 0.7666075\ttotal: 26.3ms\tremaining: 631ms\n",
      "4:\tlearn: 0.6727886\ttotal: 31.9ms\tremaining: 605ms\n",
      "5:\tlearn: 0.5949013\ttotal: 38ms\tremaining: 596ms\n",
      "6:\tlearn: 0.5305858\ttotal: 43.9ms\tremaining: 584ms\n",
      "7:\tlearn: 0.4771019\ttotal: 49.6ms\tremaining: 571ms\n",
      "8:\tlearn: 0.4316522\ttotal: 55.1ms\tremaining: 557ms\n",
      "9:\tlearn: 0.3933362\ttotal: 60.3ms\tremaining: 543ms\n",
      "10:\tlearn: 0.3588182\ttotal: 65ms\tremaining: 526ms\n",
      "11:\tlearn: 0.3289061\ttotal: 70.2ms\tremaining: 515ms\n",
      "12:\tlearn: 0.3027895\ttotal: 75ms\tremaining: 502ms\n",
      "13:\tlearn: 0.2800593\ttotal: 79.2ms\tremaining: 487ms\n",
      "14:\tlearn: 0.2601278\ttotal: 83.5ms\tremaining: 473ms\n",
      "15:\tlearn: 0.2425306\ttotal: 88ms\tremaining: 462ms\n",
      "16:\tlearn: 0.2269660\ttotal: 92ms\tremaining: 449ms\n",
      "17:\tlearn: 0.2132068\ttotal: 96ms\tremaining: 437ms\n",
      "18:\tlearn: 0.2019192\ttotal: 99.6ms\tremaining: 425ms\n",
      "19:\tlearn: 0.1910389\ttotal: 104ms\tremaining: 415ms\n",
      "20:\tlearn: 0.1814781\ttotal: 107ms\tremaining: 404ms\n",
      "21:\tlearn: 0.1730207\ttotal: 111ms\tremaining: 394ms\n",
      "22:\tlearn: 0.1659620\ttotal: 115ms\tremaining: 384ms\n",
      "23:\tlearn: 0.1593512\ttotal: 118ms\tremaining: 375ms\n",
      "24:\tlearn: 0.1531967\ttotal: 122ms\tremaining: 367ms\n",
      "25:\tlearn: 0.1479089\ttotal: 126ms\tremaining: 358ms\n",
      "26:\tlearn: 0.1436457\ttotal: 129ms\tremaining: 348ms\n",
      "27:\tlearn: 0.1393476\ttotal: 132ms\tremaining: 339ms\n",
      "28:\tlearn: 0.1353969\ttotal: 135ms\tremaining: 331ms\n",
      "29:\tlearn: 0.1319419\ttotal: 139ms\tremaining: 324ms\n",
      "30:\tlearn: 0.1289246\ttotal: 142ms\tremaining: 317ms\n",
      "31:\tlearn: 0.1261911\ttotal: 145ms\tremaining: 309ms\n",
      "32:\tlearn: 0.1238736\ttotal: 149ms\tremaining: 302ms\n",
      "33:\tlearn: 0.1218015\ttotal: 152ms\tremaining: 295ms\n",
      "34:\tlearn: 0.1198551\ttotal: 155ms\tremaining: 288ms\n",
      "35:\tlearn: 0.1181230\ttotal: 158ms\tremaining: 281ms\n",
      "36:\tlearn: 0.1168933\ttotal: 161ms\tremaining: 273ms\n",
      "37:\tlearn: 0.1153907\ttotal: 164ms\tremaining: 267ms\n",
      "38:\tlearn: 0.1145618\ttotal: 166ms\tremaining: 260ms\n",
      "39:\tlearn: 0.1135524\ttotal: 169ms\tremaining: 253ms\n",
      "40:\tlearn: 0.1125454\ttotal: 172ms\tremaining: 247ms\n",
      "41:\tlearn: 0.1116571\ttotal: 175ms\tremaining: 241ms\n",
      "42:\tlearn: 0.1108333\ttotal: 178ms\tremaining: 236ms\n",
      "43:\tlearn: 0.1100324\ttotal: 181ms\tremaining: 230ms\n",
      "44:\tlearn: 0.1092981\ttotal: 184ms\tremaining: 225ms\n",
      "45:\tlearn: 0.1087632\ttotal: 187ms\tremaining: 219ms\n",
      "46:\tlearn: 0.1081499\ttotal: 190ms\tremaining: 214ms\n",
      "47:\tlearn: 0.1076632\ttotal: 193ms\tremaining: 209ms\n",
      "48:\tlearn: 0.1072441\ttotal: 196ms\tremaining: 204ms\n",
      "49:\tlearn: 0.1068399\ttotal: 199ms\tremaining: 199ms\n",
      "50:\tlearn: 0.1064717\ttotal: 202ms\tremaining: 194ms\n",
      "51:\tlearn: 0.1061452\ttotal: 205ms\tremaining: 189ms\n",
      "52:\tlearn: 0.1058787\ttotal: 208ms\tremaining: 184ms\n",
      "53:\tlearn: 0.1056489\ttotal: 210ms\tremaining: 179ms\n",
      "54:\tlearn: 0.1054080\ttotal: 213ms\tremaining: 175ms\n",
      "55:\tlearn: 0.1052288\ttotal: 216ms\tremaining: 170ms\n",
      "56:\tlearn: 0.1050355\ttotal: 219ms\tremaining: 165ms\n",
      "57:\tlearn: 0.1048391\ttotal: 222ms\tremaining: 161ms\n",
      "58:\tlearn: 0.1046668\ttotal: 225ms\tremaining: 156ms\n",
      "59:\tlearn: 0.1044986\ttotal: 228ms\tremaining: 152ms\n",
      "60:\tlearn: 0.1043525\ttotal: 231ms\tremaining: 148ms\n",
      "61:\tlearn: 0.1042274\ttotal: 234ms\tremaining: 144ms\n",
      "62:\tlearn: 0.1041231\ttotal: 237ms\tremaining: 139ms\n",
      "63:\tlearn: 0.1040118\ttotal: 240ms\tremaining: 135ms\n",
      "64:\tlearn: 0.1039020\ttotal: 243ms\tremaining: 131ms\n",
      "65:\tlearn: 0.1038004\ttotal: 246ms\tremaining: 127ms\n",
      "66:\tlearn: 0.1037044\ttotal: 249ms\tremaining: 122ms\n",
      "67:\tlearn: 0.1036328\ttotal: 251ms\tremaining: 118ms\n",
      "68:\tlearn: 0.1035568\ttotal: 254ms\tremaining: 114ms\n",
      "69:\tlearn: 0.1035092\ttotal: 257ms\tremaining: 110ms\n",
      "70:\tlearn: 0.1034179\ttotal: 260ms\tremaining: 106ms\n",
      "71:\tlearn: 0.1033579\ttotal: 263ms\tremaining: 102ms\n",
      "72:\tlearn: 0.1032834\ttotal: 266ms\tremaining: 98.4ms\n",
      "73:\tlearn: 0.1032472\ttotal: 269ms\tremaining: 94.4ms\n",
      "74:\tlearn: 0.1032064\ttotal: 271ms\tremaining: 90.5ms\n",
      "75:\tlearn: 0.1031464\ttotal: 274ms\tremaining: 86.6ms\n",
      "76:\tlearn: 0.1031124\ttotal: 277ms\tremaining: 82.8ms\n",
      "77:\tlearn: 0.1030793\ttotal: 279ms\tremaining: 78.8ms\n",
      "78:\tlearn: 0.1030363\ttotal: 282ms\tremaining: 75ms\n",
      "79:\tlearn: 0.1029772\ttotal: 285ms\tremaining: 71.3ms\n",
      "80:\tlearn: 0.1029648\ttotal: 287ms\tremaining: 67.4ms\n",
      "81:\tlearn: 0.1029457\ttotal: 290ms\tremaining: 63.6ms\n",
      "82:\tlearn: 0.1028912\ttotal: 293ms\tremaining: 60ms\n",
      "83:\tlearn: 0.1028532\ttotal: 295ms\tremaining: 56.3ms\n",
      "84:\tlearn: 0.1028258\ttotal: 298ms\tremaining: 52.6ms\n",
      "85:\tlearn: 0.1027895\ttotal: 301ms\tremaining: 49ms\n",
      "86:\tlearn: 0.1027350\ttotal: 304ms\tremaining: 45.5ms\n",
      "87:\tlearn: 0.1026913\ttotal: 307ms\tremaining: 41.9ms\n",
      "88:\tlearn: 0.1026633\ttotal: 310ms\tremaining: 38.3ms\n",
      "89:\tlearn: 0.1026274\ttotal: 313ms\tremaining: 34.8ms\n",
      "90:\tlearn: 0.1026183\ttotal: 315ms\tremaining: 31.2ms\n",
      "91:\tlearn: 0.1025722\ttotal: 318ms\tremaining: 27.7ms\n",
      "92:\tlearn: 0.1025682\ttotal: 320ms\tremaining: 24.1ms\n",
      "93:\tlearn: 0.1025339\ttotal: 323ms\tremaining: 20.6ms\n",
      "94:\tlearn: 0.1024885\ttotal: 326ms\tremaining: 17.2ms\n",
      "95:\tlearn: 0.1024506\ttotal: 329ms\tremaining: 13.7ms\n",
      "96:\tlearn: 0.1024346\ttotal: 332ms\tremaining: 10.3ms\n",
      "97:\tlearn: 0.1024149\ttotal: 335ms\tremaining: 6.84ms\n",
      "98:\tlearn: 0.1023917\ttotal: 338ms\tremaining: 3.41ms\n",
      "99:\tlearn: 0.1023591\ttotal: 341ms\tremaining: 0us\n",
      "0:\tlearn: 1.2541750\ttotal: 7.68ms\tremaining: 760ms\n",
      "1:\tlearn: 1.0392893\ttotal: 14.3ms\tremaining: 698ms\n",
      "2:\tlearn: 0.8874572\ttotal: 20.2ms\tremaining: 653ms\n",
      "3:\tlearn: 0.7672993\ttotal: 27.4ms\tremaining: 658ms\n",
      "4:\tlearn: 0.6735124\ttotal: 33ms\tremaining: 627ms\n",
      "5:\tlearn: 0.5956149\ttotal: 39.2ms\tremaining: 614ms\n",
      "6:\tlearn: 0.5312865\ttotal: 44.9ms\tremaining: 597ms\n",
      "7:\tlearn: 0.4778120\ttotal: 50.1ms\tremaining: 577ms\n",
      "8:\tlearn: 0.4316331\ttotal: 55.4ms\tremaining: 560ms\n",
      "9:\tlearn: 0.3933983\ttotal: 60.3ms\tremaining: 542ms\n",
      "10:\tlearn: 0.3590621\ttotal: 65.5ms\tremaining: 530ms\n",
      "11:\tlearn: 0.3293532\ttotal: 70.6ms\tremaining: 518ms\n",
      "12:\tlearn: 0.3035000\ttotal: 75.5ms\tremaining: 505ms\n",
      "13:\tlearn: 0.2808895\ttotal: 80ms\tremaining: 491ms\n",
      "14:\tlearn: 0.2610732\ttotal: 84.7ms\tremaining: 480ms\n",
      "15:\tlearn: 0.2435976\ttotal: 89.4ms\tremaining: 470ms\n",
      "16:\tlearn: 0.2281471\ttotal: 93.9ms\tremaining: 459ms\n",
      "17:\tlearn: 0.2144644\ttotal: 98.2ms\tremaining: 447ms\n",
      "18:\tlearn: 0.2030932\ttotal: 102ms\tremaining: 434ms\n",
      "19:\tlearn: 0.1923089\ttotal: 106ms\tremaining: 426ms\n",
      "20:\tlearn: 0.1831123\ttotal: 110ms\tremaining: 413ms\n",
      "21:\tlearn: 0.1746838\ttotal: 113ms\tremaining: 402ms\n",
      "22:\tlearn: 0.1676133\ttotal: 117ms\tremaining: 392ms\n",
      "23:\tlearn: 0.1609301\ttotal: 121ms\tremaining: 383ms\n",
      "24:\tlearn: 0.1550584\ttotal: 125ms\tremaining: 375ms\n",
      "25:\tlearn: 0.1502033\ttotal: 128ms\tremaining: 365ms\n",
      "26:\tlearn: 0.1452870\ttotal: 132ms\tremaining: 357ms\n",
      "27:\tlearn: 0.1410815\ttotal: 136ms\tremaining: 349ms\n",
      "28:\tlearn: 0.1372080\ttotal: 139ms\tremaining: 341ms\n",
      "29:\tlearn: 0.1339015\ttotal: 142ms\tremaining: 332ms\n",
      "30:\tlearn: 0.1307945\ttotal: 146ms\tremaining: 324ms\n",
      "31:\tlearn: 0.1280728\ttotal: 149ms\tremaining: 316ms\n",
      "32:\tlearn: 0.1258804\ttotal: 152ms\tremaining: 308ms\n",
      "33:\tlearn: 0.1237289\ttotal: 155ms\tremaining: 301ms\n",
      "34:\tlearn: 0.1219058\ttotal: 158ms\tremaining: 294ms\n",
      "35:\tlearn: 0.1202524\ttotal: 161ms\tremaining: 286ms\n",
      "36:\tlearn: 0.1186952\ttotal: 164ms\tremaining: 279ms\n",
      "37:\tlearn: 0.1174098\ttotal: 167ms\tremaining: 272ms\n",
      "38:\tlearn: 0.1162419\ttotal: 170ms\tremaining: 265ms\n",
      "39:\tlearn: 0.1152232\ttotal: 172ms\tremaining: 259ms\n",
      "40:\tlearn: 0.1141800\ttotal: 175ms\tremaining: 252ms\n",
      "41:\tlearn: 0.1133913\ttotal: 178ms\tremaining: 246ms\n",
      "42:\tlearn: 0.1126383\ttotal: 181ms\tremaining: 240ms\n",
      "43:\tlearn: 0.1118931\ttotal: 184ms\tremaining: 234ms\n",
      "44:\tlearn: 0.1111854\ttotal: 187ms\tremaining: 229ms\n",
      "45:\tlearn: 0.1106240\ttotal: 190ms\tremaining: 223ms\n",
      "46:\tlearn: 0.1101391\ttotal: 193ms\tremaining: 217ms\n",
      "47:\tlearn: 0.1096947\ttotal: 196ms\tremaining: 212ms\n",
      "48:\tlearn: 0.1092298\ttotal: 199ms\tremaining: 207ms\n",
      "49:\tlearn: 0.1090419\ttotal: 201ms\tremaining: 201ms\n",
      "50:\tlearn: 0.1086070\ttotal: 204ms\tremaining: 196ms\n",
      "51:\tlearn: 0.1084311\ttotal: 206ms\tremaining: 191ms\n",
      "52:\tlearn: 0.1081536\ttotal: 209ms\tremaining: 185ms\n",
      "53:\tlearn: 0.1078973\ttotal: 212ms\tremaining: 180ms\n",
      "54:\tlearn: 0.1076871\ttotal: 215ms\tremaining: 176ms\n",
      "55:\tlearn: 0.1073734\ttotal: 218ms\tremaining: 171ms\n",
      "56:\tlearn: 0.1072668\ttotal: 220ms\tremaining: 166ms\n",
      "57:\tlearn: 0.1070529\ttotal: 223ms\tremaining: 162ms\n",
      "58:\tlearn: 0.1069219\ttotal: 226ms\tremaining: 157ms\n",
      "59:\tlearn: 0.1068080\ttotal: 229ms\tremaining: 152ms\n",
      "60:\tlearn: 0.1066412\ttotal: 232ms\tremaining: 148ms\n",
      "61:\tlearn: 0.1065197\ttotal: 235ms\tremaining: 144ms\n",
      "62:\tlearn: 0.1063942\ttotal: 238ms\tremaining: 140ms\n",
      "63:\tlearn: 0.1062268\ttotal: 241ms\tremaining: 135ms\n",
      "64:\tlearn: 0.1060786\ttotal: 244ms\tremaining: 131ms\n",
      "65:\tlearn: 0.1059857\ttotal: 247ms\tremaining: 127ms\n",
      "66:\tlearn: 0.1059552\ttotal: 249ms\tremaining: 123ms\n",
      "67:\tlearn: 0.1058844\ttotal: 252ms\tremaining: 118ms\n",
      "68:\tlearn: 0.1057466\ttotal: 255ms\tremaining: 114ms\n",
      "69:\tlearn: 0.1056966\ttotal: 257ms\tremaining: 110ms\n",
      "70:\tlearn: 0.1056034\ttotal: 260ms\tremaining: 106ms\n",
      "71:\tlearn: 0.1055315\ttotal: 263ms\tremaining: 102ms\n",
      "72:\tlearn: 0.1055156\ttotal: 266ms\tremaining: 98.2ms\n",
      "73:\tlearn: 0.1054639\ttotal: 268ms\tremaining: 94.3ms\n",
      "74:\tlearn: 0.1054121\ttotal: 271ms\tremaining: 90.5ms\n",
      "75:\tlearn: 0.1053513\ttotal: 274ms\tremaining: 86.6ms\n",
      "76:\tlearn: 0.1053295\ttotal: 277ms\tremaining: 82.7ms\n",
      "77:\tlearn: 0.1053040\ttotal: 279ms\tremaining: 78.7ms\n",
      "78:\tlearn: 0.1052189\ttotal: 282ms\tremaining: 75.1ms\n",
      "79:\tlearn: 0.1052092\ttotal: 284ms\tremaining: 71.1ms\n",
      "80:\tlearn: 0.1051554\ttotal: 287ms\tremaining: 67.4ms\n",
      "81:\tlearn: 0.1050919\ttotal: 290ms\tremaining: 63.7ms\n",
      "82:\tlearn: 0.1050341\ttotal: 293ms\tremaining: 60.1ms\n",
      "83:\tlearn: 0.1049890\ttotal: 296ms\tremaining: 56.4ms\n",
      "84:\tlearn: 0.1049698\ttotal: 299ms\tremaining: 52.7ms\n",
      "85:\tlearn: 0.1049246\ttotal: 302ms\tremaining: 49.1ms\n",
      "86:\tlearn: 0.1048929\ttotal: 304ms\tremaining: 45.5ms\n",
      "87:\tlearn: 0.1048458\ttotal: 308ms\tremaining: 41.9ms\n",
      "88:\tlearn: 0.1048052\ttotal: 311ms\tremaining: 38.4ms\n",
      "89:\tlearn: 0.1047706\ttotal: 313ms\tremaining: 34.8ms\n",
      "90:\tlearn: 0.1047384\ttotal: 316ms\tremaining: 31.3ms\n",
      "91:\tlearn: 0.1046984\ttotal: 320ms\tremaining: 27.8ms\n",
      "92:\tlearn: 0.1046642\ttotal: 323ms\tremaining: 24.3ms\n",
      "93:\tlearn: 0.1046349\ttotal: 326ms\tremaining: 20.8ms\n",
      "94:\tlearn: 0.1046003\ttotal: 329ms\tremaining: 17.3ms\n",
      "95:\tlearn: 0.1045727\ttotal: 332ms\tremaining: 13.8ms\n",
      "96:\tlearn: 0.1045462\ttotal: 335ms\tremaining: 10.3ms\n",
      "97:\tlearn: 0.1045098\ttotal: 338ms\tremaining: 6.89ms\n",
      "98:\tlearn: 0.1044821\ttotal: 341ms\tremaining: 3.44ms\n",
      "99:\tlearn: 0.1044528\ttotal: 344ms\tremaining: 0us\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6814.0    43.0    37.0   53.0  0.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    34.0     8.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.748096691281399\n",
      "Recall total:  0.7584005336923563\n",
      "F1 total:  0.7531036601498933\n",
      "BACC total:  0.7584005336923563\n",
      "MCC total:  0.9611774335535587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "start = time.time()\n",
    "\n",
    "bag_cat = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "base_classifier = bag_cat\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_cat'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baggin LGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6810.0    43.0    39.0   54.0  1.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    33.0     8.0  1228.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9768085583900651\n",
      "Precision total:  0.7473746910574443\n",
      "Recall total:  0.7584424852612487\n",
      "F1 total:  0.7527478265659621\n",
      "BACC total:  0.7584424852612487\n",
      "MCC total:  0.9608101024123531\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "base_classifier = lgbm\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_lgbm'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # Create a DMatrix for XGBoost\n",
    "# dtrain = xgb.DMatrix(X_train_01, label=y_train_01)\n",
    "# dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "\n",
    "# # Set XGBoost parameters\n",
    "# params = {\n",
    "#     'objective': 'multi:softmax',  # for multi-class classification\n",
    "#     'num_class': 5,  # specify the number of classes\n",
    "#     'max_depth': 3,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "# }\n",
    "\n",
    "# # Train the XGBoost model\n",
    "# num_round = 100\n",
    "# xgb_01 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# base_classifier = xgb\n",
    "\n",
    "# # Define the BaggingClassifier\n",
    "# bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# # Train the BaggingClassifier\n",
    "# bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "# name = 'bag_xgb'\n",
    "\n",
    "# pred_label = y_pred\n",
    "\n",
    "\n",
    "# metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "# Acc = metrics[0]\n",
    "# Precision = metrics[1]\n",
    "# Recall = metrics[2]\n",
    "# F1 = metrics[3]\n",
    "# BACC = metrics[4]\n",
    "# MCC = metrics[5]    \n",
    "\n",
    "\n",
    "# globals()[f\"{name}_acc_01\"] = Acc\n",
    "# globals()[f\"{name}_pre_01\"] = Precision\n",
    "# globals()[f\"{name}_rec_01\"] = Recall\n",
    "# globals()[f\"{name}_f1_01\"] = F1\n",
    "# globals()[f\"{name}_bacc_01\"] = BACC\n",
    "# globals()[f\"{name}_mcc_01\"] = MCC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6801.0    43.0    37.0   66.0  0.0\n",
      "1.0    54.0  4728.0    14.0    8.0  0.0\n",
      "2.0    34.0     8.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9759108251664547\n",
      "Precision total:  0.7407045908395433\n",
      "Recall total:  0.7579430074741611\n",
      "F1 total:  0.7488702680284745\n",
      "BACC total:  0.7579430074741611\n",
      "MCC total:  0.9593288885323793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "\n",
    "base_classifier = rf\n",
    "\n",
    "# Define the BaggingClassifier\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the BaggingClassifier\n",
    "bagging_classifier.fit(X_train_01, y_train_01)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_classifier.predict(X_test_01)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "name = 'bag_rf'\n",
    "\n",
    "pred_label = y_pred\n",
    "\n",
    "\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with many models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### do bootstrapping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Multiple subsets are created from the original dataset, selecting observations with replacement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "num_bootstraps = 10  # Adjust the number of bootstraps as needed\n",
    "\n",
    "original_data_df = X_train_01.assign(label = y_train_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_df = []\n",
    "for i in range(0,num_bootstraps): \n",
    "    boot_df.append(original_data_df.sample(frac = 1, replace=True).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31185</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31186</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31189 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dnn   rf  lgbm  xgb  label\n",
       "0      0.0  0.0   0.0  0.0    0.0\n",
       "1      0.0  0.0   0.0  0.0    0.0\n",
       "2      0.0  0.0   0.0  0.0    0.0\n",
       "3      1.0  1.0   1.0  1.0    1.0\n",
       "4      0.0  0.0   0.0  0.0    0.0\n",
       "...    ...  ...   ...  ...    ...\n",
       "31184  0.0  0.0   0.0  0.0    0.0\n",
       "31185  1.0  1.0   1.0  1.0    1.0\n",
       "31186  1.0  1.0   1.0  1.0    1.0\n",
       "31187  0.0  0.0   0.0  0.0    0.0\n",
       "31188  0.0  2.0   2.0  2.0    2.0\n",
       "\n",
       "[31189 rows x 5 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_df[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.A base model (weak model) is created on each of these subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_comb_pred = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "y_train_boot = boot_df[0].pop('label')\n",
    "X_train_boot = boot_df[0]\n",
    "clf.fit(X_train_boot, y_train_boot)\n",
    "preds_svm_01 = clf.predict(X_test_01)\n",
    "bag_comb_pred.append(preds_svm_01)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADA\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "ada = abc.fit(X_train_01, y_train_01)\n",
    "y_train_boot = boot_df[1].pop('label')\n",
    "X_train_boot = boot_df[1]\n",
    "preds_ada_01 = ada.predict(X_test_01)\n",
    "bag_comb_pred.append(preds_ada_01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2533149\ttest: 1.2535807\tbest: 1.2535807 (0)\ttotal: 8ms\tremaining: 792ms\n",
      "10:\tlearn: 0.3536299\ttest: 0.3541874\tbest: 0.3541874 (10)\ttotal: 75.5ms\tremaining: 611ms\n",
      "20:\tlearn: 0.1789955\ttest: 0.1800435\tbest: 0.1800435 (20)\ttotal: 120ms\tremaining: 453ms\n",
      "30:\tlearn: 0.1263620\ttest: 0.1274731\tbest: 0.1274731 (30)\ttotal: 156ms\tremaining: 348ms\n",
      "40:\tlearn: 0.1094235\ttest: 0.1107154\tbest: 0.1107154 (40)\ttotal: 186ms\tremaining: 268ms\n",
      "50:\tlearn: 0.1040628\ttest: 0.1054167\tbest: 0.1054167 (50)\ttotal: 214ms\tremaining: 205ms\n",
      "60:\tlearn: 0.1021227\ttest: 0.1036316\tbest: 0.1036316 (60)\ttotal: 241ms\tremaining: 154ms\n",
      "70:\tlearn: 0.1012287\ttest: 0.1029802\tbest: 0.1029802 (70)\ttotal: 270ms\tremaining: 110ms\n",
      "80:\tlearn: 0.1007444\ttest: 0.1026959\tbest: 0.1026959 (80)\ttotal: 298ms\tremaining: 69.9ms\n",
      "90:\tlearn: 0.1004662\ttest: 0.1026363\tbest: 0.1026272 (87)\ttotal: 327ms\tremaining: 32.3ms\n",
      "99:\tlearn: 0.1002874\ttest: 0.1025806\tbest: 0.1025806 (99)\ttotal: 350ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1025806059\n",
      "bestIteration = 99\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Catboost\n",
    "import catboost\n",
    "cat_01 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "y_train_boot = boot_df[2].pop('label')\n",
    "X_train_boot = boot_df[2]\n",
    "cat_01.fit(X_train_boot, y_train_boot, eval_set=(X_test_01, y_test_01), verbose=10)\n",
    "preds_cat = cat_01.predict(X_test_01)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "pred_label = preds_cat\n",
    "bag_comb_pred.append(preds_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "y_train_boot = boot_df[3].pop('label')\n",
    "X_train_boot = boot_df[3]\n",
    "if 1 == 1 and 0 == 0:\n",
    "    MLP = mlp.fit(X_train_boot, y_train_boot)\n",
    "    y_pred = MLP.predict_proba(X_test_01)\n",
    "    preds_mlp_01 = np.argmax(y_pred,axis = 1)\n",
    "\n",
    "bag_comb_pred.append(preds_mlp_01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#LGBM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "y_train_boot = boot_df[4].pop('label')\n",
    "X_train_boot = boot_df[4]\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "    lgbm.fit(X_train_boot, y_train_boot)\n",
    "    preds_lgbm_01 = lgbm.predict(X_test_01)\n",
    "    bag_comb_pred.append(preds_lgbm_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "y_train_boot = boot_df[5].pop('label')\n",
    "X_train_boot = boot_df[5]\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "    knn_clf_01.fit(X_train_boot,y_train_boot)\n",
    "if use_model_knn == 1:\n",
    "    preds_knn =knn_clf_01.predict(X_test_01)\n",
    "    bag_comb_pred.append(preds_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "y_train_boot = boot_df[6].pop('label')\n",
    "X_train_boot = boot_df[6]\n",
    "\n",
    "if True == True:\n",
    "    model_rf_01 = rf.fit(X_train_boot,y_train_boot)\n",
    "    preds_rf_01 = model_rf_01.predict(X_test_01)\n",
    "    bag_comb_pred.append(preds_rf_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 81/195 [===========>..................] - ETA: 0s - loss: 1.7716 - accuracy: 0.2569"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 1s 3ms/step - loss: 1.5690 - accuracy: 0.3264 - val_loss: 1.2768 - val_accuracy: 0.3471\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 1.1858 - accuracy: 0.5061 - val_loss: 1.0286 - val_accuracy: 0.5205\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 1.0130 - accuracy: 0.6429 - val_loss: 0.7443 - val_accuracy: 0.8432\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.8433 - accuracy: 0.7405 - val_loss: 0.5329 - val_accuracy: 0.8435\n",
      "Epoch 5/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.7434 - accuracy: 0.7741 - val_loss: 0.4487 - val_accuracy: 0.8424\n",
      "Epoch 6/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6956 - accuracy: 0.7880 - val_loss: 0.4066 - val_accuracy: 0.8855\n",
      "Epoch 7/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.8029 - val_loss: 0.3853 - val_accuracy: 0.9226\n",
      "Epoch 8/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.6447 - accuracy: 0.8094 - val_loss: 0.3792 - val_accuracy: 0.9227\n",
      "Epoch 9/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.6259 - accuracy: 0.8096 - val_loss: 0.3742 - val_accuracy: 0.9223\n",
      "Epoch 10/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.6121 - accuracy: 0.8180 - val_loss: 0.3720 - val_accuracy: 0.8363\n",
      "Epoch 11/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.8238 - val_loss: 0.3657 - val_accuracy: 0.9227\n",
      "Epoch 12/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5888 - accuracy: 0.8243 - val_loss: 0.3593 - val_accuracy: 0.9226\n",
      "Epoch 13/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5799 - accuracy: 0.8293 - val_loss: 0.3581 - val_accuracy: 0.9226\n",
      "Epoch 14/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.8325 - val_loss: 0.3562 - val_accuracy: 0.9226\n",
      "Epoch 15/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.8346 - val_loss: 0.3598 - val_accuracy: 0.9234\n",
      "Epoch 16/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8356 - val_loss: 0.3572 - val_accuracy: 0.9232\n",
      "Epoch 17/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5682 - accuracy: 0.8335 - val_loss: 0.3629 - val_accuracy: 0.9231\n",
      "Epoch 18/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5655 - accuracy: 0.8356 - val_loss: 0.3670 - val_accuracy: 0.9231\n",
      "Epoch 19/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5551 - accuracy: 0.8417 - val_loss: 0.3607 - val_accuracy: 0.9235\n",
      "Epoch 20/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5512 - accuracy: 0.8473 - val_loss: 0.3643 - val_accuracy: 0.9234\n",
      "Epoch 21/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5456 - accuracy: 0.8464 - val_loss: 0.3650 - val_accuracy: 0.9239\n",
      "Epoch 22/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5419 - accuracy: 0.8484 - val_loss: 0.3684 - val_accuracy: 0.9240\n",
      "Epoch 23/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.8484 - val_loss: 0.3683 - val_accuracy: 0.9240\n",
      "Epoch 24/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5388 - accuracy: 0.8503 - val_loss: 0.3764 - val_accuracy: 0.9239\n",
      "Epoch 25/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5397 - accuracy: 0.8562 - val_loss: 0.3710 - val_accuracy: 0.9239\n",
      "Epoch 26/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5315 - accuracy: 0.8585 - val_loss: 0.3723 - val_accuracy: 0.9240\n",
      "Epoch 27/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5277 - accuracy: 0.8614 - val_loss: 0.3695 - val_accuracy: 0.9277\n",
      "Epoch 28/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5273 - accuracy: 0.8629 - val_loss: 0.3724 - val_accuracy: 0.9277\n",
      "Epoch 29/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5224 - accuracy: 0.8652 - val_loss: 0.3692 - val_accuracy: 0.9277\n",
      "Epoch 30/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.8702 - val_loss: 0.3704 - val_accuracy: 0.9283\n",
      "Epoch 31/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.8705 - val_loss: 0.3635 - val_accuracy: 0.9283\n",
      "Epoch 32/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.8717 - val_loss: 0.3654 - val_accuracy: 0.9283\n",
      "Epoch 33/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5240 - accuracy: 0.8689 - val_loss: 0.3615 - val_accuracy: 0.9357\n",
      "Epoch 34/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.8744 - val_loss: 0.3659 - val_accuracy: 0.9357\n",
      "Epoch 35/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.8718 - val_loss: 0.3671 - val_accuracy: 0.9357\n",
      "Epoch 36/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.5101 - accuracy: 0.8725 - val_loss: 0.3731 - val_accuracy: 0.9283\n",
      "Epoch 37/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.4985 - accuracy: 0.8745 - val_loss: 0.3648 - val_accuracy: 0.9357\n",
      "Epoch 38/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.5042 - accuracy: 0.8723 - val_loss: 0.3674 - val_accuracy: 0.9357\n",
      "Epoch 39/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.4932 - accuracy: 0.8754 - val_loss: 0.3710 - val_accuracy: 0.9357\n",
      "Epoch 40/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.8770 - val_loss: 0.3638 - val_accuracy: 0.9357\n",
      "Epoch 41/100\n",
      "195/195 [==============================] - 0s 3ms/step - loss: 0.4917 - accuracy: 0.8781 - val_loss: 0.3626 - val_accuracy: 0.9357\n",
      "Epoch 42/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.5073 - accuracy: 0.8714 - val_loss: 0.3643 - val_accuracy: 0.9357\n",
      "Epoch 43/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.4945 - accuracy: 0.8761 - val_loss: 0.3670 - val_accuracy: 0.9357\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#Model Parameters\n",
    "y_train_boot = boot_df[7].pop('label')\n",
    "X_train_boot = boot_df[7]\n",
    "\n",
    "\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "num_columns = X_train_boot.shape[1]\n",
    "dnn_01 = tf.keras.Sequential()\n",
    "# Input layer\n",
    "dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "# Dense layers with dropout\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "# Output layer\n",
    "# dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "dnn_01.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "# dnn.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "dnn_01.fit(X_train_boot, y_train_boot, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "pred_dnn = dnn_01.predict(X_test_01)\n",
    "preds_dnn_01 = np.argmax(pred_dnn,axis = 1)\n",
    "bag_comb_pred.append(preds_dnn_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "#LogReg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_01 = LogisticRegression()\n",
    "y_train_boot = boot_df[8].pop('label')\n",
    "X_train_boot = boot_df[8]\n",
    "\n",
    "logreg_01.fit(X_train_boot,y_train_boot)\n",
    "preds_logreg =logreg_01.predict(X_test_01)\n",
    "bag_comb_pred.append(preds_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "y_train_boot = boot_df[9].pop('label')\n",
    "X_train_boot = boot_df[9]\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_boot, label=y_train_boot)\n",
    "dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_01 = xgb.train(params, dtrain, num_round)\n",
    "preds_xgb_01 = xgb_01.predict(dtest)\n",
    "bag_comb_pred.append(preds_xgb_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The models run in parallel and are independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model_0  model_1  model_2  model_3  model_4  model_5  model_6  model_7  \\\n",
      "0          0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "1          1.0      2.0      2.0        2      2.0      2.0      2.0        1   \n",
      "2          1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "3          1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "4          2.0      2.0      2.0        2      2.0      2.0      2.0        2   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "13362      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "13363      1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "13364      2.0      2.0      2.0        2      2.0      2.0      2.0        2   \n",
      "13365      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "13366      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "\n",
      "       model_8  model_9  \n",
      "0          0.0      0.0  \n",
      "1          1.0      2.0  \n",
      "2          1.0      1.0  \n",
      "3          1.0      1.0  \n",
      "4          2.0      2.0  \n",
      "...        ...      ...  \n",
      "13362      0.0      0.0  \n",
      "13363      1.0      1.0  \n",
      "13364      2.0      2.0  \n",
      "13365      0.0      0.0  \n",
      "13366      0.0      0.0  \n",
      "\n",
      "[13367 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "bag_vot_df = pd.DataFrame()\n",
    "for i in range(0,len(bag_comb_pred)):\n",
    "    bag_vot_df[f'model_{i}'] =  bag_comb_pred[i]\n",
    "print(bag_vot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       model_0  model_1  model_2  model_3  model_4  model_5  model_6  model_7  \\\n",
      "0          0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "1          1.0      2.0      2.0        2      2.0      2.0      2.0        1   \n",
      "2          1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "3          1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "4          2.0      2.0      2.0        2      2.0      2.0      2.0        2   \n",
      "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
      "13362      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "13363      1.0      1.0      1.0        1      1.0      1.0      1.0        1   \n",
      "13364      2.0      2.0      2.0        2      2.0      2.0      2.0        2   \n",
      "13365      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "13366      0.0      0.0      0.0        0      0.0      0.0      0.0        0   \n",
      "\n",
      "       model_8  model_9  ensemble  \n",
      "0          0.0      0.0         0  \n",
      "1          1.0      2.0         2  \n",
      "2          1.0      1.0         1  \n",
      "3          1.0      1.0         1  \n",
      "4          2.0      2.0         2  \n",
      "...        ...      ...       ...  \n",
      "13362      0.0      0.0         0  \n",
      "13363      1.0      1.0         1  \n",
      "13364      2.0      2.0         2  \n",
      "13365      0.0      0.0         0  \n",
      "13366      0.0      0.0         0  \n",
      "\n",
      "[13367 rows x 11 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        2\n",
       "2        1\n",
       "3        1\n",
       "4        2\n",
       "        ..\n",
       "13362    0\n",
       "13363    1\n",
       "13364    2\n",
       "13365    0\n",
       "13366    0\n",
       "Name: ensemble, Length: 13367, dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting start\n",
    "from scipy.stats import mode\n",
    "# bag_comb_pred_df = pd.DataFrame(bag_comb_pred)\n",
    "# Extract predictions columns\n",
    "\n",
    "# predictions = df[['dnn', 'rf', 'lgbm', 'ada', 'knn', 'mlp', 'svm','cat','xgb']]\n",
    "    # selected_columns = df.loc[:, ~df.columns.isin(['rf'])]\n",
    "predictions = bag_vot_df \n",
    "\n",
    "# predictions = bag_comb_pred_df.loc[:, ~df.columns.isin(['label'])] #df[column_features]\n",
    "\n",
    "# Use the mode function along axis 1 to get the most common prediction for each row\n",
    "ensemble_predictions, _ = mode(predictions.values, axis=1)\n",
    "\n",
    "# Add the ensemble predictions to the DataFrame\n",
    "bag_vot_df['ensemble'] = ensemble_predictions.astype(int)\n",
    "\n",
    "# Display the DataFrame with ensemble predictions\n",
    "print(bag_vot_df)\n",
    "\n",
    "pred_label = bag_vot_df ['ensemble'].values\n",
    "bag_vot_df.pop('ensemble')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6820.0    37.0    37.0   53.0  0.0\n",
      "1.0    59.0  4727.0    14.0    4.0  0.0\n",
      "2.0    35.0     7.0  1228.0    3.0  0.0\n",
      "3.0    41.0     0.0     7.0  287.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9771826138999028\n",
      "Precision total:  0.7482368299545823\n",
      "Recall total:  0.7574114535454235\n",
      "F1 total:  0.7527143346380285\n",
      "BACC total:  0.7574114535454235\n",
      "MCC total:  0.9614242871709717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "name='bag_comb'\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 83\n",
      "Trainable params: 83\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start_dnn = time.time()\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train_01.shape[1]\n",
    "\n",
    "dnn_01 = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# # Dense layers with dropout\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(3*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "# dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "# dnn.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "\n",
    "\n",
    "dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn_01.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "180/195 [==========================>...] - ETA: 0s - loss: 1.4075 - accuracy: 0.6022"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195/195 [==============================] - 1s 3ms/step - loss: 1.3849 - accuracy: 0.6049 - val_loss: 0.8756 - val_accuracy: 0.8775\n",
      "Epoch 2/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.9750 - accuracy: 0.6479 - val_loss: 0.6295 - val_accuracy: 0.8780\n",
      "Epoch 3/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.8233 - accuracy: 0.6819 - val_loss: 0.5226 - val_accuracy: 0.8774\n",
      "Epoch 4/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.7550 - accuracy: 0.7199 - val_loss: 0.4682 - val_accuracy: 0.8769\n",
      "Epoch 5/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6995 - accuracy: 0.7761 - val_loss: 0.4382 - val_accuracy: 0.8767\n",
      "Epoch 6/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6684 - accuracy: 0.7972 - val_loss: 0.4179 - val_accuracy: 0.9194\n",
      "Epoch 7/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6430 - accuracy: 0.8041 - val_loss: 0.4114 - val_accuracy: 0.8783\n",
      "Epoch 8/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6320 - accuracy: 0.8076 - val_loss: 0.3987 - val_accuracy: 0.9190\n",
      "Epoch 9/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6135 - accuracy: 0.8107 - val_loss: 0.3975 - val_accuracy: 0.9198\n",
      "Epoch 10/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.6012 - accuracy: 0.8161 - val_loss: 0.3930 - val_accuracy: 0.9200\n",
      "Epoch 11/100\n",
      "195/195 [==============================] - 1s 3ms/step - loss: 0.5984 - accuracy: 0.8251 - val_loss: 0.3879 - val_accuracy: 0.9202\n",
      "Epoch 12/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5907 - accuracy: 0.8269 - val_loss: 0.3780 - val_accuracy: 0.9206\n",
      "Epoch 13/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5877 - accuracy: 0.8334 - val_loss: 0.3797 - val_accuracy: 0.9203\n",
      "Epoch 14/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5712 - accuracy: 0.8390 - val_loss: 0.3768 - val_accuracy: 0.9203\n",
      "Epoch 15/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5749 - accuracy: 0.8397 - val_loss: 0.3759 - val_accuracy: 0.9203\n",
      "Epoch 16/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5639 - accuracy: 0.8470 - val_loss: 0.3767 - val_accuracy: 0.9195\n",
      "Epoch 17/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5560 - accuracy: 0.8462 - val_loss: 0.3690 - val_accuracy: 0.9203\n",
      "Epoch 18/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5626 - accuracy: 0.8456 - val_loss: 0.3659 - val_accuracy: 0.9208\n",
      "Epoch 19/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.8527 - val_loss: 0.3707 - val_accuracy: 0.9205\n",
      "Epoch 20/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.8586 - val_loss: 0.3625 - val_accuracy: 0.9271\n",
      "Epoch 21/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.8582 - val_loss: 0.3691 - val_accuracy: 0.9205\n",
      "Epoch 22/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5446 - accuracy: 0.8606 - val_loss: 0.3606 - val_accuracy: 0.9271\n",
      "Epoch 23/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.8630 - val_loss: 0.3595 - val_accuracy: 0.9210\n",
      "Epoch 24/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5323 - accuracy: 0.8653 - val_loss: 0.3650 - val_accuracy: 0.9210\n",
      "Epoch 25/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5375 - accuracy: 0.8648 - val_loss: 0.3630 - val_accuracy: 0.9210\n",
      "Epoch 26/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5334 - accuracy: 0.8641 - val_loss: 0.3649 - val_accuracy: 0.9210\n",
      "Epoch 27/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5325 - accuracy: 0.8651 - val_loss: 0.3649 - val_accuracy: 0.9210\n",
      "Epoch 28/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5198 - accuracy: 0.8666 - val_loss: 0.3588 - val_accuracy: 0.9277\n",
      "Epoch 29/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5232 - accuracy: 0.8688 - val_loss: 0.3566 - val_accuracy: 0.9367\n",
      "Epoch 30/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.8651 - val_loss: 0.3653 - val_accuracy: 0.9210\n",
      "Epoch 31/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.8680 - val_loss: 0.3564 - val_accuracy: 0.9360\n",
      "Epoch 32/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5159 - accuracy: 0.8680 - val_loss: 0.3581 - val_accuracy: 0.9277\n",
      "Epoch 33/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5193 - accuracy: 0.8710 - val_loss: 0.3585 - val_accuracy: 0.9277\n",
      "Epoch 34/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.8686 - val_loss: 0.3565 - val_accuracy: 0.9277\n",
      "Epoch 35/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.8720 - val_loss: 0.3547 - val_accuracy: 0.9367\n",
      "Epoch 36/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5102 - accuracy: 0.8721 - val_loss: 0.3538 - val_accuracy: 0.9367\n",
      "Epoch 37/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.8669 - val_loss: 0.3607 - val_accuracy: 0.9271\n",
      "Epoch 38/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.8681 - val_loss: 0.3569 - val_accuracy: 0.9367\n",
      "Epoch 39/100\n",
      "195/195 [==============================] - 0s 2ms/step - loss: 0.5222 - accuracy: 0.8707 - val_loss: 0.3606 - val_accuracy: 0.9367\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "try:\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    # Convert Y_test back to its original format\n",
    "    # y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    # dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size)\n",
    "    dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # End the timer\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    # joblib.dump(dnn_01, 'dnn_level_01.joblib')\n",
    "    # dnn_01.save(\"dnn_level_01.h5\")\n",
    "\n",
    "    # Calculate the time taken and print it out\n",
    "    # print(f'Time taken for training: {time_taken} seconds')\n",
    "except: \n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dnn_01 = load_model(\"dnn_level_01.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "try:\n",
    "    start = time.time()\n",
    "    pred_dnn = dnn_01.predict(X_test_01)\n",
    "    preds_dnn_01 = np.argmax(pred_dnn,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "except:\n",
    "        with open(output_file_name, \"a\") as f: print('error', file = f)\n",
    "        preds_dnn_01 = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0  3.0  4.0\n",
      "0.0  6789.0    92.0    66.0  0.0  0.0\n",
      "1.0   152.0  4635.0    17.0  0.0  0.0\n",
      "2.0    32.0    94.0  1147.0  0.0  0.0\n",
      "3.0    41.0    90.0   204.0  0.0  0.0\n",
      "4.0     3.0     0.0     5.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9404503628338445\n",
      "Precision total:  0.5416776843537068\n",
      "Recall total:  0.5686197123822615\n",
      "F1 total:  0.5544844483397615\n",
      "BACC total:  0.5686197123822615\n",
      "MCC total:  0.8989145776368807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    name = 'dnn'\n",
    "    pred_label = preds_dnn_01\n",
    "        \n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start_dnn\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n",
    "except: None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start_svm = time.time()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "clf.fit(X_train_01, y_train_01)\n",
    "end = time.time()\n",
    "clf.score(X_train_01, y_train_01)\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "joblib.dump(clf, 'svm_level_01.joblib')\n",
    "\n",
    "\n",
    "clf = loaded_model = joblib.load('svm_level_01.joblib')\n",
    "\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "preds_svm_01 = clf.predict(X_test_01)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6775.0    68.0    30.0   74.0  0.0\n",
      "1.0   393.0  4377.0    24.0   10.0  0.0\n",
      "2.0    34.0    85.0  1097.0   57.0  0.0\n",
      "3.0    41.0     0.0    31.0  263.0  0.0\n",
      "4.0     2.0     1.0     1.0    4.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9360365078177602\n",
      "Precision total:  0.6946101802118573\n",
      "Recall total:  0.7066350774083011\n",
      "F1 total:  0.6987575234351153\n",
      "BACC total:  0.7066350774083011\n",
      "MCC total:  0.8918628137323983\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_label = preds_svm_01\n",
    "name = 'svm'\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start_svm\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6806.0    37.0    37.0   66.0  1.0\n",
      "1.0    58.0  4720.0    21.0    5.0  0.0\n",
      "2.0    35.0     7.0  1226.0    5.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9756115807585846\n",
      "Precision total:  0.740647588728183\n",
      "Recall total:  0.7575967895239228\n",
      "F1 total:  0.7487061247778477\n",
      "BACC total:  0.7575967895239228\n",
      "MCC total:  0.9588253035723328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start_rf = time.time()\n",
    "\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if True == True:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf_01 = rf.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(model_rf_01, X_train_01, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf_01, 'rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    model_rf_01  = joblib.load('rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf_01 = model_rf_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "pred_label = preds_rf_01\n",
    "name='rf'\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start_rf\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6818.0    37.0    37.0   54.0  1.0\n",
      "1.0    60.0  4726.0    14.0    4.0  0.0\n",
      "2.0    35.0     7.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.7477891714131457\n",
      "Recall total:  0.7583491634209292\n",
      "F1 total:  0.7529089816824175\n",
      "BACC total:  0.7583491634209292\n",
      "MCC total:  0.9611771014915665\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "start_lgbm = time.time()\n",
    "\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    lgbm = joblib.load('lgbm_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm_01 = lgbm.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    pred_label = preds_lgbm_01\n",
    "    name='lgbm'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start_lgbm\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6806.0    37.0    38.0   66.0  0.0\n",
      "1.0    59.0  4727.0    14.0    4.0  0.0\n",
      "2.0    35.0     7.0  1225.0    6.0  0.0\n",
      "3.0    41.0     0.0     6.0  288.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9759856362684223\n",
      "Precision total:  0.7412371201178718\n",
      "Recall total:  0.7571340892211509\n",
      "F1 total:  0.7487977493724916\n",
      "BACC total:  0.7571340892211509\n",
      "MCC total:  0.9594408017182013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start_mlp = time.time()\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    MLP = joblib.load('mlp_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test_01)\n",
    "    preds_mlp_01 = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP 01 model', file = f)\n",
    "    pred_label = preds_mlp_01\n",
    "    name='mlp'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start_mlp\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6788.0    42.0    63.0   53.0  1.0\n",
      "1.0    58.0  4717.0    26.0    3.0  0.0\n",
      "2.0    33.0    25.0  1212.0    3.0  0.0\n",
      "3.0    41.0     0.0     6.0  288.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9729183810877534\n",
      "Precision total:  0.7423982090774246\n",
      "Recall total:  0.7541571407070778\n",
      "F1 total:  0.7481408519574112\n",
      "BACC total:  0.7541571407070778\n",
      "MCC total:  0.9542852143741783\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "# from sklearn.multioutput import MultiOutputClassifier\n",
    "start_ada = time.time()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_01.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    ada = joblib.load('ada_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada_01 = ada.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA 01 model', file = f)\n",
    "\n",
    "\n",
    "    pred_label = preds_ada_01\n",
    "    name='ada'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start_ada\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6836.0    43.0    38.0   30.0  0.0\n",
      "1.0    59.0  4730.0    14.0    1.0  0.0\n",
      "2.0    38.0     8.0  1223.0    4.0  0.0\n",
      "3.0   214.0     0.0     5.0  116.0  0.0\n",
      "4.0     6.0     0.0     0.0    2.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9654372708910002\n",
      "Precision total:  0.7317308780899096\n",
      "Recall total:  0.6551218817602262\n",
      "F1 total:  0.6780205078436203\n",
      "BACC total:  0.6551218817602262\n",
      "MCC total:  0.9412285447556674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "start_knn = time.time()\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf_01, 'knn_01.joblib')\n",
    "\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf_01 = joblib.load('knn_01.joblib')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf_01.predict(X_test_01)\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN 01 model', file = f)\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    name='knn'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start_knn\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Logistic Regression Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LR \n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6761.0    92.0    29.0   65.0  0.0\n",
      "1.0   279.0  4501.0    16.0    8.0  0.0\n",
      "2.0    35.0   114.0  1094.0   30.0  0.0\n",
      "3.0    41.0     0.0    31.0  263.0  0.0\n",
      "4.0     2.0     1.0     1.0    4.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9440412957282861\n",
      "Precision total:  0.7101865588373641\n",
      "Recall total:  0.7109230628547418\n",
      "F1 total:  0.7098260037732529\n",
      "BACC total:  0.7109230628547418\n",
      "MCC total:  0.9050050973807555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Logistic Regression Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "logreg_01 = LogisticRegression()\n",
    "start_lr = time.time()\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LR ')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LR', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    logreg_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(logreg_01, 'logreg_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    logreg_01 = joblib.load('logreg_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #lR\n",
    "    start = time.time()\n",
    "    preds_logreg =logreg_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#LR\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LR 01 model', file = f)\n",
    "\n",
    "    pred_label = preds_logreg\n",
    "    # pred_label = label[ypred]\n",
    "    name='lr'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start_lr\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2535772\ttest: 1.2535988\tbest: 1.2535988 (0)\ttotal: 7.05ms\tremaining: 698ms\n",
      "10:\tlearn: 0.3555002\ttest: 0.3552112\tbest: 0.3552112 (10)\ttotal: 62.3ms\tremaining: 504ms\n",
      "20:\tlearn: 0.1799081\ttest: 0.1794701\tbest: 0.1794701 (20)\ttotal: 99.8ms\tremaining: 376ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30:\tlearn: 0.1270475\ttest: 0.1265006\tbest: 0.1265006 (30)\ttotal: 132ms\tremaining: 295ms\n",
      "40:\tlearn: 0.1107173\ttest: 0.1098160\tbest: 0.1098160 (40)\ttotal: 164ms\tremaining: 235ms\n",
      "50:\tlearn: 0.1051834\ttest: 0.1040343\tbest: 0.1040343 (50)\ttotal: 194ms\tremaining: 187ms\n",
      "60:\tlearn: 0.1031147\ttest: 0.1020319\tbest: 0.1020319 (60)\ttotal: 226ms\tremaining: 145ms\n",
      "70:\tlearn: 0.1022579\ttest: 0.1013140\tbest: 0.1013140 (70)\ttotal: 262ms\tremaining: 107ms\n",
      "80:\tlearn: 0.1017087\ttest: 0.1008360\tbest: 0.1008360 (80)\ttotal: 292ms\tremaining: 68.5ms\n",
      "90:\tlearn: 0.1013916\ttest: 0.1006072\tbest: 0.1006072 (90)\ttotal: 322ms\tremaining: 31.8ms\n",
      "99:\tlearn: 0.1011591\ttest: 0.1005284\tbest: 0.1005174 (98)\ttotal: 350ms\tremaining: 0us\n",
      "\n",
      "bestTest = 0.100517435\n",
      "bestIteration = 98\n",
      "\n",
      "Shrink model to first 99 iterations.\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6814.0    43.0    37.0   53.0  0.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    34.0     8.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.748096691281399\n",
      "Recall total:  0.7584005336923563\n",
      "F1 total:  0.7531036601498933\n",
      "BACC total:  0.7584005336923563\n",
      "MCC total:  0.9611774335535587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "start = time.time()\n",
    "\n",
    "cat_01 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "cat_01.fit(X_train_01, y_train_01, eval_set=(X_test_01, y_test_01), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = cat_01.predict(X_test_01)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('catboost', file = f)\n",
    "\n",
    "\n",
    "pred_label = preds_cat\n",
    "name='cat'\n",
    "metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "Acc = metrics[0]\n",
    "Precision = metrics[1]\n",
    "Recall = metrics[2]\n",
    "F1 = metrics[3]\n",
    "BACC = metrics[4]\n",
    "MCC = metrics[5]    \n",
    "\n",
    "\n",
    "globals()[f\"{name}_acc_01\"] = Acc\n",
    "globals()[f\"{name}_pre_01\"] = Precision\n",
    "globals()[f\"{name}_rec_01\"] = Recall\n",
    "globals()[f\"{name}_f1_01\"] = F1\n",
    "globals()[f\"{name}_bacc_01\"] = BACC\n",
    "globals()[f\"{name}_mcc_01\"] = MCC\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0     2.0    3.0  4.0\n",
      "0.0  6814.0    43.0    37.0   53.0  0.0\n",
      "1.0    56.0  4730.0    14.0    4.0  0.0\n",
      "2.0    34.0     8.0  1227.0    4.0  0.0\n",
      "3.0    41.0     0.0     5.0  289.0  0.0\n",
      "4.0     3.0     0.0     0.0    5.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770329916959677\n",
      "Precision total:  0.748096691281399\n",
      "Recall total:  0.7584005336923563\n",
      "F1 total:  0.7531036601498933\n",
      "BACC total:  0.7584005336923563\n",
      "MCC total:  0.9611774335535587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "start = time.time()\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_01, label=y_train_01)\n",
    "dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_01 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb_01 = xgb_01.predict(dtest)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "    pred_label = preds_xgb_01\n",
    "    name='xgb'\n",
    "    metrics = confusion_metrics(name, pred_label, y_test_01)\n",
    "\n",
    "    Acc = metrics[0]\n",
    "    Precision = metrics[1]\n",
    "    Recall = metrics[2]\n",
    "    F1 = metrics[3]\n",
    "    BACC = metrics[4]\n",
    "    MCC = metrics[5]    \n",
    "\n",
    "\n",
    "    globals()[f\"{name}_acc_01\"] = Acc\n",
    "    globals()[f\"{name}_pre_01\"] = Precision\n",
    "    globals()[f\"{name}_rec_01\"] = Recall\n",
    "    globals()[f\"{name}_f1_01\"] = F1\n",
    "    globals()[f\"{name}_bacc_01\"] = BACC\n",
    "    globals()[f\"{name}_mcc_01\"] = MCC\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    globals()[f\"{name}_time_01\"] = time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Generating Summary Metric Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+----------+----------+----------+\n",
      "| Models      |   ACC-01 |   PRE-01 |   REC-01 |    F1-01 |\n",
      "+=============+==========+==========+==========+==========+\n",
      "| Bag_knn     | 0.977033 | 0.748408 | 0.758401 | 0.753272 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| CAT         | 0.977033 | 0.748097 | 0.758401 | 0.753104 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| XGB         | 0.977033 | 0.748097 | 0.758401 | 0.753104 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_cat     | 0.977033 | 0.748097 | 0.758401 | 0.753104 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_DT      | 0.976809 | 0.747832 | 0.758442 | 0.75299  |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_mlp     | 0.977033 | 0.748305 | 0.757794 | 0.752928 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| LGBM        | 0.977033 | 0.747789 | 0.758349 | 0.752909 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_lgbm    | 0.976809 | 0.747375 | 0.758442 | 0.752748 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_comb    | 0.977183 | 0.748237 | 0.757411 | 0.752714 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_rf      | 0.975911 | 0.740705 | 0.757943 | 0.74887  |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| MLP         | 0.975986 | 0.741237 | 0.757134 | 0.748798 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| RF          | 0.975612 | 0.740648 | 0.757597 | 0.748706 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| ADA         | 0.972918 | 0.742398 | 0.754157 | 0.748141 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_ada     | 0.97247  | 0.741974 | 0.753326 | 0.747522 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_LR      | 0.944041 | 0.711227 | 0.710923 | 0.710404 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| LR          | 0.944041 | 0.710187 | 0.710923 | 0.709826 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| SVM         | 0.936037 | 0.69461  | 0.706635 | 0.698758 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| Bag_svm     | 0.935363 | 0.694661 | 0.70626  | 0.698564 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| KNN         | 0.965437 | 0.731731 | 0.655122 | 0.678021 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| VOTING      | 0.934989 | 0.72212  | 0.571448 | 0.589588 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| DNN         | 0.94045  | 0.541678 | 0.56862  | 0.554484 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| avg         | 0.929004 | 0.537886 | 0.562045 | 0.549401 |\n",
      "+-------------+----------+----------+----------+----------+\n",
      "| weighed_avg | 0.884641 | 0.547221 | 0.470751 | 0.488753 |\n",
      "+-------------+----------+----------+----------+----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING',\n",
    "                'Bag_svm',\n",
    "                'Bag_knn',\n",
    "                'Bag_DT',\n",
    "                'Bag_LR',\n",
    "                'Bag_mlp',\n",
    "\n",
    "                'Bag_rf',\n",
    "                'Bag_ada',\n",
    "                'Bag_lgbm',\n",
    "                # 'Bag_xgb',\n",
    "                'Bag_cat',\n",
    "                'Bag_comb',\n",
    "                'avg',\n",
    "                'weighed_avg'\n",
    "                ]\n",
    "\n",
    "data = [[\"\" for _ in range(5)] for _ in range(len(names_models))]\n",
    "\n",
    "level_01_acc = [\n",
    "                ada_acc_01,\n",
    "                svm_acc_01,\n",
    "                dnn_acc_01,\n",
    "                mlp_acc_01,\n",
    "                knn_acc_01,\n",
    "                cat_acc_01,\n",
    "                xgb_acc_01,\n",
    "                lgbm_acc_01,\n",
    "                rf_acc_01,\n",
    "                lr_acc_01,\n",
    "                voting_acc_01,\n",
    "                bag_svm_acc_01,\n",
    "                bag_knn_acc_01,\n",
    "                bag_dt_acc_01,\n",
    "                bag_lr_acc_01,\n",
    "                bag_mlp_acc_01,\n",
    "\n",
    "                bag_rf_acc_01,\n",
    "                bag_ada_acc_01,\n",
    "                bag_lgbm_acc_01,\n",
    "                # bag_xgb_acc_01,\n",
    "                bag_cat_acc_01,\n",
    "                bag_comb_acc_01,\n",
    "\n",
    "                avg_acc_01,\n",
    "                weighed_avg_acc_01\n",
    "                ]  \n",
    "\n",
    "\n",
    "level_01_pre = [\n",
    "                ada_pre_01,\n",
    "                svm_pre_01,\n",
    "                dnn_pre_01,\n",
    "                mlp_pre_01,\n",
    "                knn_pre_01,\n",
    "                cat_pre_01,\n",
    "                xgb_pre_01,\n",
    "                lgbm_pre_01,\n",
    "                rf_pre_01,\n",
    "                lr_pre_01,\n",
    "                voting_pre_01,\n",
    "                bag_svm_pre_01,\n",
    "                bag_knn_pre_01,\n",
    "                bag_dt_pre_01,\n",
    "                bag_lr_pre_01,\n",
    "                bag_mlp_pre_01,\n",
    "\n",
    "                bag_rf_pre_01,\n",
    "                bag_ada_pre_01,\n",
    "                bag_lgbm_pre_01,\n",
    "                # bag_xgb_pre_01,\n",
    "                bag_cat_pre_01,\n",
    "                bag_comb_pre_01,\n",
    "\n",
    "                avg_pre_01,\n",
    "                weighed_avg_pre_01\n",
    "                ]  \n",
    "\n",
    "level_01_rec = [\n",
    "                ada_rec_01,\n",
    "                svm_rec_01,\n",
    "                dnn_rec_01,\n",
    "                mlp_rec_01,\n",
    "                knn_rec_01,\n",
    "                cat_rec_01,\n",
    "                xgb_rec_01,\n",
    "                lgbm_rec_01,\n",
    "                rf_rec_01,\n",
    "                lr_rec_01,\n",
    "                voting_rec_01,\n",
    "                bag_svm_rec_01,\n",
    "                bag_knn_rec_01,\n",
    "                bag_dt_rec_01,\n",
    "                bag_lr_rec_01,\n",
    "                bag_mlp_rec_01,\n",
    "\n",
    "                bag_rf_rec_01,\n",
    "                bag_ada_rec_01,\n",
    "                bag_lgbm_rec_01,\n",
    "                # bag_xgb_rec_01,\n",
    "                bag_cat_rec_01,\n",
    "                bag_comb_rec_01,\n",
    "\n",
    "                avg_rec_01,\n",
    "                weighed_avg_rec_01\n",
    "                ]  \n",
    "\n",
    "level_01_f1 = [\n",
    "                ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01,\n",
    "                bag_svm_f1_01,\n",
    "                bag_knn_f1_01,\n",
    "                bag_dt_f1_01,\n",
    "                bag_lr_f1_01,\n",
    "                bag_mlp_f1_01,\n",
    "\n",
    "                bag_rf_f1_01,\n",
    "                bag_ada_f1_01,\n",
    "                bag_lgbm_f1_01,\n",
    "                # bag_xgb_f1_01,\n",
    "                bag_cat_f1_01,\n",
    "                bag_comb_f1_01,\n",
    "\n",
    "                avg_f1_01,\n",
    "                weighed_avg_f1_01\n",
    "                ]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine data into a list of tuples for sorting\n",
    "model_data = list(zip(names_models, level_01_acc, level_01_pre, level_01_rec, level_01_f1))\n",
    "\n",
    "# Sort by F1-01 score in descending order\n",
    "model_data_sorted = sorted(model_data, key=lambda x: x[4], reverse=True)\n",
    "\n",
    "# Separate the sorted data back into individual lists\n",
    "sorted_names_models, sorted_level_01_acc, sorted_level_01_pre, sorted_level_01_rec, sorted_level_01_f1 = zip(*model_data_sorted)\n",
    "\n",
    "# Assign the sorted data to the table\n",
    "for i in range(len(sorted_names_models)):\n",
    "    data[i][0] = sorted_names_models[i]\n",
    "    data[i][1] = sorted_level_01_acc[i]\n",
    "    data[i][2] = sorted_level_01_pre[i] \n",
    "    data[i][3] = sorted_level_01_rec[i] \n",
    "    data[i][4] = sorted_level_01_f1[i]\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Models\", \"ACC-01\", \"PRE-01\", \"REC-01\", \"F1-01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "with open(output_file_name, \"a\") as f: print('Summary table', file = f)\n",
    "if pick_prob == 1: \n",
    "    with open(output_file_name, \"a\") as f: print('Level 01 - Probabilities', file = f)\n",
    "else:\n",
    "    with open(output_file_name, \"a\") as f: print('Level 01 - CLASSES', file = f)\n",
    "if feature_selection_bit == 1: \n",
    "    with open(output_file_name, \"a\") as f: print('Feature Selection was applied', file = f)\n",
    "else:\n",
    "    with open(output_file_name, \"a\") as f: print('All features were used', file = f)\n",
    "\n",
    "\n",
    "    \n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+\n",
      "| Models      |   time-01(sec) |\n",
      "+=============+================+\n",
      "| avg         |      0.0900705 |\n",
      "+-------------+----------------+\n",
      "| weighed_avg |      0.113719  |\n",
      "+-------------+----------------+\n",
      "| Bag_DT      |      0.140293  |\n",
      "+-------------+----------------+\n",
      "| SVM         |      0.171345  |\n",
      "+-------------+----------------+\n",
      "| VOTING      |      0.281623  |\n",
      "+-------------+----------------+\n",
      "| RF          |      0.310224  |\n",
      "+-------------+----------------+\n",
      "| ADA         |      0.504403  |\n",
      "+-------------+----------------+\n",
      "| CAT         |      0.53714   |\n",
      "+-------------+----------------+\n",
      "| LR          |      0.970188  |\n",
      "+-------------+----------------+\n",
      "| Bag_svm     |      1.33788   |\n",
      "+-------------+----------------+\n",
      "| KNN         |      2.04351   |\n",
      "+-------------+----------------+\n",
      "| Bag_rf      |      3.04271   |\n",
      "+-------------+----------------+\n",
      "| Bag_ada     |      3.79403   |\n",
      "+-------------+----------------+\n",
      "| Bag_cat     |      4.46931   |\n",
      "+-------------+----------------+\n",
      "| Bag_LR      |      9.47927   |\n",
      "+-------------+----------------+\n",
      "| MLP         |      9.72239   |\n",
      "+-------------+----------------+\n",
      "| Bag_knn     |     17.0837    |\n",
      "+-------------+----------------+\n",
      "| DNN         |     19.1406    |\n",
      "+-------------+----------------+\n",
      "| Bag_comb    |     36.4508    |\n",
      "+-------------+----------------+\n",
      "| XGB         |     58.5499    |\n",
      "+-------------+----------------+\n",
      "| Bag_mlp     |    107.793     |\n",
      "+-------------+----------------+\n",
      "| Bag_lgbm    |    115.826     |\n",
      "+-------------+----------------+\n",
      "| LGBM        |    366.456     |\n",
      "+-------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# implement time table\n",
    "from tabulate import tabulate\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING',\n",
    "                'Bag_svm',\n",
    "                'Bag_knn',\n",
    "                'Bag_DT',\n",
    "                'Bag_LR',\n",
    "                'Bag_mlp',\n",
    "\n",
    "                'Bag_rf',\n",
    "                'Bag_ada',\n",
    "                'Bag_lgbm',\n",
    "                # 'Bag_xgb',\n",
    "                'Bag_cat',\n",
    "                'Bag_comb',\n",
    "                'avg',\n",
    "                'weighed_avg'\n",
    "                ]\n",
    "\n",
    "data = [[\"\" for _ in range(2)] for _ in range(len(names_models))]\n",
    "\n",
    "level_01_time = [\n",
    "                ada_time_01,\n",
    "                svm_time_01,\n",
    "                dnn_time_01,\n",
    "                mlp_time_01,\n",
    "                knn_time_01,\n",
    "                cat_time_01,\n",
    "                xgb_time_01,\n",
    "                lgbm_time_01,\n",
    "                rf_time_01,\n",
    "                lr_time_01,\n",
    "                voting_time_01,\n",
    "                bag_svm_time_01,\n",
    "                bag_knn_time_01,\n",
    "                bag_dt_time_01,\n",
    "                bag_lr_time_01,\n",
    "                bag_mlp_time_01,\n",
    "\n",
    "                bag_rf_time_01,\n",
    "                bag_ada_time_01,\n",
    "                bag_lgbm_time_01,\n",
    "                # bag_xgb_time_01,\n",
    "                bag_cat_time_01,\n",
    "                bag_comb_time_01,\n",
    "\n",
    "                avg_time_01,\n",
    "                weighed_avg_time_01\n",
    "                ]  \n",
    "\n",
    "\n",
    "# Combine data into a list of tuples for sorting\n",
    "model_data = list(zip(names_models, level_01_time))\n",
    "\n",
    "# Sort by F1-01 score in descending order\n",
    "model_data_sorted = sorted(model_data, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Separate the sorted data back into individual lists\n",
    "sorted_names_models, sorted_level_01_time = zip(*model_data_sorted)\n",
    "\n",
    "# Assign the sorted data to the table\n",
    "for i in range(len(sorted_names_models)):\n",
    "    data[i][0] = sorted_names_models[i]\n",
    "    data[i][1] = sorted_level_01_time[i]\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Models\", \"time-01(sec)\"]\n",
    "\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "with open(output_file_name, \"a\") as f: print('Time is counted is seconds', file = f)\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n",
    "end_program = time.time()\n",
    "time_program = end_program - start_program\n",
    "with open(output_file_name, \"a\") as f: print('Running time of entire program is:', time_program ,' seconds',file = f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_feature_importance == 1:\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating SHAP explanation')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "  with open(output_file_name, \"a\") as f:print('ADA FEATURE IMPORTANCE',file = f)\n",
    "\n",
    "      #START TIMER MODEL\n",
    "  start = time.time()\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating explainer')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "  test = X_test_01\n",
    "  train = X_train_01\n",
    "  # ## Summary Bar Plot Global\n",
    "  start_index = 0\n",
    "  end_index = 250\n",
    "  # test.pop('Label')\n",
    "  # test.pop('is_train')\n",
    "  # print(label2)\n",
    "\n",
    "\n",
    "  # models = [ada,dnn_01,clf,knn_clf_01,cat_01,xgb_01, rf, lgbm, mlp,logreg_01]\n",
    "  explainer = shap.KernelExplainer(ada.predict_proba, test[start_index:end_index])\n",
    "\n",
    "  shap_values = explainer.shap_values(test[start_index:end_index])\n",
    "\n",
    "  shap.summary_plot(shap_values = shap_values,\n",
    "                    features = test[start_index:end_index],\n",
    "                    # class_names=[column_features[:-1]],\n",
    "                    show=False)\n",
    "\n",
    "  # if feature_selection_bit == 1 # On\n",
    "  # pick_prob = 0 # set equal one to choose the dataset with probabilities, set to 0 to choose one with the classes.\n",
    "  if pick_prob == 1:\n",
    "    plt.savefig('ADA_SHAP_NSL_prob_01.png')\n",
    "  elif pick_prob == 0:\n",
    "    plt.savefig('ADA_SHAP_NSL_class_01.png')\n",
    "        \n",
    "  else: None\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "  vals= np.abs(shap_values).mean(1)\n",
    "  feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "  feature_importance.head()\n",
    "  print(feature_importance.to_string())\n",
    "\n",
    "  with open(output_file_name, \"a\") as f:print('Feature Importance: ',feature_importance.to_string(),file = f)\n",
    "\n",
    "\n",
    "\n",
    "  end = time.time()\n",
    "  with open(output_file_name, \"a\") as f:print('ELAPSE TIME LIME GLOBAL: ',(end - start)/60, 'min',file = f)\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  # feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "  feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "  # col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "  feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "  # for item1, item2 in zip(feature_name, feature_val):\n",
    "  #     print(item1, item2)\n",
    "\n",
    "\n",
    "  # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "  zipped_lists = list(zip(feature_name, feature_val))\n",
    "  zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "  # Convert the sorted result back into separate lists\n",
    "  sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f: print(\"df.pop('\",k,\"')\", sep='', file = f)\n",
    "\n",
    "  # with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  # with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# start_index = 0\n",
    "# end_index = samples\n",
    "# shap_values = explainer.shap_values(test[start_index:end_index])\n",
    "# shap_obj = explainer(test[start_index:end_index])\n",
    "# shap.summary_plot(shap_values = shap_values,\n",
    "#                   features = test[start_index:end_index],\n",
    "#                 show=False)\n",
    "# plt.savefig('Light_SHAP_CIC_Summary.png')\n",
    "# plt.clf()\n",
    "\n",
    "\n",
    "# vals= np.abs(shap_values).mean(1)\n",
    "# feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "# feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "# feature_importance.head()\n",
    "# print(feature_importance.to_string())\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# # feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "# feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "# # col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "# feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "# # for item1, item2 in zip(feature_name, feature_val):\n",
    "# #     print(item1, item2)\n",
    "\n",
    "\n",
    "# # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "# zipped_lists = list(zip(feature_name, feature_val))\n",
    "# zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "# # Convert the sorted result back into separate lists\n",
    "# sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "# for k in sorted_list1:\n",
    "#   with open(output_file_name, \"a\") as f:print(\"df.pop('\",k,\"')\", sep='', file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "# for k in sorted_list1:\n",
    "#   with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "# with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "# print('---------------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_feature_importance == 1:\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating SHAP explanation')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "\n",
    "  with open(output_file_name, \"a\") as f:print('XGB FEATURE IMPORTANCE',file = f)\n",
    "\n",
    "      #START TIMER MODEL\n",
    "  start = time.time()\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating explainer')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "  test = X_test_01\n",
    "  train = X_train_01\n",
    "  # ## Summary Bar Plot Global\n",
    "  start_index = 0\n",
    "  end_index = 250\n",
    "  # test.pop('Label')\n",
    "  # test.pop('is_train')\n",
    "  # print(label2)\n",
    "\n",
    "\n",
    "  explainer = shap.TreeExplainer(xgb_01)\n",
    "\n",
    "  shap_values = explainer.shap_values(test[start_index:end_index])\n",
    "  shap_obj = explainer(test[start_index:end_index])\n",
    "  shap.summary_plot(shap_values = shap_values,\n",
    "                    features = test[start_index:end_index],\n",
    "                  show=False)\n",
    "  # plt.clf()\n",
    "\n",
    "  # if feature_selection_bit == 1 # On\n",
    "  # pick_prob = 0 # set equal one to choose the dataset with probabilities, set to 0 to choose one with the classes.\n",
    "  if pick_prob == 1:\n",
    "    plt.savefig('XGB_SHAP_NSL_prob_01.png')\n",
    "  elif pick_prob == 0:\n",
    "    plt.savefig('XGB_SHAP_NSL_class_01.png')\n",
    "\n",
    "  else: None\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "  vals= np.abs(shap_values).mean(1)\n",
    "  feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "  feature_importance.head()\n",
    "  print(feature_importance.to_string())\n",
    "  with open(output_file_name, \"a\") as f:print('Feature Importance: ',feature_importance.to_string(),file = f)\n",
    "\n",
    "\n",
    "\n",
    "  end = time.time()\n",
    "  with open(output_file_name, \"a\") as f:print('ELAPSE TIME LIME GLOBAL: ',(end - start)/60, 'min',file = f)\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  # feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "  feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "  # col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "  feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "  # for item1, item2 in zip(feature_name, feature_val):\n",
    "  #     print(item1, item2)\n",
    "\n",
    "\n",
    "  # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "  zipped_lists = list(zip(feature_name, feature_val))\n",
    "  zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "  # Convert the sorted result back into separate lists\n",
    "  sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f: print(\"df.pop('\",k,\"')\", sep='', file = f)\n",
    "\n",
    "  # with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  # with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_feature_importance == 1:\n",
    "\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating SHAP explanation')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "\n",
    "  with open(output_file_name, \"a\") as f:print('LGBM FEATURE IMPORTANCE',file = f)\n",
    "\n",
    "      #START TIMER MODEL\n",
    "  start = time.time()\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating explainer')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "  test = X_test_01\n",
    "  train = X_train_01\n",
    "  # ## Summary Bar Plot Global\n",
    "  start_index = 0\n",
    "  end_index = 250\n",
    "  # test.pop('Label')\n",
    "  # test.pop('is_train')\n",
    "  # print(label2)\n",
    "\n",
    "\n",
    "  explainer = shap.TreeExplainer(lgbm)\n",
    "\n",
    "  shap_values = explainer.shap_values(test[start_index:end_index])\n",
    "  shap_obj = explainer(test[start_index:end_index])\n",
    "  shap.summary_plot(shap_values = shap_values,\n",
    "                    features = test[start_index:end_index],\n",
    "                  show=False)\n",
    "  # plt.clf()\n",
    "\n",
    "  # if feature_selection_bit == 1 # On\n",
    "  # pick_prob = 0 # set equal one to choose the dataset with probabilities, set to 0 to choose one with the classes.\n",
    "  if pick_prob == 1:\n",
    "    plt.savefig('LGBM_SHAP_NSL_prob_01.png')\n",
    "  elif pick_prob == 0:\n",
    "    plt.savefig('LGBM_SHAP_NSL_class_01.png')\n",
    "\n",
    "  else: None\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "  vals= np.abs(shap_values).mean(1)\n",
    "  feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "  feature_importance.head()\n",
    "  print(feature_importance.to_string())\n",
    "  with open(output_file_name, \"a\") as f:print('Feature Importance: ',feature_importance.to_string(),file = f)\n",
    "\n",
    "\n",
    "\n",
    "  end = time.time()\n",
    "  with open(output_file_name, \"a\") as f:print('ELAPSE TIME LIME GLOBAL: ',(end - start)/60, 'min',file = f)\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  # feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "  feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "  # col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "  feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "  # for item1, item2 in zip(feature_name, feature_val):\n",
    "  #     print(item1, item2)\n",
    "\n",
    "\n",
    "  # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "  zipped_lists = list(zip(feature_name, feature_val))\n",
    "  zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "  # Convert the sorted result back into separate lists\n",
    "  sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f: print(\"df.pop('\",k,\"')\", sep='', file = f)\n",
    "\n",
    "  # with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  # with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_feature_importance == 1:\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating SHAP explanation')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "\n",
    "  with open(output_file_name, \"a\") as f:print('RF FEATURE IMPORTANCE',file = f)\n",
    "\n",
    "      #START TIMER MODEL\n",
    "  start = time.time()\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('Generating explainer')\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  print('')\n",
    "  test = X_test_01\n",
    "  train = X_train_01\n",
    "  # ## Summary Bar Plot Global\n",
    "  start_index = 0\n",
    "  end_index = 250\n",
    "  # test.pop('Label')\n",
    "  # test.pop('is_train')\n",
    "  # print(label2)\n",
    "\n",
    "\n",
    "  explainer = shap.TreeExplainer(rf)\n",
    "\n",
    "  shap_values = explainer.shap_values(test[start_index:end_index])\n",
    "  shap_obj = explainer(test[start_index:end_index])\n",
    "  shap.summary_plot(shap_values = shap_values,\n",
    "                    features = test[start_index:end_index],\n",
    "                  show=False)\n",
    "  # plt.clf()\n",
    "\n",
    "  # if feature_selection_bit == 1 # On\n",
    "  # pick_prob = 0 # set equal one to choose the dataset with probabilities, set to 0 to choose one with the classes.\n",
    "  if pick_prob == 1:\n",
    "    plt.savefig('RF_SHAP_NSL_prob_01.png')\n",
    "  elif pick_prob == 0:\n",
    "    plt.savefig('RF_SHAP_NSL_class_01.png')\n",
    "\n",
    "  else: None\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "  vals= np.abs(shap_values).mean(1)\n",
    "  feature_importance = pd.DataFrame(list(zip(train.columns, sum(vals))), columns=['col_name','feature_importance_vals'])\n",
    "  feature_importance.sort_values(by=['feature_importance_vals'], ascending=False,inplace=True)\n",
    "  feature_importance.head()\n",
    "  print(feature_importance.to_string())\n",
    "  with open(output_file_name, \"a\") as f:print('Feature Importance: ',feature_importance.to_string(),file = f)\n",
    "\n",
    "\n",
    "\n",
    "  end = time.time()\n",
    "  with open(output_file_name, \"a\") as f:print('ELAPSE TIME LIME GLOBAL: ',(end - start)/60, 'min',file = f)\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "  # feature_importance_vals = 'feature_importance_vals'  # Replace with the name of the column you want to extract\n",
    "  feature_val = feature_importance['feature_importance_vals'].tolist()\n",
    "\n",
    "  # col_name = 'col_name'  # Replace with the name of the column you want to extract\n",
    "  feature_name = feature_importance['col_name'].tolist()\n",
    "\n",
    "\n",
    "  # for item1, item2 in zip(feature_name, feature_val):\n",
    "  #     print(item1, item2)\n",
    "\n",
    "\n",
    "  # Use zip to combine the two lists, sort based on list1, and then unzip them\n",
    "  zipped_lists = list(zip(feature_name, feature_val))\n",
    "  zipped_lists.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "  # Convert the sorted result back into separate lists\n",
    "  sorted_list1, sorted_list2 = [list(x) for x in zip(*zipped_lists)]\n",
    "\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f: print(\"df.pop('\",k,\"')\", sep='', file = f)\n",
    "\n",
    "  # with open(output_file_name, \"a\") as f:print(\"Trial_ =[\", file = f)\n",
    "  # for k in sorted_list1:\n",
    "  #   with open(output_file_name, \"a\") as f:print(\"'\",k,\"',\", sep='', file = f)\n",
    "  # with open(output_file_name, \"a\") as f:print(\"]\", file = f)\n",
    "\n",
    "  print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
