{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First ensemble with NSL-KDD\n",
    "# Parameters\n",
    "\n",
    "#----------------------------------------------\n",
    "# 0 for not using it as base learner\n",
    "# 1 for using it as base learner\n",
    "\n",
    "use_model_ada = 1 \n",
    "use_model_dnn = 1 \n",
    "use_model_mlp = 1 \n",
    "use_model_lgbm = 1 \n",
    "use_model_rf = 1 \n",
    "use_model_svm = 1\n",
    "use_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "# 0 for training the model\n",
    "# 1 for using the saved version of the model\n",
    "\n",
    "load_model_ada = 0 \n",
    "load_model_dnn = 0 \n",
    "load_model_mlp = 0 \n",
    "load_model_lgbm = 0 \n",
    "load_model_rf = 0 \n",
    "load_model_svm = 0\n",
    "load_model_knn = 0 \n",
    "#----------------------------------------------\n",
    "\n",
    "# load_model_ada = 1\n",
    "# load_model_dnn = 1 \n",
    "# load_model_mlp = 1 \n",
    "# load_model_lgbm = 1 \n",
    "# load_model_rf = 1 \n",
    "# load_model_svm = 1\n",
    "# load_model_knn = 1 \n",
    "#----------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the name of the output text file\n",
    "output_file_name = \"ensemble_prob_AF.txt\"\n",
    "with open(output_file_name, \"w\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('---- Start Ensemble Model Info - v0 ----', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle # saving and loading trained model\n",
    "from os import path\n",
    "\n",
    "\n",
    "# importing required libraries for normalizing data\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import (StandardScaler, OrdinalEncoder,LabelEncoder, MinMaxScaler, OneHotEncoder)\n",
    "from sklearn.preprocessing import Normalizer, MaxAbsScaler , RobustScaler, PowerTransformer\n",
    "\n",
    "# importing library for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
    "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
    "from sklearn.metrics import classification_report # for generating a classification report of model\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense # importing dense layer\n",
    "\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "# representation of model layers\n",
    "#from keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import shap\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "# #Defining metric functions\n",
    "# def ACC(TP,TN,FP,FN):\n",
    "#     Acc = (TP+TN)/(TP+FP+FN+TN)\n",
    "#     return Acc\n",
    "# def ACC_2 (TP, FN):\n",
    "#     ac = (TP/(TP+FN))\n",
    "#     return ac\n",
    "# def PRECISION(TP,FP):\n",
    "#     eps = 1e-7\n",
    "#     Precision = TP/(TP+FP+eps)\n",
    "    \n",
    "\n",
    "#     return Precision\n",
    "# def RECALL(TP,FN):\n",
    "#     Recall = TP/(TP+FN)\n",
    "#     return Recall\n",
    "# def F1(Recall, Precision):\n",
    "#     F1 = 2 * Recall * Precision / (Recall + Precision)\n",
    "#     return F1\n",
    "# def BACC(TP,TN,FP,FN):\n",
    "#     BACC =(TP/(TP+FN)+ TN/(TN+FP))*0.5\n",
    "#     return BACC\n",
    "# def MCC(TP,TN,FP,FN):\n",
    "#     eps = 1e-7\n",
    "#     MCC = (TN*TP-FN*FP)/(((TP+FP+eps)*(TP+FN+eps)*(TN+FP+eps)*(TN+FN+eps))**.5)\n",
    "#     return MCC\n",
    "# def AUC_ROC(y_test_bin,y_score):\n",
    "#     fpr = dict()\n",
    "#     tpr = dict()\n",
    "#     roc_auc = dict()\n",
    "#     auc_avg = 0\n",
    "#     counting = 0\n",
    "#     for i in range(n_classes):\n",
    "#       fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "#      # plt.plot(fpr[i], tpr[i], color='darkorange', lw=2)\n",
    "#       #print('AUC for Class {}: {}'.format(i+1, auc(fpr[i], tpr[i])))\n",
    "#       auc_avg += auc(fpr[i], tpr[i])\n",
    "#       counting = i+1\n",
    "#     return auc_avg/counting\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "# attach the column names to the dataset\n",
    "feature=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\n",
    "          \"num_failed_logins\",\"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\"num_shells\",\n",
    "          \"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n",
    "          \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\", \n",
    "          \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\n",
    "          \"dst_host_srv_serror_rate\",\"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty\"]\n",
    "# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n",
    "# these have already been removed.\n",
    "\n",
    "train='KDDTrain+.txt'\n",
    "test='KDDTest+.txt'\n",
    "\n",
    "df=pd.read_csv(train,names=feature)\n",
    "df_test=pd.read_csv(test,names=feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the Training set: (125973, 43)\n",
      "Dimensions of the Test set: (22544, 43)\n",
      "Label distribution Training set:\n",
      "normal             67343\n",
      "neptune            41214\n",
      "satan               3633\n",
      "ipsweep             3599\n",
      "portsweep           2931\n",
      "smurf               2646\n",
      "nmap                1493\n",
      "back                 956\n",
      "teardrop             892\n",
      "warezclient          890\n",
      "pod                  201\n",
      "guess_passwd          53\n",
      "buffer_overflow       30\n",
      "warezmaster           20\n",
      "land                  18\n",
      "imap                  11\n",
      "rootkit               10\n",
      "loadmodule             9\n",
      "ftp_write              8\n",
      "multihop               7\n",
      "phf                    4\n",
      "perl                   3\n",
      "spy                    2\n",
      "Name: label, dtype: int64\n",
      "\n",
      "Label distribution Test set:\n",
      "normal             9711\n",
      "neptune            4657\n",
      "guess_passwd       1231\n",
      "mscan               996\n",
      "warezmaster         944\n",
      "apache2             737\n",
      "satan               735\n",
      "processtable        685\n",
      "smurf               665\n",
      "back                359\n",
      "snmpguess           331\n",
      "saint               319\n",
      "mailbomb            293\n",
      "snmpgetattack       178\n",
      "portsweep           157\n",
      "ipsweep             141\n",
      "httptunnel          133\n",
      "nmap                 73\n",
      "pod                  41\n",
      "buffer_overflow      20\n",
      "multihop             18\n",
      "named                17\n",
      "ps                   15\n",
      "sendmail             14\n",
      "xterm                13\n",
      "rootkit              13\n",
      "teardrop             12\n",
      "xlock                 9\n",
      "land                  7\n",
      "xsnoop                4\n",
      "ftp_write             3\n",
      "sqlattack             2\n",
      "udpstorm              2\n",
      "phf                   2\n",
      "loadmodule            2\n",
      "perl                  2\n",
      "worm                  2\n",
      "imap                  1\n",
      "Name: label, dtype: int64\n",
      "Training set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 70 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 23 categories\n",
      "\n",
      "Distribution of categories in service:\n",
      "http        40338\n",
      "private     21853\n",
      "domain_u     9043\n",
      "smtp         7313\n",
      "ftp_data     6860\n",
      "Name: service, dtype: int64\n",
      "Test set:\n",
      "Feature 'protocol_type' has 3 categories\n",
      "Feature 'service' has 64 categories\n",
      "Feature 'flag' has 11 categories\n",
      "Feature 'label' has 38 categories\n",
      "['Protocol_type_icmp', 'Protocol_type_tcp', 'Protocol_type_udp', 'service_IRC', 'service_X11', 'service_Z39_50', 'service_aol', 'service_auth', 'service_bgp', 'service_courier', 'service_csnet_ns', 'service_ctf', 'service_daytime', 'service_discard', 'service_domain', 'service_domain_u', 'service_echo', 'service_eco_i', 'service_ecr_i', 'service_efs', 'service_exec', 'service_finger', 'service_ftp', 'service_ftp_data', 'service_gopher', 'service_harvest', 'service_hostnames', 'service_http', 'service_http_2784', 'service_http_443', 'service_http_8001', 'service_imap4', 'service_iso_tsap', 'service_klogin', 'service_kshell', 'service_ldap', 'service_link', 'service_login', 'service_mtp', 'service_name', 'service_netbios_dgm', 'service_netbios_ns', 'service_netbios_ssn', 'service_netstat', 'service_nnsp', 'service_nntp', 'service_ntp_u', 'service_other', 'service_pm_dump', 'service_pop_2', 'service_pop_3', 'service_printer', 'service_private', 'service_red_i', 'service_remote_job', 'service_rje', 'service_shell', 'service_smtp', 'service_sql_net', 'service_ssh', 'service_sunrpc', 'service_supdup', 'service_systat', 'service_telnet', 'service_tftp_u', 'service_tim_i', 'service_time', 'service_urh_i', 'service_urp_i', 'service_uucp', 'service_uucp_path', 'service_vmnet', 'service_whois', 'flag_OTH', 'flag_REJ', 'flag_RSTO', 'flag_RSTOS0', 'flag_RSTR', 'flag_S0', 'flag_S1', 'flag_S2', 'flag_S3', 'flag_SF', 'flag_SH']\n",
      "   protocol_type  service  flag\n",
      "0              1       20     9\n",
      "1              2       44     9\n",
      "2              1       49     5\n",
      "3              1       24     9\n",
      "4              1       24     9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125973, 123)\n",
      "(22544, 123)\n",
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: label, dtype: int64\n",
      "X_train has shape: (125973, 122) \n",
      "y_train has shape: (125973, 1)\n",
      "X_test has shape: (22544, 122) \n",
      "y_test has shape: (22544, 1)\n",
      "Counter({0: 67343, 1: 45927, 2: 11656, 3: 995, 4: 52})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# shape, this gives the dimensions of the dataset\n",
    "print('Dimensions of the Training set:',df.shape)\n",
    "print('Dimensions of the Test set:',df_test.shape)\n",
    "\n",
    "\n",
    "df.drop(['difficulty'],axis=1,inplace=True)\n",
    "df_test.drop(['difficulty'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "print('Label distribution Training set:')\n",
    "print(df['label'].value_counts())\n",
    "print()\n",
    "print('Label distribution Test set:')\n",
    "print(df_test['label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n",
    "# explore categorical features\n",
    "print('Training set:')\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\n",
    "print()\n",
    "print('Distribution of categories in service:')\n",
    "print(df['service'].value_counts().sort_values(ascending=False).head())\n",
    "\n",
    "\n",
    "\n",
    "# Test set\n",
    "print('Test set:')\n",
    "for col_name in df_test.columns:\n",
    "    if df_test[col_name].dtypes == 'object' :\n",
    "        unique_cat = len(df_test[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "categorical_columns=['protocol_type', 'service', 'flag']\n",
    "# insert code to get a list of categorical columns into a variable, categorical_columns\n",
    "categorical_columns=['protocol_type', 'service', 'flag'] \n",
    " # Get the categorical values into a 2D numpy array\n",
    "df_categorical_values = df[categorical_columns]\n",
    "testdf_categorical_values = df_test[categorical_columns]\n",
    "df_categorical_values.head()\n",
    "\n",
    "\n",
    "# protocol type\n",
    "unique_protocol=sorted(df.protocol_type.unique())\n",
    "string1 = 'Protocol_type_'\n",
    "unique_protocol2=[string1 + x for x in unique_protocol]\n",
    "# service\n",
    "unique_service=sorted(df.service.unique())\n",
    "string2 = 'service_'\n",
    "unique_service2=[string2 + x for x in unique_service]\n",
    "# flag\n",
    "unique_flag=sorted(df.flag.unique())\n",
    "string3 = 'flag_'\n",
    "unique_flag2=[string3 + x for x in unique_flag]\n",
    "# put together\n",
    "dumcols=unique_protocol2 + unique_service2 + unique_flag2\n",
    "print(dumcols)\n",
    "\n",
    "#do same for test set\n",
    "unique_service_test=sorted(df_test.service.unique())\n",
    "unique_service2_test=[string2 + x for x in unique_service_test]\n",
    "testdumcols=unique_protocol2 + unique_service2_test + unique_flag2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "print(df_categorical_values_enc.head())\n",
    "# test set\n",
    "testdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "df_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\n",
    "df_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n",
    "# test set\n",
    "testdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\n",
    "testdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n",
    "\n",
    "df_cat_data.head()\n",
    "\n",
    "\n",
    "trainservice=df['service'].tolist()\n",
    "testservice= df_test['service'].tolist()\n",
    "difference=list(set(trainservice) - set(testservice))\n",
    "string = 'service_'\n",
    "difference=[string + x for x in difference]\n",
    "difference\n",
    "\n",
    "for col in difference:\n",
    "    testdf_cat_data[col] = 0\n",
    "\n",
    "testdf_cat_data.shape\n",
    "\n",
    "newdf=df.join(df_cat_data)\n",
    "newdf.drop('flag', axis=1, inplace=True)\n",
    "newdf.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf.drop('service', axis=1, inplace=True)\n",
    "# test data\n",
    "newdf_test=df_test.join(testdf_cat_data)\n",
    "newdf_test.drop('flag', axis=1, inplace=True)\n",
    "newdf_test.drop('protocol_type', axis=1, inplace=True)\n",
    "newdf_test.drop('service', axis=1, inplace=True)\n",
    "print(newdf.shape)\n",
    "print(newdf_test.shape)\n",
    "\n",
    "\n",
    "# take label column\n",
    "labeldf=newdf['label']\n",
    "labeldf_test=newdf_test['label']\n",
    "# change the label column\n",
    "newlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "newlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n",
    "                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n",
    "                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n",
    "                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n",
    "# put the new label column back\n",
    "newdf['label'] = newlabeldf\n",
    "newdf_test['label'] = newlabeldf_test\n",
    "print(newdf['label'].head())\n",
    "\n",
    "\n",
    "# Specify your selected features. Note that you'll need to modify this list according to your final processed dataframe\n",
    "#Uncomment the below lines to use these top 20 features from shap analysis\n",
    "#selected_features = [\"root_shell\",\"service_telnet\",\"num_shells\",\"service_uucp\",\"dst_host_same_src_port_rate\"\n",
    "#                     ,\"dst_host_rerror_rate\",\"dst_host_srv_serror_rate\",\"dst_host_srv_count\",\"service_private\",\"logged_in\",\n",
    "#                    \"dst_host_serror_rate\",\"serror_rate\",\"srv_serror_rate\",\"flag_S0\",\"diff_srv_rate\",\"dst_host_srv_diff_host_rate\",\"num_file_creations\",\"flag_RSTR\"#,\"dst_host_same_srv_rate\",\"service_Idap\",\"label\"]\n",
    "                     \n",
    "\n",
    "# Select those features from your dataframe\n",
    "#newdf = newdf[selected_features]\n",
    "#newdf_test = newdf_test[selected_features]\n",
    "\n",
    "# Now your dataframe only contains your selected features.\n",
    "\n",
    "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
    "multi_data = newdf.copy()\n",
    "multi_label = pd.DataFrame(multi_data.label)\n",
    "\n",
    "multi_data_test=newdf_test.copy()\n",
    "multi_label_test = pd.DataFrame(multi_data_test.label)\n",
    "\n",
    "\n",
    "# using standard scaler for normalizing\n",
    "std_scaler = StandardScaler()\n",
    "def standardization(df,col):\n",
    "    for i in col:\n",
    "        arr = df[i]\n",
    "        arr = np.array(arr)\n",
    "        df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
    "    return df\n",
    "\n",
    "numeric_col = multi_data.select_dtypes(include='number').columns\n",
    "data = standardization(multi_data,numeric_col)\n",
    "numeric_col_test = multi_data_test.select_dtypes(include='number').columns\n",
    "data_test = standardization(multi_data_test,numeric_col_test)\n",
    "\n",
    "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
    "le2 = preprocessing.LabelEncoder()\n",
    "le2_test = preprocessing.LabelEncoder()\n",
    "enc_label = multi_label.apply(le2.fit_transform)\n",
    "enc_label_test = multi_label_test.apply(le2_test.fit_transform)\n",
    "multi_data = multi_data.copy()\n",
    "multi_data_test = multi_data_test.copy()\n",
    "\n",
    "multi_data['intrusion'] = enc_label\n",
    "multi_data_test['intrusion'] = enc_label_test\n",
    "\n",
    "#y_mul = multi_data['intrusion']\n",
    "multi_data\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "\n",
    "multi_data.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data\n",
    "multi_data_test.drop(labels= [ 'label'], axis=1, inplace=True)\n",
    "multi_data_test\n",
    "\n",
    "\n",
    "y_train_multi= multi_data[['intrusion']]\n",
    "X_train_multi= multi_data.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_train has shape:',X_train_multi.shape,'\\ny_train has shape:',y_train_multi.shape)\n",
    "\n",
    "y_test_multi= multi_data_test[['intrusion']]\n",
    "X_test_multi= multi_data_test.drop(labels=['intrusion'], axis=1)\n",
    "\n",
    "print('X_test has shape:',X_test_multi.shape,'\\ny_test has shape:',y_test_multi.shape)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(y_train_multi['intrusion'])\n",
    "print(label_counts)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "y_train_multi = LabelBinarizer().fit_transform(y_train_multi)\n",
    "\n",
    "y_test_multi = LabelBinarizer().fit_transform(y_test_multi)\n",
    "\n",
    "\n",
    "Y_train=y_train_multi.copy()\n",
    "X_train=X_train_multi.copy()\n",
    "\n",
    "Y_test=y_test_multi.copy()\n",
    "X_test=X_test_multi.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.feature_selection import SelectKBest, f_classif\\n\\n# Number of best features you want to select\\nk = 15\\n\\n# Initialize a dataframe to store the scores for each feature against each class\\nfeature_scores = pd.DataFrame(index=X_train.columns)\\n\\n# Loop through each class\\nfor class_index in range(Y_train.shape[1]):\\n    \\n    # Get the current class labels\\n    y_train_current_class = Y_train[:, class_index]\\n    \\n    # Select K best features for the current class\\n    best_features = SelectKBest(score_func=f_classif, k=\\'all\\')\\n    fit = best_features.fit(X_train, y_train_current_class)\\n\\n    # Get the scores\\n    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\\n    \\n    # Concatenate the scores to the main dataframe\\n    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\\n\\n# Get the sum of the scores for each feature\\nfeature_scores[\\'total\\'] = feature_scores.sum(axis=1)\\n\\n# Get the top k features in a list\\ntop_k_features = feature_scores.nlargest(k, \\'total\\').index.tolist()\\n\\nprint(top_k_features)\\n\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# In[24]:\n",
    "\n",
    "'''\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Number of best features you want to select\n",
    "k = 15\n",
    "\n",
    "# Initialize a dataframe to store the scores for each feature against each class\n",
    "feature_scores = pd.DataFrame(index=X_train.columns)\n",
    "\n",
    "# Loop through each class\n",
    "for class_index in range(Y_train.shape[1]):\n",
    "    \n",
    "    # Get the current class labels\n",
    "    y_train_current_class = Y_train[:, class_index]\n",
    "    \n",
    "    # Select K best features for the current class\n",
    "    best_features = SelectKBest(score_func=f_classif, k='all')\n",
    "    fit = best_features.fit(X_train, y_train_current_class)\n",
    "\n",
    "    # Get the scores\n",
    "    df_scores = pd.DataFrame(fit.scores_, index=X_train.columns, columns=[f\"class_{class_index}\"])\n",
    "    \n",
    "    # Concatenate the scores to the main dataframe\n",
    "    feature_scores = pd.concat([feature_scores, df_scores],axis=1)\n",
    "\n",
    "# Get the sum of the scores for each feature\n",
    "feature_scores['total'] = feature_scores.sum(axis=1)\n",
    "\n",
    "# Get the top k features in a list\n",
    "top_k_features = feature_scores.nlargest(k, 'total').index.tolist()\n",
    "\n",
    "print(top_k_features)\n",
    "\n",
    "'''\n",
    "# In[32]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.10249223e-01, -7.67859947e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.73736981e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-1.10249223e-01, -7.76224074e-03, -4.91864438e-03, ...,\n",
       "        -1.97262160e-02, -1.21190076e+00, -4.64315895e-02],\n",
       "       ...,\n",
       "       [-9.29714678e-02, -7.36430591e-03, -3.87394518e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [-8.68282658e-02, -7.36430591e-03, -3.87568593e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02],\n",
       "       [ 1.61587463e-01, -7.46804833e-03,  1.06953862e-03, ...,\n",
       "        -1.97262160e-02,  8.25150071e-01, -4.64315895e-02]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Assuming you have features X and labels Y\n",
    "# X, Y = make_classification()\n",
    "\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=100)\n",
    "\n",
    "X_train, Y_train = ros.fit_resample(X_train, Y_train)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "print(Y_test)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "X_train.values\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_class_train = np.argmax(y_train_multi, axis=1)\n",
    "single_class_test = np.argmax(y_test_multi, axis=1)\n",
    "\n",
    "\n",
    "df1 = X_train_multi.assign(Label = single_class_train)\n",
    "df2 =  X_test_multi.assign(Label = single_class_test)\n",
    "\n",
    "frames = [df1,  df2]\n",
    "\n",
    "df = pd.concat(frames,ignore_index=True)\n",
    "\n",
    "y = df.pop('Label')\n",
    "X = df\n",
    "\n",
    "y1, y2 = pd.factorize(y)\n",
    "\n",
    "y_0 = pd.DataFrame(y1)\n",
    "y_1 = pd.DataFrame(y1)\n",
    "y_2 = pd.DataFrame(y1)\n",
    "y_3 = pd.DataFrame(y1)\n",
    "y_4 = pd.DataFrame(y1)\n",
    "\n",
    "\n",
    "# y_0 = y_0.replace(0, 0)\n",
    "# y_0 = y_0.replace(1, 1)\n",
    "y_0 = y_0.replace(2, 1)\n",
    "y_0 = y_0.replace(3, 1)\n",
    "y_0 = y_0.replace(4, 1)\n",
    "\n",
    "\n",
    "y_1 = y_1.replace(1, 999)\n",
    "y_1 = y_1.replace(0, 1)\n",
    "# y_1 = y_1.replace(1, 0)\n",
    "y_1 = y_1.replace(2, 1)\n",
    "y_1 = y_1.replace(3, 1)\n",
    "y_1 = y_1.replace(4, 1)\n",
    "y_1 = y_1.replace(999, 1)\n",
    "\n",
    "\n",
    "y_2 = y_2.replace(0, 1)\n",
    "y_2 = y_2.replace(1, 1)\n",
    "y_2 = y_2.replace(2, 0)\n",
    "y_2 = y_2.replace(3, 1)\n",
    "y_2 = y_2.replace(4, 1)\n",
    "\n",
    "\n",
    "y_3 = y_3.replace(0, 1)\n",
    "# y_3 = y_3.replace(1, 1)\n",
    "y_3 = y_3.replace(2, 1)\n",
    "y_3 = y_3.replace(3, 0)\n",
    "y_3 = y_3.replace(4, 1)\n",
    "\n",
    "\n",
    "y_4 = y_4.replace(0, 1)\n",
    "# y_4 = y_4.replace(1, 1)\n",
    "y_4 = y_4.replace(2, 1)\n",
    "y_4 = y_4.replace(3, 1)\n",
    "y_4 = y_4.replace(4, 0)\n",
    "\n",
    "\n",
    "\n",
    "df = df.assign(Label = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the dataset between level 00 and level 01\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "split = 0.5 # 0.7\n",
    "\n",
    "# X_00,X_01, y_00, y_01 = sklearn.model_selection.train_test_split(X, y, train_size=split)\n",
    "X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 77054, 1: 53387, 2: 14077, 3: 3880, 4: 119})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts2 = Counter(y)\n",
    "print(label_counts2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base learner Split\n",
    "# split = 0.7\n",
    "\n",
    "# X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X_00, y_00, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37230</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>-0.004889</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17528</th>\n",
       "      <td>-0.109865</td>\n",
       "      <td>-0.007560</td>\n",
       "      <td>-0.004837</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143061</th>\n",
       "      <td>-0.155534</td>\n",
       "      <td>-0.021973</td>\n",
       "      <td>-0.083842</td>\n",
       "      <td>-0.017624</td>\n",
       "      <td>-0.059104</td>\n",
       "      <td>-0.019459</td>\n",
       "      <td>-0.113521</td>\n",
       "      <td>-0.143999</td>\n",
       "      <td>-0.890373</td>\n",
       "      <td>-0.016494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.453815</td>\n",
       "      <td>-0.18843</td>\n",
       "      <td>-0.009419</td>\n",
       "      <td>-0.174880</td>\n",
       "      <td>-0.313124</td>\n",
       "      <td>-0.030535</td>\n",
       "      <td>-0.025803</td>\n",
       "      <td>-0.105681</td>\n",
       "      <td>0.718027</td>\n",
       "      <td>-0.056997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87792</th>\n",
       "      <td>-0.104874</td>\n",
       "      <td>-0.007744</td>\n",
       "      <td>-0.004883</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115107</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>1.616978</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-1.211901</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11869</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007714</td>\n",
       "      <td>-0.004738</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235694</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75598</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>1.616978</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-1.211901</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97245</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007761</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>-0.618438</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825150</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125911</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>1.616978</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-1.211901</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111713</th>\n",
       "      <td>-0.110249</td>\n",
       "      <td>-0.007762</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089486</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809262</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.312889</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139982</td>\n",
       "      <td>1.616978</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031767</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>-1.211901</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74258 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
       "37230  -0.110249  -0.007755  -0.004889 -0.014089       -0.089486 -0.007736   \n",
       "17528  -0.109865  -0.007560  -0.004837 -0.014089       -0.089486 -0.007736   \n",
       "143061 -0.155534  -0.021973  -0.083842 -0.017624       -0.059104 -0.019459   \n",
       "87792  -0.104874  -0.007744  -0.004883 -0.014089       -0.089486 -0.007736   \n",
       "115107 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "...          ...        ...        ...       ...             ...       ...   \n",
       "11869  -0.110249  -0.007714  -0.004738 -0.014089       -0.089486 -0.007736   \n",
       "75598  -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "97245  -0.110249  -0.007761  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "125911 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "111713 -0.110249  -0.007762  -0.004919 -0.014089       -0.089486 -0.007736   \n",
       "\n",
       "             hot  num_failed_logins  logged_in  num_compromised  ...  \\\n",
       "37230  -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "17528  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "143061 -0.113521          -0.143999  -0.890373        -0.016494  ...   \n",
       "87792  -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "115107 -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "...          ...                ...        ...              ...  ...   \n",
       "11869  -0.095076          -0.027023   1.235694        -0.011664  ...   \n",
       "75598  -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "97245  -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "125911 -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "111713 -0.095076          -0.027023  -0.809262        -0.011664  ...   \n",
       "\n",
       "        flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1  \\\n",
       "37230  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "17528  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "143061 -0.453815   -0.18843    -0.009419  -0.174880 -0.313124 -0.030535   \n",
       "87792  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "115107 -0.312889   -0.11205    -0.028606  -0.139982  1.616978 -0.053906   \n",
       "...          ...        ...          ...        ...       ...       ...   \n",
       "11869  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "75598  -0.312889   -0.11205    -0.028606  -0.139982  1.616978 -0.053906   \n",
       "97245  -0.312889   -0.11205    -0.028606  -0.139982 -0.618438 -0.053906   \n",
       "125911 -0.312889   -0.11205    -0.028606  -0.139982  1.616978 -0.053906   \n",
       "111713 -0.312889   -0.11205    -0.028606  -0.139982  1.616978 -0.053906   \n",
       "\n",
       "         flag_S2   flag_S3   flag_SF   flag_SH  \n",
       "37230  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "17528  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "143061 -0.025803 -0.105681  0.718027 -0.056997  \n",
       "87792  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "115107 -0.031767 -0.019726 -1.211901 -0.046432  \n",
       "...          ...       ...       ...       ...  \n",
       "11869  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "75598  -0.031767 -0.019726 -1.211901 -0.046432  \n",
       "97245  -0.031767 -0.019726  0.825150 -0.046432  \n",
       "125911 -0.031767 -0.019726 -1.211901 -0.046432  \n",
       "111713 -0.031767 -0.019726 -1.211901 -0.046432  \n",
       "\n",
       "[74258 rows x 122 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37230     0\n",
       "17528     0\n",
       "143061    0\n",
       "87792     0\n",
       "115107    1\n",
       "         ..\n",
       "11869     0\n",
       "75598     1\n",
       "97245     2\n",
       "125911    1\n",
       "111713    1\n",
       "Name: Label, Length: 74258, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEVEL 0 - Weak models - Base Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 3)                 369       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 437\n",
      "Trainable params: 437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------START of WEAK LEARNERS (BASE MODELS) - STACK 00 -----------------', file = f)\n",
    "\n",
    "#Defining Basemodels\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "\n",
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "\n",
    "#DNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# #Model Parameters\n",
    "# dropout_rate = 0.01\n",
    "# nodes = 70\n",
    "# out_layer = 5\n",
    "# optimizer='adam'\n",
    "# loss='sparse_categorical_crossentropy'\n",
    "# epochs=1\n",
    "# batch_size=2*256\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train.shape[1]\n",
    "\n",
    "dnn = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "dnn.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn.summary()\n",
    "\n",
    "\n",
    "\n",
    "# dnn = Sequential()\n",
    "# dnn.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))  # Input layer\n",
    "# dnn.add(Dense(64, activation='relu'))  # Hidden layer\n",
    "# dnn.add(Dense(5))  # Output layer\n",
    "\n",
    "# dnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# # summary of model layers\n",
    "# dnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SVM\n",
    "# # Wrap SGDClassifier with MultiOutputClassifier\n",
    "# multi_target_clf = MultiOutputClassifier(clf)\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# multi_target_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.76373552 0.89489631 0.51642876 0.79139452 0.91717729]\n",
      "Mean accuracy: 0.7767264822490351\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.94647186 0.94694317 0.94431726 0.94680493 0.93656993]\n",
      "Mean accuracy: 0.944221429771841\n",
      "---------------------------------------------------------------------------------\n",
      "Training SVM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.9672098  0.96848909 0.96949906 0.96747694 0.96673625]\n",
      "Mean accuracy: 0.9678822274869956\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.98828441 0.98828441 0.98707245 0.98875497 0.98855296]\n",
      "Mean accuracy: 0.988189837165707\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.98323458 0.97468354 0.98330191 0.97851996 0.98363747]\n",
      "Mean accuracy: 0.9806754936362843\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n",
      "Cross-validation scores: [0.99346889 0.99171829 0.99151629 0.99158306 0.99205441]\n",
      "Mean accuracy: 0.9920681879540195\n",
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 4.0439 - accuracy: 0.5089 - val_loss: 2.2821 - val_accuracy: 0.7744\n",
      "Epoch 2/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 2.7600 - accuracy: 0.6116 - val_loss: 1.1206 - val_accuracy: 0.8243\n",
      "Epoch 3/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 2.1140 - accuracy: 0.6486 - val_loss: 0.9958 - val_accuracy: 0.8315\n",
      "Epoch 4/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.7034 - accuracy: 0.6602 - val_loss: 1.0433 - val_accuracy: 0.8332\n",
      "Epoch 5/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.5104 - accuracy: 0.6791 - val_loss: 0.8607 - val_accuracy: 0.8426\n",
      "Epoch 6/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.3561 - accuracy: 0.7176 - val_loss: 0.7658 - val_accuracy: 0.8496\n",
      "Epoch 7/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.2689 - accuracy: 0.7056 - val_loss: 0.6638 - val_accuracy: 0.8532\n",
      "Epoch 8/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.1880 - accuracy: 0.7076 - val_loss: 0.6456 - val_accuracy: 0.8578\n",
      "Epoch 9/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.1408 - accuracy: 0.6985 - val_loss: 0.6917 - val_accuracy: 0.8520\n",
      "Epoch 10/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0366 - accuracy: 0.7324 - val_loss: 0.5796 - val_accuracy: 0.8556\n",
      "Epoch 11/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 1.0151 - accuracy: 0.7272 - val_loss: 0.5877 - val_accuracy: 0.8486\n",
      "Epoch 12/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.9541 - accuracy: 0.7319 - val_loss: 0.5416 - val_accuracy: 0.8533\n",
      "Epoch 13/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.9487 - accuracy: 0.7260 - val_loss: 0.5035 - val_accuracy: 0.8552\n",
      "Epoch 14/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.9769 - accuracy: 0.7321 - val_loss: 0.5273 - val_accuracy: 0.8540\n",
      "Epoch 15/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.8721 - accuracy: 0.7531 - val_loss: 0.5049 - val_accuracy: 0.8554\n",
      "Epoch 16/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.8459 - accuracy: 0.7521 - val_loss: 0.5038 - val_accuracy: 0.8566\n",
      "Epoch 17/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.8181 - accuracy: 0.7507 - val_loss: 0.4873 - val_accuracy: 0.8567\n",
      "Epoch 18/100\n",
      "465/465 [==============================] - 1s 3ms/step - loss: 0.8088 - accuracy: 0.7570 - val_loss: 0.4887 - val_accuracy: 0.8532\n"
     ]
    }
   ],
   "source": [
    "#Training Basemodels\n",
    "import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "n_splits = 5  # You can adjust the number of folds as needed\n",
    "\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training Model')\n",
    "with open(output_file_name, \"a\") as f: print('Training weak models - level 0', file = f)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1 and load_model_ada == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_rf == 1 and load_model_rf == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf = rf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(model_rf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf, 'rf_base_model.joblib')\n",
    "\n",
    "if use_model_svm == 1 and load_model_svm == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    # clf.score(X_train, y_train)\n",
    "    time_taken = end - start\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(clf, 'svm_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_knn == 1 and load_model_knn == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf.fit(X_train,y_train)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf, 'knn_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_lgbm == 1 and load_model_lgbm == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_base_model.joblib')\n",
    "\n",
    "if use_model_mlp == 1 and load_model_mlp == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    # Create the StratifiedKFold object\n",
    "    stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Perform cross-validation\n",
    "    cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_base_model.joblib')\n",
    "\n",
    "\n",
    "if use_model_dnn == 1 and load_model_dnn == 0:\n",
    "    from keras.callbacks import EarlyStopping\n",
    "\n",
    "    # Define EarlyStopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    # Convert Y_test back to its original format\n",
    "    # y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "    # Start the timer\n",
    "    start = time.time()\n",
    "    # dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "    dnn.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # End the timer\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(dnn, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    dnn.save(\"DNN_base_model.h5\")\n",
    "\n",
    "    # Calculate the time taken and print it out\n",
    "    # print(f'Time taken for training: {time_taken} seconds')\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# # Define your Keras model as a function\n",
    "# def create_model(optimizer='adam', hidden_layer_size=16):\n",
    "#     # model = Sequential()\n",
    "#     # model.add(Dense(hidden_layer_size, input_dim=input_size, activation='relu'))\n",
    "#     # model.add(Dense(1, activation='sigmoid'))\n",
    "#     # model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "#     dnn = tf.keras.Sequential()\n",
    "\n",
    "#     # Input layer\n",
    "#     dnn.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "#     # Dense layers with dropout\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     dnn.add(tf.keras.layers.Dense(nodes))\n",
    "#     dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "#     # Output layer\n",
    "#     dnn.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "\n",
    "\n",
    "#     dnn.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "#     dnn.summary()\n",
    "#     return dnn\n",
    "\n",
    "# # Create a KerasClassifier\n",
    "# dnn = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "# # Define the parameter grid for GridSearchCV\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam', 'sgd'],\n",
    "#     'hidden_layer_size': [8, 16, 32]\n",
    "# }\n",
    "\n",
    "# # Create the StratifiedKFold\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# # Create GridSearchCV\n",
    "# grid = GridSearchCV(estimator=dnn, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "# grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# # Print the best parameters and best accuracy\n",
    "# print(\"Best Parameters: \", grid_result.best_params_)\n",
    "# print(\"Best Accuracy: \", grid_result.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Models\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "if load_model_ada == 1:\n",
    "    ada = joblib.load('ada_base_model.joblib')\n",
    "\n",
    "if load_model_svm == 1:\n",
    "    clf =  joblib.load('svm_base_model.joblib')\n",
    "\n",
    "if load_model_dnn == 1:\n",
    "    dnn = load_model(\"DNN_base_model.h5\")\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf = joblib.load('knn_base_model.joblib')\n",
    "\n",
    "if load_model_mlp == 1:\n",
    "    MLP = joblib.load('mlp_base_model.joblib')\n",
    "\n",
    "if load_model_rf == 1:\n",
    "    rf = joblib.load('rf_base_model.joblib')\n",
    "\n",
    "if load_model_lgbm == 1:\n",
    "    lgbm = joblib.load('lgbm_base_model.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "# preds_svm = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# y_scores = y_pred\n",
    "# y_true = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base leaners predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction SVM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction DNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction MLP\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Prediction KNN\n",
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "with open(output_file_name, \"a\") as f: print('Generating Predictions', file = f)\n",
    "\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf = rf.predict(X_test)\n",
    "    preds_rf_prob = rf.predict_proba(X_test)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    print('Prediction SVM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction SVM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #SVM\n",
    "    start = time.time()\n",
    "    preds_svm = clf.predict(X_test)\n",
    "    # preds_svm_prob = clf.predict_proba(X_test)\n",
    "\n",
    "    #Since SVM does not deal with prob by nature we use a meta learner\n",
    "    # https://stackoverflow.com/questions/55250963/how-to-get-probabilities-for-sgdclassifier-linearsvm\n",
    "\n",
    "    model = CalibratedClassifierCV(clf)\n",
    "\n",
    "    model.fit(X, y)\n",
    "    preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm = lgbm.predict(X_test)\n",
    "    preds_lgbm_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    print('Prediction DNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction DNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #DNN\n",
    "    start = time.time()\n",
    "    pred_dnn = dnn.predict(X_test)\n",
    "    preds_dnn_prob = pred_dnn\n",
    "    preds_dnn = np.argmax(pred_dnn,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada = ada.predict(X_test)\n",
    "    preds_ada_prob = ada.predict_proba(X_test)\n",
    "\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test)\n",
    "    preds_mlp_prob = y_pred\n",
    "    preds_mlp = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf.predict(X_test)\n",
    "    preds_knn_prob =knn_clf.predict_proba(X_test)\n",
    "\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# model = CalibratedClassifierCV(clf)\n",
    "\n",
    "# model.fit(X, y)\n",
    "# preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "# print(preds_ada_prob)\n",
    "# print(preds_knn_prob)\n",
    "# print(preds_dnn_prob)\n",
    "# print(preds_mlp_prob)\n",
    "# print(preds_rf_prob)\n",
    "# print(preds_svm_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.72093733e-01 7.45689618e-03 2.85238807e-02 9.04090624e-02\n",
      "  1.51642739e-03]\n",
      " [8.58613008e-01 2.04807623e-04 1.09593308e-01 3.09428029e-02\n",
      "  6.46073102e-04]\n",
      " [1.25468259e-01 8.27396976e-01 3.85425919e-02 8.41942724e-03\n",
      "  1.72745838e-04]\n",
      " ...\n",
      " [3.93095065e-01 6.06350746e-01 2.88756006e-04 8.55018688e-07\n",
      "  2.64577935e-04]\n",
      " [9.47981386e-01 2.33621240e-03 9.44433097e-03 3.99189637e-02\n",
      "  3.19107346e-04]\n",
      " [3.21028204e-01 3.57404714e-02 6.43035240e-01 5.19631068e-08\n",
      "  1.96033090e-04]]\n",
      "[0 0 1 ... 1 0 2]\n",
      "[0 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(preds_svm_prob)\n",
    "preds_3 = np.argmax(preds_svm_prob,axis = 1)\n",
    "print(preds_3)\n",
    "\n",
    "print(preds_svm)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METRICS - Base Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "# >>> \n",
    "# >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
    "# 0.99...\n",
    "# >>> roc_auc_score(y, clf.decision_function(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.2936113\ttest: 1.2929301\tbest: 1.2929301 (0)\ttotal: 27.4ms\tremaining: 2.71s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10:\tlearn: 0.4169585\ttest: 0.4165954\tbest: 0.4165954 (10)\ttotal: 229ms\tremaining: 1.85s\n",
      "20:\tlearn: 0.2087191\ttest: 0.2088132\tbest: 0.2088132 (20)\ttotal: 431ms\tremaining: 1.62s\n",
      "30:\tlearn: 0.1274849\ttest: 0.1280464\tbest: 0.1280464 (30)\ttotal: 622ms\tremaining: 1.38s\n",
      "40:\tlearn: 0.0911167\ttest: 0.0919316\tbest: 0.0919316 (40)\ttotal: 800ms\tremaining: 1.15s\n",
      "50:\tlearn: 0.0723743\ttest: 0.0734236\tbest: 0.0734236 (50)\ttotal: 964ms\tremaining: 926ms\n",
      "60:\tlearn: 0.0608892\ttest: 0.0620483\tbest: 0.0620483 (60)\ttotal: 1.12s\tremaining: 718ms\n",
      "70:\tlearn: 0.0529493\ttest: 0.0543260\tbest: 0.0543260 (70)\ttotal: 1.28s\tremaining: 524ms\n",
      "80:\tlearn: 0.0478477\ttest: 0.0492849\tbest: 0.0492849 (80)\ttotal: 1.44s\tremaining: 337ms\n",
      "90:\tlearn: 0.0434513\ttest: 0.0449536\tbest: 0.0449536 (90)\ttotal: 1.59s\tremaining: 158ms\n",
      "99:\tlearn: 0.0410736\ttest: 0.0426801\tbest: 0.0426801 (99)\ttotal: 1.74s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.04268008494\n",
      "bestIteration = 99\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38316.0     31.0    77.0    90.0   4.0\n",
      "1     35.0  26655.0     5.0     1.0   0.0\n",
      "2     94.0     19.0  6913.0     5.0   0.0\n",
      "3    178.0      2.0     8.0  1765.0   2.0\n",
      "4     32.0      0.0     0.0    10.0  17.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9920144359606243\n",
      "Precision total:  0.9317815323212976\n",
      "Recall total:  0.8334771923068951\n",
      "F1 total:  0.8627391808106865\n",
      "BACC total:  0.8334771923068951\n",
      "MCC total:  0.9864960767039153\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "cat_00 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "cat_00.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = cat_00.predict(X_test)\n",
    "preds_cat_prob = cat_00.predict_proba(X_test)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Catboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_cat\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    cat_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    cat_acc_00 = Acc\n",
    "    cat_pre_00 = Precision\n",
    "    cat_rec_00 = Recall\n",
    "    cat_f1_00 = F1\n",
    "    cat_bacc_00 = BACC\n",
    "    cat_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your features and labels as X and y\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_00 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb = xgb_00.predict(dtest)\n",
    "# preds_xgb_prob = xgb_00.predict_proba(dtest)\n",
    "\n",
    "\n",
    "# Get class probabilities\n",
    "# Assuming binary classification, get the probability for the positive class (class 1)\n",
    "preds_xgb_margin = xgb_00.predict(dtest, output_margin=True)\n",
    "preds_xgb_prob = 1 / (1 + np.exp(-preds_xgb_margin))\n",
    "\n",
    "# Print or use positive_class_probabilities as needed\n",
    "# print(positive_class_probabilities)\n",
    "\n",
    "\n",
    "# Convert predicted probabilities to class labels (if necessary)\n",
    "# y_pred_labels = [round(value) for value in y_pred]\n",
    "\n",
    "# Evaluate the accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0      1.0     2.0     3.0   4.0\n",
      "0.0  38253.0     34.0   109.0   116.0   6.0\n",
      "1.0     33.0  26656.0     7.0     0.0   0.0\n",
      "2.0     81.0     10.0  6924.0    16.0   0.0\n",
      "3.0    146.0      0.0     2.0  1797.0  10.0\n",
      "4.0     18.0      0.0     0.0     7.0  34.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9919875031982656\n",
      "Precision total:  0.916516593252323\n",
      "Recall total:  0.8943712402251023\n",
      "F1 total:  0.9045829083245825\n",
      "BACC total:  0.8943712402251023\n",
      "MCC total:  0.9864640766078165\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    xgb_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    xgb_acc_00 = Acc\n",
    "    xgb_pre_00 = Precision\n",
    "    xgb_rec_00 = Recall\n",
    "    xgb_f1_00 = F1\n",
    "    xgb_bacc_00 = BACC\n",
    "    xgb_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "# pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2      3    4\n",
      "0  38438.0     19.0    53.0    8.0  0.0\n",
      "1    896.0  25784.0    16.0    0.0  0.0\n",
      "2    657.0    185.0  6186.0    3.0  0.0\n",
      "3   1616.0      6.0    64.0  269.0  0.0\n",
      "4     59.0      0.0     0.0    0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9517634226154406\n",
      "Precision total:  0.7708229249221301\n",
      "Recall total:  0.5962348969277227\n",
      "F1 total:  0.6209815960412555\n",
      "BACC total:  0.5962348969277227\n",
      "MCC total:  0.9185607045845918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "if use_model_rf == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('RF base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_rf\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    rf_acc_00 = Acc\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    rf_acc_00 = Acc\n",
    "    rf_pre_00 = Precision\n",
    "    rf_rec_00 = Recall\n",
    "    rf_f1_00 = F1\n",
    "    rf_bacc_00 = BACC\n",
    "    rf_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0        1     2    3    4\n",
      "0  38118.0    375.0  25.0  0.0  0.0\n",
      "1   1058.0  25638.0   0.0  0.0  0.0\n",
      "2   6412.0    619.0   0.0  0.0  0.0\n",
      "3   1943.0     11.0   1.0  0.0  0.0\n",
      "4     54.0      2.0   3.0  0.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8585760648540918\n",
      "Precision total:  0.35954806058543765\n",
      "Recall total:  0.3900990696636019\n",
      "F1 total:  0.369539913853707\n",
      "BACC total:  0.3900990696636019\n",
      "MCC total:  0.7605774539729424\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "if use_model_dnn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('DNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_dnn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    dnn_acc_00 = Acc\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    dnn_acc_00 = Acc\n",
    "    dnn_pre_00 = Precision\n",
    "    dnn_rec_00 = Recall\n",
    "    dnn_f1_00 = F1\n",
    "    dnn_bacc_00 = BACC\n",
    "    dnn_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1        2       3     4\n",
      "0  36891.0   338.0    625.0   656.0   8.0\n",
      "1   3152.0  3451.0  20090.0     3.0   0.0\n",
      "2   2588.0   251.0   4192.0     0.0   0.0\n",
      "3    777.0    16.0     14.0  1143.0   5.0\n",
      "4     19.0     0.0      0.0    14.0  26.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.615454019041463\n",
      "Precision total:  0.632923263976516\n",
      "Recall total:  0.541715952581571\n",
      "F1 total:  0.5048072353676654\n",
      "BACC total:  0.541715952581571\n",
      "MCC total:  0.4591745406397964\n"
     ]
    }
   ],
   "source": [
    "#ADA\n",
    "if use_model_ada == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    ada_acc_00 = Acc\n",
    "\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "\n",
    "    ada_acc_00 = Acc\n",
    "    ada_pre_00 = Precision\n",
    "    ada_rec_00 = Recall\n",
    "    ada_f1_00 = F1\n",
    "    ada_bacc_00 = BACC\n",
    "    ada_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  37851.0    195.0   316.0   149.0   7.0\n",
      "1    485.0  26139.0    22.0    50.0   0.0\n",
      "2    274.0    117.0  6616.0    24.0   0.0\n",
      "3    573.0      9.0     9.0  1361.0   3.0\n",
      "4     32.0      1.0     0.0     7.0  19.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9693909155792564\n",
      "Precision total:  0.8827645887402553\n",
      "Recall total:  0.784198426409409\n",
      "F1 total:  0.8204655892263848\n",
      "BACC total:  0.784198426409409\n",
      "MCC total:  0.9481081827724333\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "if use_model_svm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('SVM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_svm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    svm_acc_00 = Acc\n",
    "    svm_pre_00 = Precision\n",
    "    svm_rec_00 = Recall\n",
    "    svm_f1_00 = F1\n",
    "    svm_bacc_00 = BACC\n",
    "    svm_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38242.0     47.0   100.0   124.0   5.0\n",
      "1     39.0  26654.0     3.0     0.0   0.0\n",
      "2    110.0    177.0  6738.0     6.0   0.0\n",
      "3    162.0      8.0     6.0  1775.0   4.0\n",
      "4     21.0      0.0     1.0     8.0  29.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9889441010517244\n",
      "Precision total:  0.9315441895221477\n",
      "Recall total:  0.8698084938389755\n",
      "F1 total:  0.8947376203615685\n",
      "BACC total:  0.8698084938389755\n",
      "MCC total:  0.9813055345538203\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "\n",
    "    knn_acc_00 = Acc\n",
    "    knn_pre_00 = Precision\n",
    "    knn_rec_00 = Recall\n",
    "    knn_f1_00 = F1\n",
    "    knn_bacc_00 = BACC\n",
    "    knn_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38235.0     37.0    98.0   143.0   5.0\n",
      "1     35.0  26653.0     5.0     3.0   0.0\n",
      "2     71.0     22.0  6931.0     7.0   0.0\n",
      "3    102.0      1.0     4.0  1844.0   4.0\n",
      "4     15.0      0.0     1.0    11.0  32.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9924049610148264\n",
      "Precision total:  0.9350853557454828\n",
      "Recall total:  0.8924829434684349\n",
      "F1 total:  0.9094645814131586\n",
      "BACC total:  0.8924829434684349\n",
      "MCC total:  0.9871796012439795\n"
     ]
    }
   ],
   "source": [
    "#MLP\n",
    "if use_model_mlp == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "\n",
    "    mlp_acc_00 = Acc\n",
    "    mlp_pre_00 = Precision\n",
    "    mlp_rec_00 = Recall\n",
    "    mlp_f1_00 = F1\n",
    "    mlp_bacc_00 = BACC\n",
    "    mlp_mcc_00 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0        1       2       3     4\n",
      "0  38019.0    199.0   128.0   117.0  55.0\n",
      "1     79.0  26361.0   118.0   132.0   6.0\n",
      "2     98.0    270.0  6627.0    22.0  14.0\n",
      "3    108.0     11.0    25.0  1791.0  20.0\n",
      "4     30.0      3.0     0.0     9.0  17.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9805545455769671\n",
      "Precision total:  0.7902176964513938\n",
      "Recall total:  0.82425692517516\n",
      "F1 total:  0.8028434238196789\n",
      "BACC total:  0.82425692517516\n",
      "MCC total:  0.9672085090622727\n"
     ]
    }
   ],
   "source": [
    "#lgbm\n",
    "\n",
    "if use_model_lgbm == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_lgbm\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test, pred_label)\n",
    "    Precision = precision_score(y_test, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test, pred_label)\n",
    "    lgbm_acc_00 = Acc\n",
    "    lgbm_pre_00 = Precision\n",
    "    lgbm_rec_00 = Recall\n",
    "    lgbm_f1_00 = F1\n",
    "    lgbm_bacc_00 = BACC\n",
    "    lgbm_mcc_00 = MCC\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the stronger model - STACK level 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74259 74259\n"
     ]
    }
   ],
   "source": [
    "print(len(preds_dnn_prob), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54891     0\n",
      "32459     1\n",
      "82084     1\n",
      "115832    0\n",
      "88729     1\n",
      "         ..\n",
      "146686    0\n",
      "77911     2\n",
      "139901    0\n",
      "39294     0\n",
      "130461    1\n",
      "Name: Label, Length: 74259, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        index  Label\n",
      "0       54891      0\n",
      "1       32459      1\n",
      "2       82084      1\n",
      "3      115832      0\n",
      "4       88729      1\n",
      "...       ...    ...\n",
      "74254  146686      0\n",
      "74255   77911      2\n",
      "74256  139901      0\n",
      "74257   39294      0\n",
      "74258  130461      1\n",
      "\n",
      "[74259 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         54891\n",
       "1         32459\n",
       "2         82084\n",
       "3        115832\n",
       "4         88729\n",
       "          ...  \n",
       "74254    146686\n",
       "74255     77911\n",
       "74256    139901\n",
       "74257     39294\n",
       "74258    130461\n",
       "Name: index, Length: 74259, dtype: int64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_series = y_test.to_frame()\n",
    "y_test_reset_index = df_from_series.reset_index()\n",
    "# y_test2 = y_test.reset_index(inplace=True)\n",
    "print(y_test_reset_index)\n",
    "y_test_reset_index.pop('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_reset_index.values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_dnn_2 = []\n",
    "preds_svm_2 = []\n",
    "preds_rf_2 = []\n",
    "preds_mlp_2 = []\n",
    "preds_ada_2 = []\n",
    "preds_knn_2 = []\n",
    "preds_lgbm_2 = []\n",
    "preds_cat_2 = []\n",
    "preds_xgb_2 = []\n",
    "\n",
    "for i in range(0,len(preds_dnn_prob)):  \n",
    "    # print(i)\n",
    "    # print(preds_dnn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_dnn_2.append(preds_dnn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_svm_2.append(preds_svm_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_rf_2.append(preds_rf_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_mlp_2.append(preds_mlp_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_ada_2.append(preds_ada_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_knn_2.append(preds_knn_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_lgbm_2.append(preds_lgbm_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_cat_2.append(preds_cat_prob[i][y_test_reset_index.values[i][0]])\n",
    "    preds_xgb_2.append(preds_xgb_prob[i][y_test_reset_index.values[i][0]])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2974636  0.83474243 0.99985337 ... 0.98826399 0.98765093 0.        ]\n",
      " [0.72982407 0.99633156 1.         ... 0.99718377 0.99776554 1.        ]\n",
      " [0.70787787 0.76533758 1.         ... 0.97737626 0.99565542 1.        ]\n",
      " ...\n",
      " [0.3834421  0.81693161 0.99999204 ... 0.98769927 0.98844504 0.        ]\n",
      " [0.62206382 0.95777388 1.         ... 0.99638074 0.99334705 0.        ]\n",
      " [0.9766444  0.94678884 1.         ... 0.99247126 0.99238926 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('------------START of STRONGER LEARNER - STACK 01 -----------------', file = f)\n",
    "\n",
    "\n",
    "# Stack the vectors horizontally to create a matrix\n",
    "column_features = ['dnn','rf','lgbm','ada','knn','mlp','svm','cat','xgb','label']\n",
    "training_matrix = np.column_stack((\n",
    "                          preds_dnn_2,\n",
    "                          preds_rf_2,\n",
    "                          preds_lgbm_2,\n",
    "                          preds_ada_2,\n",
    "                          preds_knn_2, \n",
    "                          preds_mlp_2,\n",
    "                          preds_svm_2,\n",
    "                          preds_cat_2,\n",
    "                          preds_xgb_2,\n",
    "                          y_test\n",
    "                          ))\n",
    "\n",
    "# Print the resulting matrix\n",
    "print(training_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_level_01 = pd.DataFrame(training_matrix, columns=column_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming df is your DataFrame\n",
    "df_level_01.to_csv('models7dataset_prob.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_01 = df_level_01.pop('label')\n",
    "X_01 = df_level_01\n",
    "df_level_01 = df_level_01.assign(label = y_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297464</td>\n",
       "      <td>0.834742</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.263048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872094</td>\n",
       "      <td>0.988264</td>\n",
       "      <td>0.987651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.729824</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282531</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.997184</td>\n",
       "      <td>0.997766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707878</td>\n",
       "      <td>0.765338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827397</td>\n",
       "      <td>0.977376</td>\n",
       "      <td>0.995655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397967</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970162</td>\n",
       "      <td>0.992576</td>\n",
       "      <td>0.997013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.695965</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.293208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082876</td>\n",
       "      <td>0.996701</td>\n",
       "      <td>0.997390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.360355</td>\n",
       "      <td>0.866679</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.266256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981861</td>\n",
       "      <td>0.995916</td>\n",
       "      <td>0.995133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>0.282128</td>\n",
       "      <td>0.803362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.979811</td>\n",
       "      <td>0.996661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.816932</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.256380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.987699</td>\n",
       "      <td>0.988445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.622064</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.264476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981942</td>\n",
       "      <td>0.996381</td>\n",
       "      <td>0.993347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.976644</td>\n",
       "      <td>0.946789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.992471</td>\n",
       "      <td>0.992389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dnn        rf      lgbm       ada  knn       mlp       svm  \\\n",
       "0      0.297464  0.834742  0.999853  0.263048  1.0  1.000000  0.872094   \n",
       "1      0.729824  0.996332  1.000000  0.282531  1.0  1.000000  0.000205   \n",
       "2      0.707878  0.765338  1.000000  0.237124  1.0  1.000000  0.827397   \n",
       "3      0.397967  0.922330  1.000000  0.277063  1.0  1.000000  0.970162   \n",
       "4      0.695965  0.996332  1.000000  0.293208  1.0  1.000000  0.082876   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "74254  0.360355  0.866679  0.999983  0.266256  1.0  1.000000  0.981861   \n",
       "74255  0.282128  0.803362  1.000000  0.328061  1.0  0.999999  0.038218   \n",
       "74256  0.383442  0.816932  0.999992  0.256380  1.0  1.000000  0.008064   \n",
       "74257  0.622064  0.957774  1.000000  0.264476  1.0  1.000000  0.981942   \n",
       "74258  0.976644  0.946789  1.000000  0.275003  1.0  1.000000  0.033696   \n",
       "\n",
       "            cat       xgb  \n",
       "0      0.988264  0.987651  \n",
       "1      0.997184  0.997766  \n",
       "2      0.977376  0.995655  \n",
       "3      0.992576  0.997013  \n",
       "4      0.996701  0.997390  \n",
       "...         ...       ...  \n",
       "74254  0.995916  0.995133  \n",
       "74255  0.979811  0.996661  \n",
       "74256  0.987699  0.988445  \n",
       "74257  0.996381  0.993347  \n",
       "74258  0.992471  0.992389  \n",
       "\n",
       "[74259 rows x 9 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        1.0\n",
       "2        1.0\n",
       "3        0.0\n",
       "4        1.0\n",
       "        ... \n",
       "74254    0.0\n",
       "74255    2.0\n",
       "74256    0.0\n",
       "74257    0.0\n",
       "74258    1.0\n",
       "Name: label, Length: 74259, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dnn</th>\n",
       "      <th>rf</th>\n",
       "      <th>lgbm</th>\n",
       "      <th>ada</th>\n",
       "      <th>knn</th>\n",
       "      <th>mlp</th>\n",
       "      <th>svm</th>\n",
       "      <th>cat</th>\n",
       "      <th>xgb</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.297464</td>\n",
       "      <td>0.834742</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>0.263048</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872094</td>\n",
       "      <td>0.988264</td>\n",
       "      <td>0.987651</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.729824</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.282531</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.997184</td>\n",
       "      <td>0.997766</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.707878</td>\n",
       "      <td>0.765338</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.237124</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.827397</td>\n",
       "      <td>0.977376</td>\n",
       "      <td>0.995655</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.397967</td>\n",
       "      <td>0.922330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.277063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970162</td>\n",
       "      <td>0.992576</td>\n",
       "      <td>0.997013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.695965</td>\n",
       "      <td>0.996332</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.293208</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.082876</td>\n",
       "      <td>0.996701</td>\n",
       "      <td>0.997390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74254</th>\n",
       "      <td>0.360355</td>\n",
       "      <td>0.866679</td>\n",
       "      <td>0.999983</td>\n",
       "      <td>0.266256</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981861</td>\n",
       "      <td>0.995916</td>\n",
       "      <td>0.995133</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74255</th>\n",
       "      <td>0.282128</td>\n",
       "      <td>0.803362</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328061</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.038218</td>\n",
       "      <td>0.979811</td>\n",
       "      <td>0.996661</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74256</th>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.816932</td>\n",
       "      <td>0.999992</td>\n",
       "      <td>0.256380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.987699</td>\n",
       "      <td>0.988445</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74257</th>\n",
       "      <td>0.622064</td>\n",
       "      <td>0.957774</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.264476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981942</td>\n",
       "      <td>0.996381</td>\n",
       "      <td>0.993347</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74258</th>\n",
       "      <td>0.976644</td>\n",
       "      <td>0.946789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.275003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033696</td>\n",
       "      <td>0.992471</td>\n",
       "      <td>0.992389</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74259 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            dnn        rf      lgbm       ada  knn       mlp       svm  \\\n",
       "0      0.297464  0.834742  0.999853  0.263048  1.0  1.000000  0.872094   \n",
       "1      0.729824  0.996332  1.000000  0.282531  1.0  1.000000  0.000205   \n",
       "2      0.707878  0.765338  1.000000  0.237124  1.0  1.000000  0.827397   \n",
       "3      0.397967  0.922330  1.000000  0.277063  1.0  1.000000  0.970162   \n",
       "4      0.695965  0.996332  1.000000  0.293208  1.0  1.000000  0.082876   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "74254  0.360355  0.866679  0.999983  0.266256  1.0  1.000000  0.981861   \n",
       "74255  0.282128  0.803362  1.000000  0.328061  1.0  0.999999  0.038218   \n",
       "74256  0.383442  0.816932  0.999992  0.256380  1.0  1.000000  0.008064   \n",
       "74257  0.622064  0.957774  1.000000  0.264476  1.0  1.000000  0.981942   \n",
       "74258  0.976644  0.946789  1.000000  0.275003  1.0  1.000000  0.033696   \n",
       "\n",
       "            cat       xgb  label  \n",
       "0      0.988264  0.987651    0.0  \n",
       "1      0.997184  0.997766    1.0  \n",
       "2      0.977376  0.995655    1.0  \n",
       "3      0.992576  0.997013    0.0  \n",
       "4      0.996701  0.997390    1.0  \n",
       "...         ...       ...    ...  \n",
       "74254  0.995916  0.995133    0.0  \n",
       "74255  0.979811  0.996661    2.0  \n",
       "74256  0.987699  0.988445    0.0  \n",
       "74257  0.996381  0.993347    0.0  \n",
       "74258  0.992471  0.992389    1.0  \n",
       "\n",
       "[74259 rows x 10 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_level_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.7\n",
    "\n",
    "X_train_01,X_test_01, y_train_01, y_test_01 = sklearn.model_selection.train_test_split(X_01, y_01, train_size=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# # Define EarlyStopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "# # Compile the model\n",
    "# # model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# # Train the model with EarlyStopping callback\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# # Save the trained model\n",
    "# # model.save(\"CNN_CIC_1.h5\")\n",
    "# model = load_model(\"CNN_CIC_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining DNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 3)                 30        \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 5)                 20        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining DNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "#Model Parameters\n",
    "dropout_rate = 0.2\n",
    "nodes = 3\n",
    "out_layer = 5\n",
    "optimizer='adam'\n",
    "loss='sparse_categorical_crossentropy'\n",
    "epochs=100\n",
    "batch_size=128\n",
    "\n",
    "\n",
    "num_columns = X_train_01.shape[1]\n",
    "\n",
    "dnn_01 = tf.keras.Sequential()\n",
    "\n",
    "# Input layer\n",
    "dnn_01.add(tf.keras.Input(shape=(num_columns,)))\n",
    "\n",
    "# # Dense layers with dropout\n",
    "# dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(3*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn_01.add(tf.keras.layers.Dense(2*nodes))\n",
    "# dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# dnn.add(tf.keras.layers.Dense(nodes))\n",
    "# dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "\n",
    "\n",
    "# Dense layers with dropout\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(nodes))\n",
    "dnn_01.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "dnn.add(tf.keras.layers.Dense(nodes))\n",
    "dnn.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "# Output layer\n",
    "# dnn_01.add(tf.keras.layers.Dense(out_layer))\n",
    "\n",
    "dnn_01.add(tf.keras.layers.Dense(out_layer, activation='softmax'))\n",
    "\n",
    "\n",
    "dnn_01.compile(optimizer=optimizer, loss=loss,metrics=['accuracy'])\n",
    "\n",
    "dnn_01.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Training DNN\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/100\n",
      "193/325 [================>.............] - ETA: 0s - loss: 1.4107 - accuracy: 0.4446"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "325/325 [==============================] - 1s 3ms/step - loss: 1.3045 - accuracy: 0.4752 - val_loss: 1.0024 - val_accuracy: 0.6551\n",
      "Epoch 2/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 1.0343 - accuracy: 0.5392 - val_loss: 0.8701 - val_accuracy: 0.6791\n",
      "Epoch 3/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 0.9312 - accuracy: 0.5669 - val_loss: 0.7621 - val_accuracy: 0.6826\n",
      "Epoch 4/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8692 - accuracy: 0.6055 - val_loss: 0.7259 - val_accuracy: 0.7005\n",
      "Epoch 5/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8318 - accuracy: 0.6308 - val_loss: 0.7000 - val_accuracy: 0.7309\n",
      "Epoch 6/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8285 - accuracy: 0.6432 - val_loss: 0.6957 - val_accuracy: 0.7353\n",
      "Epoch 7/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8129 - accuracy: 0.6533 - val_loss: 0.6861 - val_accuracy: 0.7124\n",
      "Epoch 8/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.8056 - accuracy: 0.6609 - val_loss: 0.6828 - val_accuracy: 0.7470\n",
      "Epoch 9/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7976 - accuracy: 0.6641 - val_loss: 0.6757 - val_accuracy: 0.7400\n",
      "Epoch 10/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7893 - accuracy: 0.6730 - val_loss: 0.6696 - val_accuracy: 0.7421\n",
      "Epoch 11/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7803 - accuracy: 0.6707 - val_loss: 0.6637 - val_accuracy: 0.7506\n",
      "Epoch 12/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7854 - accuracy: 0.6747 - val_loss: 0.6625 - val_accuracy: 0.7569\n",
      "Epoch 13/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7770 - accuracy: 0.6735 - val_loss: 0.6559 - val_accuracy: 0.7559\n",
      "Epoch 14/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7751 - accuracy: 0.6754 - val_loss: 0.6576 - val_accuracy: 0.7417\n",
      "Epoch 15/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7643 - accuracy: 0.6805 - val_loss: 0.6526 - val_accuracy: 0.7308\n",
      "Epoch 16/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7795 - accuracy: 0.6793 - val_loss: 0.6553 - val_accuracy: 0.7387\n",
      "Epoch 17/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7613 - accuracy: 0.6822 - val_loss: 0.6484 - val_accuracy: 0.7407\n",
      "Epoch 18/100\n",
      "325/325 [==============================] - 1s 2ms/step - loss: 0.7517 - accuracy: 0.6819 - val_loss: 0.6405 - val_accuracy: 0.7766\n",
      "Epoch 19/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7606 - accuracy: 0.6837 - val_loss: 0.6448 - val_accuracy: 0.7602\n",
      "Epoch 20/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7583 - accuracy: 0.6822 - val_loss: 0.6400 - val_accuracy: 0.7628\n",
      "Epoch 21/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7486 - accuracy: 0.6845 - val_loss: 0.6342 - val_accuracy: 0.7562\n",
      "Epoch 22/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7479 - accuracy: 0.6877 - val_loss: 0.6376 - val_accuracy: 0.7639\n",
      "Epoch 23/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7532 - accuracy: 0.6854 - val_loss: 0.6346 - val_accuracy: 0.7622\n",
      "Epoch 24/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7507 - accuracy: 0.6862 - val_loss: 0.6291 - val_accuracy: 0.7683\n",
      "Epoch 25/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7497 - accuracy: 0.6887 - val_loss: 0.6348 - val_accuracy: 0.7648\n",
      "Epoch 26/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7420 - accuracy: 0.6924 - val_loss: 0.6305 - val_accuracy: 0.7436\n",
      "Epoch 27/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7468 - accuracy: 0.6896 - val_loss: 0.6284 - val_accuracy: 0.7489\n",
      "Epoch 28/100\n",
      "325/325 [==============================] - 1s 3ms/step - loss: 0.7412 - accuracy: 0.6890 - val_loss: 0.6280 - val_accuracy: 0.7599\n"
     ]
    }
   ],
   "source": [
    "#DNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Training DNN')\n",
    "with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Training DNN', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "# Convert Y_test back to its original format\n",
    "# y_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "# Start the timer\n",
    "start = time.time()\n",
    "# dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size)\n",
    "dnn_01.fit(X_train_01, y_train_01, epochs=epochs, batch_size=batch_size,validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# model.fit(x_train, Y_train, epochs=100, batch_size=128, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# End the timer\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "# joblib.dump(dnn_01, 'dnn_level_01.joblib')\n",
    "dnn_01.save(\"dnn_level_01.h5\")\n",
    "\n",
    "# Calculate the time taken and print it out\n",
    "# print(f'Time taken for training: {time_taken} seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_01 = load_model(\"dnn_level_01.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNN\n",
    "start = time.time()\n",
    "pred_dnn = dnn_01.predict(X_test_01)\n",
    "preds_dnn_01 = np.argmax(pred_dnn,axis = 1)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = y_test_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "with open(output_file_name, \"a\") as f: print('Stack model - Strong learner - level 01', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0   3.0  4.0\n",
      "0.0  10612.0  1004.0    43.0  11.0  0.0\n",
      "1.0   2446.0  5246.0   266.0   3.0  0.0\n",
      "2.0    619.0     2.0  1456.0   1.0  0.0\n",
      "3.0      0.0     3.0   549.0   0.0  0.0\n",
      "4.0      0.0     0.0     6.0  11.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.777179280007182\n",
      "Precision total:  0.4484352807140194\n",
      "Recall total:  0.45379527103143447\n",
      "F1 total:  0.44749970805741135\n",
      "BACC total:  0.45379527103143447\n",
      "MCC total:  0.6166245634393946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('DNN', file = f)\n",
    "pred_label = preds_dnn_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "dnn_acc_01 = Acc\n",
    "dnn_pre_01 = Precision\n",
    "dnn_rec_01 = Recall\n",
    "dnn_f1_01 = F1\n",
    "dnn_bacc_01 = BACC\n",
    "dnn_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining SVM Model\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining SVM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Instantiate the SGDClassifier with additional hyperparameters\n",
    "clf = SGDClassifier(\n",
    "    loss='hinge',           # hinge loss for linear SVM\n",
    "    penalty='l2',           # L2 regularization to prevent overfitting\n",
    "    alpha=1e-4,             # Learning rate (small value for fine-grained updates)\n",
    "    max_iter=1000,          # Number of passes over the training data\n",
    "    random_state=42,        # Seed for reproducible results\n",
    "    learning_rate='optimal' # Automatically adjusts the learning rate based on the training data\n",
    ")\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "clf.fit(X_train_01, y_train_01)\n",
    "end = time.time()\n",
    "clf.score(X_train_01, y_train_01)\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "joblib.dump(clf, 'svm_level_01.joblib')\n",
    "\n",
    "\n",
    "clf = loaded_model = joblib.load('svm_level_01.joblib')\n",
    "\n",
    "\n",
    "#SVM\n",
    "start = time.time()\n",
    "preds_svm_01 = clf.predict(X_test_01)\n",
    "end = time.time()\n",
    "time_taken = end - start\n",
    "with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  10530.0  1032.0    97.0    0.0  11.0\n",
      "1.0   2076.0  5677.0   174.0   30.0   4.0\n",
      "2.0    201.0     1.0  1825.0   50.0   1.0\n",
      "3.0     15.0     0.0   311.0  226.0   0.0\n",
      "4.0      4.0     0.0     9.0    4.0   0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8195529221653649\n",
      "Precision total:  0.6302904680558632\n",
      "Recall total:  0.5806167198793248\n",
      "F1 total:  0.5940391791421344\n",
      "BACC total:  0.5806167198793248\n",
      "MCC total:  0.6928887053722315\n"
     ]
    }
   ],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('SVM', file = f)\n",
    "pred_label = preds_svm_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "\n",
    "svm_acc_01 = Acc\n",
    "svm_pre_01 = Precision\n",
    "svm_rec_01 = Recall\n",
    "svm_f1_01 = F1\n",
    "svm_bacc_01 = BACC\n",
    "svm_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining RF Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training RF\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Prediction RF\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11563.0    91.0    16.0    0.0  0.0\n",
      "1.0    842.0  6895.0   189.0   35.0  0.0\n",
      "2.0     54.0    21.0  1962.0   41.0  0.0\n",
      "3.0     12.0     2.0    53.0  485.0  0.0\n",
      "4.0      0.0     0.0     1.0   16.0  0.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.938369692072897\n",
      "Precision total:  0.7269733471723125\n",
      "Recall total:  0.735945739363979\n",
      "F1 total:  0.7302137693773736\n",
      "BACC total:  0.735945739363979\n",
      "MCC total:  0.8970103559378139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining RF Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "rf = RandomForestClassifier(max_depth = 5,  n_estimators = 10, min_samples_split = 2, n_jobs = -1)\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "if True == True:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Training RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    model_rf_01 = rf.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(model_rf_01, X_train_01, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(model_rf_01, 'rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    model_rf_01  = joblib.load('rf_base_model_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Prediction RF')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #RF\n",
    "    start = time.time()\n",
    "    preds_rf_01 = model_rf_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('-------------------------------------------------------', file = f)\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "with open(output_file_name, \"a\") as f: print('RF', file = f)\n",
    "pred_label = preds_rf_01\n",
    "\n",
    "# pred_label = ypred\n",
    "#pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# Precision = PRECISION(TP_total, FP_total)\n",
    "# Recall = RECALL(TP_total, FN_total)\n",
    "# F1 = F1(Recall,Precision)\n",
    "# BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "rf_acc_01 = Acc\n",
    "rf_pre_01 = Precision\n",
    "rf_rec_01 = Recall\n",
    "rf_f1_01 = F1\n",
    "rf_bacc_01 = BACC\n",
    "rf_mcc_01 = MCC\n",
    "\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.938369692072897"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining LGBM Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LGBM\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction LGBM\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  11522.0    87.0    53.0    6.0  2.0\n",
      "1.0    105.0  7755.0    67.0   25.0  9.0\n",
      "2.0     15.0    66.0  1958.0   32.0  7.0\n",
      "3.0      2.0     5.0     9.0  528.0  8.0\n",
      "4.0      0.0     2.0     1.0   11.0  3.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9770176856091211\n",
      "Precision total:  0.777514374044633\n",
      "Recall total:  0.8073372511731085\n",
      "F1 total:  0.7901736960971538\n",
      "BACC total:  0.8073372511731085\n",
      "MCC total:  0.9610472908008874\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining LGBM Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#LGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "lgbm = LGBMClassifier()\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    lgbm.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(lgbm, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(lgbm, 'lgbm_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    lgbm = joblib.load('lgbm_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction LGBM')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #LGBM\n",
    "    start = time.time()\n",
    "    preds_lgbm_01 = lgbm.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('LGBM', file = f)\n",
    "    pred_label = preds_lgbm_01\n",
    "\n",
    "    # pred_label = ypred\n",
    "    #pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    lgbm_acc_01 = Acc\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    lgbm_acc_01 = Acc\n",
    "    lgbm_pre_01 = Precision\n",
    "    lgbm_rec_01 = Recall\n",
    "    lgbm_f1_01 = F1\n",
    "    lgbm_bacc_01 = BACC\n",
    "    lgbm_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining MLP Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training MLP\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0       1       2      3     4\n",
      "0  11267.0   377.0    23.0    2.0   1.0\n",
      "1    729.0  7065.0   160.0    7.0   0.0\n",
      "2      2.0    43.0  2007.0   26.0   0.0\n",
      "3      2.0     0.0    15.0  535.0   0.0\n",
      "4      0.0     0.0     0.0    0.0  17.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9377412694137714\n",
      "Precision total:  0.9352098919045874\n",
      "Recall total:  0.9575907528934161\n",
      "F1 total:  0.9458166048227209\n",
      "BACC total:  0.9575907528934161\n",
      "MCC total:  0.8946421624803115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#MLP\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining MLP Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import time\n",
    "\n",
    "# create MLPClassifier instance\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=1)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training MLP')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training MLP', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    start = time.time()\n",
    "    MLP = mlp.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(MLP, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(MLP, 'mlp_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "    MLP = joblib.load('mlp_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #MLP\n",
    "    start = time.time()\n",
    "    y_pred = MLP.predict_proba(X_test_01)\n",
    "    preds_mlp_01 = np.argmax(y_pred,axis = 1)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('MLP 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_mlp_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    mlp_acc_01 = Acc\n",
    "    mlp_pre_01 = Precision\n",
    "    mlp_rec_01 = Recall\n",
    "    mlp_f1_01 = F1\n",
    "    mlp_bacc_01 = BACC\n",
    "    mlp_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp_acc_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining ADA Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training ADA\n",
      "---------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction ADA\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "        0.0     1.0   2.0    3.0   4.0\n",
      "0.0  3804.0  7859.0   7.0    0.0   0.0\n",
      "1.0  6916.0   991.0  46.0    8.0   0.0\n",
      "2.0     3.0  1997.0  43.0   35.0   0.0\n",
      "3.0     0.0   265.0  32.0  255.0   0.0\n",
      "4.0     0.0     0.0   2.0    0.0  15.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.2292844959152527\n",
      "Precision total:  0.5260816525863901\n",
      "Recall total:  0.36308965924515896\n",
      "F1 total:  0.40402296705975915\n",
      "BACC total:  0.36308965924515896\n",
      "MCC total:  -0.36530994306887465\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining ADA Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "#ADA\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import time\n",
    "abc = AdaBoostClassifier(n_estimators=50, learning_rate=1.0)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    ada = abc.fit(X_train_01, y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(ada, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "\n",
    "    # Assuming 'model' is your trained model\n",
    "    joblib.dump(ada, 'ada_01.joblib')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    ada = joblib.load('ada_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    print('Prediction ADA')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    #ADA\n",
    "    start = time.time()\n",
    "    preds_ada_01 = ada.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('ADA 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_ada_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    ada_acc_01 = Acc\n",
    "    ada_pre_01 = Precision\n",
    "    ada_rec_01 = Recall\n",
    "    ada_f1_01 = F1\n",
    "    ada_bacc_01 = BACC\n",
    "    ada_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining KNN Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training KNN\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  11346.0   301.0    22.0    0.0   1.0\n",
      "1.0    276.0  7628.0    53.0    4.0   0.0\n",
      "2.0     10.0    35.0  2007.0   26.0   0.0\n",
      "3.0      0.0     1.0    13.0  538.0   0.0\n",
      "4.0      0.0     0.0     0.0    7.0  10.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9663793877367807\n",
      "Precision total:  0.9471681715525812\n",
      "Recall total:  0.8918226188895708\n",
      "F1 total:  0.9125366326903638\n",
      "BACC total:  0.8918226188895708\n",
      "MCC total:  0.9429543199596224\n"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining KNN Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf_01=KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training KNN')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training KNN', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    knn_clf_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(knn_clf_01, 'knn_01.joblib')\n",
    "\n",
    "\n",
    "if load_model_knn == 1:\n",
    "    knn_clf_01 = joblib.load('knn_01.joblib')\n",
    "\n",
    "if use_model_knn == 1:\n",
    "\n",
    "    #KNN\n",
    "    start = time.time()\n",
    "    preds_knn =knn_clf_01.predict(X_test_01)\n",
    "    preds_knn\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#MLP\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('KNN 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_knn\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "    knn_acc_01 = Acc\n",
    "    knn_pre_01 = Precision\n",
    "    knn_rec_01 = Recall\n",
    "    knn_f1_01 = F1\n",
    "    knn_bacc_01 = BACC\n",
    "    knn_mcc_01 = MCC    \n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "Defining Logistic Regression Model\n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "Training LR \n",
      "---------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  10518.0  1076.0    73.0    2.0  1.0\n",
      "1.0   1833.0  5912.0   185.0   31.0  0.0\n",
      "2.0    142.0     6.0  1832.0   98.0  0.0\n",
      "3.0      7.0     0.0   104.0  437.0  4.0\n",
      "4.0      2.0     0.0     0.0   14.0  1.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8393931232606159\n",
      "Precision total:  0.6878263380381966\n",
      "Recall total:  0.6752025512643705\n",
      "F1 total:  0.6752518450356755\n",
      "BACC total:  0.6752025512643705\n",
      "MCC total:  0.7270951233416538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Logistic Regression\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('Defining Logistic Regression Model')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "logreg_01 = LogisticRegression()\n",
    "\n",
    "if 1 == 1 and 0 == 0:\n",
    "\n",
    "    #KNN\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('Training LR ')\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Training LR', file = f)\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    start = time.time()\n",
    "    logreg_01.fit(X_train_01,y_train_01)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    # # Create the StratifiedKFold object\n",
    "    # stratified_kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # # Perform cross-validation\n",
    "    # cv_scores = cross_val_score(knn_clf, X_train, y_train, cv=stratified_kfold, scoring='accuracy')\n",
    "    # # Print the cross-validation scores\n",
    "    # print(\"Cross-validation scores:\", cv_scores)\n",
    "    # print(\"Mean accuracy:\", cv_scores.mean())\n",
    "    # with open(output_file_name, \"a\") as f: print('mean accuracy', cv_scores.mean() , file = f)\n",
    "\n",
    "\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed training time ', time_taken, file = f)\n",
    "    joblib.dump(logreg_01, 'logreg_01.joblib')\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "    logreg_01 = joblib.load('logreg_01.joblib')\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    #lR\n",
    "    start = time.time()\n",
    "    preds_logreg =logreg_01.predict(X_test_01)\n",
    "    end = time.time()\n",
    "    time_taken = end - start\n",
    "    with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#LR\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('LR 01 model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_logreg\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    lr_acc_01 = Acc\n",
    "    lr_pre_01 = Precision\n",
    "    lr_rec_01 = Recall\n",
    "    lr_f1_01 = F1\n",
    "    lr_bacc_01 = BACC\n",
    "    lr_mcc_01 = MCC\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3159672\ttotal: 18.8ms\tremaining: 1.86s\n",
      "1:\tlearn: 1.1257824\ttotal: 35ms\tremaining: 1.71s\n",
      "2:\tlearn: 0.9889719\ttotal: 51.4ms\tremaining: 1.66s\n",
      "3:\tlearn: 0.8795065\ttotal: 67.8ms\tremaining: 1.63s\n",
      "4:\tlearn: 0.7832594\ttotal: 84.2ms\tremaining: 1.6s\n",
      "5:\tlearn: 0.7054163\ttotal: 100ms\tremaining: 1.57s\n",
      "6:\tlearn: 0.6392608\ttotal: 116ms\tremaining: 1.54s\n",
      "7:\tlearn: 0.5839469\ttotal: 133ms\tremaining: 1.52s\n",
      "8:\tlearn: 0.5358800\ttotal: 150ms\tremaining: 1.52s\n",
      "9:\tlearn: 0.4963636\ttotal: 167ms\tremaining: 1.51s\n",
      "10:\tlearn: 0.4598019\ttotal: 183ms\tremaining: 1.48s\n",
      "11:\tlearn: 0.4258480\ttotal: 199ms\tremaining: 1.46s\n",
      "12:\tlearn: 0.3978712\ttotal: 216ms\tremaining: 1.45s\n",
      "13:\tlearn: 0.3728625\ttotal: 231ms\tremaining: 1.42s\n",
      "14:\tlearn: 0.3496802\ttotal: 247ms\tremaining: 1.4s\n",
      "15:\tlearn: 0.3285943\ttotal: 265ms\tremaining: 1.39s\n",
      "16:\tlearn: 0.3092808\ttotal: 281ms\tremaining: 1.37s\n",
      "17:\tlearn: 0.2930996\ttotal: 298ms\tremaining: 1.35s\n",
      "18:\tlearn: 0.2774927\ttotal: 314ms\tremaining: 1.34s\n",
      "19:\tlearn: 0.2635279\ttotal: 330ms\tremaining: 1.32s\n",
      "20:\tlearn: 0.2506635\ttotal: 345ms\tremaining: 1.3s\n",
      "21:\tlearn: 0.2395038\ttotal: 362ms\tremaining: 1.28s\n",
      "22:\tlearn: 0.2290368\ttotal: 378ms\tremaining: 1.26s\n",
      "23:\tlearn: 0.2189217\ttotal: 394ms\tremaining: 1.25s\n",
      "24:\tlearn: 0.2098895\ttotal: 413ms\tremaining: 1.24s\n",
      "25:\tlearn: 0.2010638\ttotal: 432ms\tremaining: 1.23s\n",
      "26:\tlearn: 0.1927545\ttotal: 447ms\tremaining: 1.21s\n",
      "27:\tlearn: 0.1856980\ttotal: 464ms\tremaining: 1.19s\n",
      "28:\tlearn: 0.1799763\ttotal: 478ms\tremaining: 1.17s\n",
      "29:\tlearn: 0.1736565\ttotal: 494ms\tremaining: 1.15s\n",
      "30:\tlearn: 0.1680234\ttotal: 509ms\tremaining: 1.13s\n",
      "31:\tlearn: 0.1631139\ttotal: 524ms\tremaining: 1.11s\n",
      "32:\tlearn: 0.1578428\ttotal: 542ms\tremaining: 1.1s\n",
      "33:\tlearn: 0.1534297\ttotal: 556ms\tremaining: 1.08s\n",
      "34:\tlearn: 0.1488088\ttotal: 569ms\tremaining: 1.06s\n",
      "35:\tlearn: 0.1454842\ttotal: 583ms\tremaining: 1.04s\n",
      "36:\tlearn: 0.1411809\ttotal: 598ms\tremaining: 1.02s\n",
      "37:\tlearn: 0.1374233\ttotal: 615ms\tremaining: 1s\n",
      "38:\tlearn: 0.1341830\ttotal: 629ms\tremaining: 983ms\n",
      "39:\tlearn: 0.1317133\ttotal: 642ms\tremaining: 964ms\n",
      "40:\tlearn: 0.1285191\ttotal: 657ms\tremaining: 946ms\n",
      "41:\tlearn: 0.1257573\ttotal: 673ms\tremaining: 930ms\n",
      "42:\tlearn: 0.1232248\ttotal: 688ms\tremaining: 913ms\n",
      "43:\tlearn: 0.1204975\ttotal: 703ms\tremaining: 895ms\n",
      "44:\tlearn: 0.1172064\ttotal: 721ms\tremaining: 881ms\n",
      "45:\tlearn: 0.1151697\ttotal: 735ms\tremaining: 863ms\n",
      "46:\tlearn: 0.1134082\ttotal: 749ms\tremaining: 844ms\n",
      "47:\tlearn: 0.1117979\ttotal: 763ms\tremaining: 827ms\n",
      "48:\tlearn: 0.1104840\ttotal: 777ms\tremaining: 809ms\n",
      "49:\tlearn: 0.1089744\ttotal: 793ms\tremaining: 793ms\n",
      "50:\tlearn: 0.1063464\ttotal: 811ms\tremaining: 779ms\n",
      "51:\tlearn: 0.1045515\ttotal: 826ms\tremaining: 762ms\n",
      "52:\tlearn: 0.1029574\ttotal: 843ms\tremaining: 747ms\n",
      "53:\tlearn: 0.1016602\ttotal: 858ms\tremaining: 731ms\n",
      "54:\tlearn: 0.0996965\ttotal: 874ms\tremaining: 715ms\n",
      "55:\tlearn: 0.0980592\ttotal: 889ms\tremaining: 699ms\n",
      "56:\tlearn: 0.0968468\ttotal: 905ms\tremaining: 683ms\n",
      "57:\tlearn: 0.0949610\ttotal: 921ms\tremaining: 667ms\n",
      "58:\tlearn: 0.0931858\ttotal: 938ms\tremaining: 652ms\n",
      "59:\tlearn: 0.0918645\ttotal: 957ms\tremaining: 638ms\n",
      "60:\tlearn: 0.0906806\ttotal: 970ms\tremaining: 620ms\n",
      "61:\tlearn: 0.0897011\ttotal: 985ms\tremaining: 604ms\n",
      "62:\tlearn: 0.0888573\ttotal: 1000ms\tremaining: 587ms\n",
      "63:\tlearn: 0.0876002\ttotal: 1.01s\tremaining: 571ms\n",
      "64:\tlearn: 0.0864520\ttotal: 1.03s\tremaining: 554ms\n",
      "65:\tlearn: 0.0850997\ttotal: 1.04s\tremaining: 538ms\n",
      "66:\tlearn: 0.0844383\ttotal: 1.06s\tremaining: 520ms\n",
      "67:\tlearn: 0.0838499\ttotal: 1.07s\tremaining: 503ms\n",
      "68:\tlearn: 0.0829670\ttotal: 1.08s\tremaining: 486ms\n",
      "69:\tlearn: 0.0819963\ttotal: 1.09s\tremaining: 469ms\n",
      "70:\tlearn: 0.0815365\ttotal: 1.11s\tremaining: 452ms\n",
      "71:\tlearn: 0.0801839\ttotal: 1.12s\tremaining: 436ms\n",
      "72:\tlearn: 0.0789416\ttotal: 1.14s\tremaining: 421ms\n",
      "73:\tlearn: 0.0782120\ttotal: 1.15s\tremaining: 405ms\n",
      "74:\tlearn: 0.0768678\ttotal: 1.17s\tremaining: 389ms\n",
      "75:\tlearn: 0.0763689\ttotal: 1.18s\tremaining: 373ms\n",
      "76:\tlearn: 0.0757235\ttotal: 1.19s\tremaining: 357ms\n",
      "77:\tlearn: 0.0745973\ttotal: 1.21s\tremaining: 341ms\n",
      "78:\tlearn: 0.0741425\ttotal: 1.23s\tremaining: 326ms\n",
      "79:\tlearn: 0.0733982\ttotal: 1.24s\tremaining: 310ms\n",
      "80:\tlearn: 0.0731660\ttotal: 1.25s\tremaining: 294ms\n",
      "81:\tlearn: 0.0725664\ttotal: 1.27s\tremaining: 279ms\n",
      "82:\tlearn: 0.0719588\ttotal: 1.28s\tremaining: 263ms\n",
      "83:\tlearn: 0.0712826\ttotal: 1.3s\tremaining: 248ms\n",
      "84:\tlearn: 0.0708260\ttotal: 1.31s\tremaining: 232ms\n",
      "85:\tlearn: 0.0697533\ttotal: 1.33s\tremaining: 217ms\n",
      "86:\tlearn: 0.0693257\ttotal: 1.35s\tremaining: 201ms\n",
      "87:\tlearn: 0.0686094\ttotal: 1.36s\tremaining: 186ms\n",
      "88:\tlearn: 0.0679090\ttotal: 1.38s\tremaining: 171ms\n",
      "89:\tlearn: 0.0667922\ttotal: 1.4s\tremaining: 155ms\n",
      "90:\tlearn: 0.0663771\ttotal: 1.41s\tremaining: 140ms\n",
      "91:\tlearn: 0.0660943\ttotal: 1.43s\tremaining: 124ms\n",
      "92:\tlearn: 0.0654995\ttotal: 1.44s\tremaining: 108ms\n",
      "93:\tlearn: 0.0651672\ttotal: 1.45s\tremaining: 92.8ms\n",
      "94:\tlearn: 0.0646553\ttotal: 1.47s\tremaining: 77.2ms\n",
      "95:\tlearn: 0.0636650\ttotal: 1.48s\tremaining: 61.8ms\n",
      "96:\tlearn: 0.0631230\ttotal: 1.5s\tremaining: 46.4ms\n",
      "97:\tlearn: 0.0627638\ttotal: 1.51s\tremaining: 30.9ms\n",
      "98:\tlearn: 0.0625746\ttotal: 1.53s\tremaining: 15.4ms\n",
      "99:\tlearn: 0.0616477\ttotal: 1.54s\tremaining: 0us\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (22278,1) into shape (22278)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-893d0dd0b569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                          ], voting='hard')\n\u001b[1;32m     18\u001b[0m \u001b[0mvoting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train_01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mvoring_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/HITL/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \"\"\"\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HITL/lib/python3.6/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 'hard' voting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             maj = np.apply_along_axis(\n\u001b[1;32m    287\u001b[0m                 lambda x: np.argmax(\n",
      "\u001b[0;32m~/anaconda3/envs/HITL/lib/python3.6/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/HITL/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (22278,1) into shape (22278)"
     ]
    }
   ],
   "source": [
    "# #Voting\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# # model1 = LogisticRegression(random_state=1)\n",
    "# # model2 = tree.DecisionTreeClassifier(random_state=1)\n",
    "# voting = VotingClassifier(estimators=[\n",
    "#                                         ('ada', ada),\n",
    "#                                        ('rf', rf),\n",
    "#                                        ('svm', clf),\n",
    "#                                        ('knn', knn_clf), \n",
    "#                                        ('lgbm', lgbm),\n",
    "#                                       #  ('xgb', xgb_00),\n",
    "#                                        ('cat', cat_00),\n",
    "\n",
    "#                                          ('mlp', mlp)\n",
    "#                                         #  ,('dnn', dnn_01)\n",
    "\n",
    "#                                          ], voting='hard')\n",
    "# voting.fit(X_train_01,y_train_01)\n",
    "# # voring_acc = voting.score(X_test_01,y_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VotingClassifier' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-504b633da256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds_voting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'VotingClassifier' object is not callable"
     ]
    }
   ],
   "source": [
    "# preds_voting = voting(X_test_01,y_test_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('CONFUSION MATRIX')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = preds_voting\n",
    "# # pred_label = label[ypred]\n",
    "\n",
    "# confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "# all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "# z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "# rows, cols = confusion_matrix.shape\n",
    "# z[:rows, :cols] = confusion_matrix\n",
    "# confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "# print(confusion_matrix)\n",
    "# with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "# FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "# FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "# TP = np.diag(confusion_matrix)\n",
    "# TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "# TP_total = sum(TP)\n",
    "# TN_total = sum(TN)\n",
    "# FP_total = sum(FP)\n",
    "# FN_total = sum(FN)\n",
    "\n",
    "# TP_total = np.array(TP_total,dtype=np.float64)\n",
    "# TN_total = np.array(TN_total,dtype=np.float64)\n",
    "# FP_total = np.array(FP_total,dtype=np.float64)\n",
    "# FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "# #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "# print('METRICS')\n",
    "# print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# # Precision = PRECISION(TP_total, FP_total)\n",
    "# # Recall = RECALL(TP_total, FN_total)\n",
    "# # F1 = F1(Recall,Precision)\n",
    "# # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "# # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "\n",
    "# Acc = accuracy_score(y_test_01, pred_label)\n",
    "# Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "# Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "# F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "# BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "# MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "# voting_acc_01 = Acc\n",
    "# voting_pre_01 = Precision\n",
    "# voting_rec_01 = Recall\n",
    "# voting_f1_01 = F1\n",
    "# voting_bacc_01 = BACC\n",
    "# voting_mcc_01 = MCC\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "# print('Accuracy total: ', Acc)\n",
    "# print('Precision total: ', Precision )\n",
    "# print('Recall total: ', Recall )\n",
    "# print('F1 total: ', F1 )\n",
    "# print('BACC total: ', BACC)\n",
    "# print('MCC total: ', MCC)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.calibration import CalibratedClassifierCV\n",
    "# with open(output_file_name, \"a\") as f: print('Generating Predictions', file = f)\n",
    "\n",
    "# if use_model_rf == 1:\n",
    "\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction RF')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction RF', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #RF\n",
    "#     start = time.time()\n",
    "#     preds_rf = rf.predict(X_test)\n",
    "#     preds_rf_prob = rf.predict_proba(X_test)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_svm == 1:\n",
    "\n",
    "#     print('Prediction SVM')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction SVM', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #SVM\n",
    "#     start = time.time()\n",
    "#     preds_svm = clf.predict(X_test)\n",
    "#     # preds_svm_prob = clf.predict_proba(X_test)\n",
    "\n",
    "#     #Since SVM does not deal with prob by nature we use a meta learner\n",
    "#     # https://stackoverflow.com/questions/55250963/how-to-get-probabilities-for-sgdclassifier-linearsvm\n",
    "\n",
    "#     model = CalibratedClassifierCV(clf)\n",
    "\n",
    "#     model.fit(X, y)\n",
    "#     preds_svm_prob = model.predict_proba(X)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_lgbm == 1:\n",
    "\n",
    "#     print('Prediction LGBM')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction LGBM', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #LGBM\n",
    "#     start = time.time()\n",
    "#     preds_lgbm = lgbm.predict(X_test)\n",
    "#     preds_lgbm_prob = lgbm.predict_proba(X_test)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_dnn == 1:\n",
    "\n",
    "#     print('Prediction DNN')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction DNN', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #DNN\n",
    "#     start = time.time()\n",
    "#     pred_dnn = dnn.predict(X_test)\n",
    "#     preds_dnn_prob = pred_dnn\n",
    "#     preds_dnn = np.argmax(pred_dnn,axis = 1)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_ada == 1:\n",
    "\n",
    "#     print('Prediction ADA')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction ADA', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     #ADA\n",
    "#     start = time.time()\n",
    "#     preds_ada = ada.predict(X_test)\n",
    "#     preds_ada_prob = ada.predict_proba(X_test)\n",
    "\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction MLP')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction MLP', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_mlp == 1:\n",
    "\n",
    "#     #MLP\n",
    "#     start = time.time()\n",
    "#     y_pred = MLP.predict_proba(X_test)\n",
    "#     preds_mlp_prob = y_pred\n",
    "#     preds_mlp = np.argmax(y_pred,axis = 1)\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "#     print('Prediction KNN')\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "#     with open(output_file_name, \"a\") as f: print('Prediction KNN', file = f)\n",
    "#     print('---------------------------------------------------------------------------------')\n",
    "\n",
    "# if use_model_knn == 1:\n",
    "\n",
    "#     #KNN\n",
    "#     start = time.time()\n",
    "#     preds_knn =knn_clf.predict(X_test)\n",
    "#     preds_knn_prob =knn_clf.predict_proba(X_test)\n",
    "\n",
    "#     preds_knn\n",
    "#     end = time.time()\n",
    "#     time_taken = end - start\n",
    "#     with open(output_file_name, \"a\") as f: print('Elapsed prediction time ', time_taken, file = f)\n",
    "#     with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3159672\ttest: 1.3145653\tbest: 1.3145653 (0)\ttotal: 21.3ms\tremaining: 2.11s\n",
      "10:\tlearn: 0.4598019\ttest: 0.4551612\tbest: 0.4551612 (10)\ttotal: 183ms\tremaining: 1.48s\n",
      "20:\tlearn: 0.2506635\ttest: 0.2463249\tbest: 0.2463249 (20)\ttotal: 315ms\tremaining: 1.18s\n",
      "30:\tlearn: 0.1680234\ttest: 0.1644089\tbest: 0.1644089 (30)\ttotal: 472ms\tremaining: 1.05s\n",
      "40:\tlearn: 0.1285191\ttest: 0.1255907\tbest: 0.1255907 (40)\ttotal: 604ms\tremaining: 870ms\n",
      "50:\tlearn: 0.1063464\ttest: 0.1038684\tbest: 0.1038684 (50)\ttotal: 733ms\tremaining: 704ms\n",
      "60:\tlearn: 0.0906806\ttest: 0.0885082\tbest: 0.0885082 (60)\ttotal: 868ms\tremaining: 555ms\n",
      "70:\tlearn: 0.0815365\ttest: 0.0798485\tbest: 0.0798485 (70)\ttotal: 992ms\tremaining: 405ms\n",
      "80:\tlearn: 0.0731660\ttest: 0.0720154\tbest: 0.0720154 (80)\ttotal: 1.12s\tremaining: 263ms\n",
      "90:\tlearn: 0.0663771\ttest: 0.0655500\tbest: 0.0655500 (90)\ttotal: 1.2s\tremaining: 119ms\n",
      "99:\tlearn: 0.0616477\ttest: 0.0610695\tbest: 0.0610695 (99)\ttotal: 1.26s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.06106954033\n",
      "bestIteration = 99\n",
      "\n",
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0  4.0\n",
      "0.0  10518.0  1076.0    73.0    2.0  1.0\n",
      "1.0   1833.0  5912.0   185.0   31.0  0.0\n",
      "2.0    142.0     6.0  1832.0   98.0  0.0\n",
      "3.0      7.0     0.0   104.0  437.0  4.0\n",
      "4.0      2.0     0.0     0.0   14.0  1.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.8393931232606159\n",
      "Precision total:  0.6878263380381966\n",
      "Recall total:  0.6752025512643705\n",
      "F1 total:  0.6752518450356755\n",
      "BACC total:  0.6752025512643705\n",
      "MCC total:  0.7270951233416538\n"
     ]
    }
   ],
   "source": [
    "import catboost\n",
    "\n",
    "cat_01 = catboost.CatBoostClassifier(iterations=100, depth=6, learning_rate=0.1, loss_function='MultiClass', custom_metric='Accuracy')\n",
    "\n",
    "# Fit the model\n",
    "cat_01.fit(X_train_01, y_train_01, eval_set=(X_test_01, y_test_01), verbose=10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_cat = cat_01.predict(X_test_01)\n",
    "preds_cat = np.squeeze(preds_cat)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('--------------------------------------------------------------------------', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('catboost', file = f)\n",
    "\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('CONFUSION MATRIX')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# pred_label = label[ypred]\n",
    "\n",
    "confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "rows, cols = confusion_matrix.shape\n",
    "z[:rows, :cols] = confusion_matrix\n",
    "confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "# confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "# with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "print(confusion_matrix)\n",
    "with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "TP = np.diag(confusion_matrix)\n",
    "TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "TP_total = sum(TP)\n",
    "TN_total = sum(TN)\n",
    "FP_total = sum(FP)\n",
    "FN_total = sum(FN)\n",
    "\n",
    "TP_total = np.array(TP_total,dtype=np.float64)\n",
    "TN_total = np.array(TN_total,dtype=np.float64)\n",
    "FP_total = np.array(FP_total,dtype=np.float64)\n",
    "FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "print('METRICS')\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "Acc = accuracy_score(y_test_01, pred_label)\n",
    "Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "\n",
    "cat_acc_01 = Acc\n",
    "cat_pre_01 = Precision\n",
    "cat_rec_01 = Recall\n",
    "cat_f1_01 = F1\n",
    "cat_bacc_01 = BACC\n",
    "cat_mcc_01 = MCC\n",
    "\n",
    "# with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "print('Accuracy total: ', Acc)\n",
    "cat_acc_01 = Acc\n",
    "print('Precision total: ', Precision )\n",
    "print('Recall total: ', Recall )\n",
    "print('F1 total: ', F1 )\n",
    "print('BACC total: ', BACC)\n",
    "print('MCC total: ', MCC)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------\n",
      "CONFUSION MATRIX\n",
      "---------------------------------------------------------------------------------\n",
      "         0.0     1.0     2.0    3.0   4.0\n",
      "0.0  11579.0    77.0    12.0    2.0   0.0\n",
      "1.0    157.0  7732.0    66.0    6.0   0.0\n",
      "2.0      3.0    11.0  2047.0   17.0   0.0\n",
      "3.0      0.0     1.0     6.0  545.0   0.0\n",
      "4.0      0.0     0.0     0.0    1.0  16.0\n",
      "---------------------------------------------------------------------------------\n",
      "METRICS\n",
      "---------------------------------------------------------------------------------\n",
      "Accuracy total:  0.9838854475267079\n",
      "Precision total:  0.978007667155827\n",
      "Recall total:  0.9754028236072573\n",
      "F1 total:  0.9764234163571771\n",
      "BACC total:  0.9754028236072573\n",
      "MCC total:  0.9726851550972443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create a DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_01, label=y_train_01)\n",
    "dtest = xgb.DMatrix(X_test_01, label=y_test_01)\n",
    "\n",
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # for multi-class classification\n",
    "    'num_class': 5,  # specify the number of classes\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'eval_metric': 'mlogloss'  # metric for multi-class classification\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "num_round = 100\n",
    "xgb_01 = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds_xgb_01 = xgb_01.predict(dtest)\n",
    "\n",
    "\n",
    "if 1 == 1:\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('---------------------------------------------------------------------------------', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('xgboost base model', file = f)\n",
    "\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('CONFUSION MATRIX')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "    pred_label = preds_xgb_01\n",
    "    # pred_label = label[ypred]\n",
    "\n",
    "    confusion_matrix = pd.crosstab(y_test_01, pred_label,rownames=['Actual ALERT'],colnames = ['Predicted ALERT'], dropna=False).sort_index(axis=0).sort_index(axis=1)\n",
    "    all_unique_values = sorted(set(pred_label) | set(y_test_01))\n",
    "    z = np.zeros((len(all_unique_values), len(all_unique_values)))\n",
    "    rows, cols = confusion_matrix.shape\n",
    "    z[:rows, :cols] = confusion_matrix\n",
    "    confusion_matrix  = pd.DataFrame(z, columns=all_unique_values, index=all_unique_values)\n",
    "    # confusion_matrix.to_csv('Ensemble_conf_matrix.csv')\n",
    "    # with open(output_file_name, \"a\") as f:print(confusion_matrix,file=f)\n",
    "    print(confusion_matrix)\n",
    "    with open(output_file_name, \"a\") as f: print('Confusion Matrix', file = f)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print(confusion_matrix, file = f)\n",
    "\n",
    "\n",
    "    FP = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    FN = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    TP = np.diag(confusion_matrix)\n",
    "    TN = confusion_matrix.values.sum() - (FP + FN + TP)\n",
    "    TP_total = sum(TP)\n",
    "    TN_total = sum(TN)\n",
    "    FP_total = sum(FP)\n",
    "    FN_total = sum(FN)\n",
    "\n",
    "    TP_total = np.array(TP_total,dtype=np.float64)\n",
    "    TN_total = np.array(TN_total,dtype=np.float64)\n",
    "    FP_total = np.array(FP_total,dtype=np.float64)\n",
    "    FN_total = np.array(FN_total,dtype=np.float64)\n",
    "\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------------#----------------------------------------------------------------\n",
    "\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "    print('METRICS')\n",
    "    print('---------------------------------------------------------------------------------')\n",
    "\n",
    "    # Acc = ACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # Precision = PRECISION(TP_total, FP_total)\n",
    "    # Recall = RECALL(TP_total, FN_total)\n",
    "    # F1 = F1(Recall,Precision)\n",
    "    # BACC = BACC(TP_total,TN_total, FP_total, FN_total)\n",
    "    # MCC = MCC(TP_total,TN_total, FP_total, FN_total)\n",
    "\n",
    "    Acc = accuracy_score(y_test_01, pred_label)\n",
    "    Precision = precision_score(y_test_01, pred_label, average='macro')\n",
    "    Recall = recall_score(y_test_01, pred_label, average='macro')\n",
    "    F1 =  f1_score(y_test_01, pred_label, average='macro')\n",
    "    BACC = balanced_accuracy_score(y_test_01, pred_label)\n",
    "    MCC = matthews_corrcoef(y_test_01, pred_label)\n",
    "\n",
    "    xgb_acc_01 = Acc\n",
    "    xgb_pre_01 = Precision\n",
    "    xgb_rec_01 = Recall\n",
    "    xgb_f1_01 = F1\n",
    "    xgb_bacc_01 = BACC\n",
    "    xgb_mcc_01 = MCC\n",
    "\n",
    "\n",
    "    # with open(output_file_name, \"a\") as f:print('Accuracy total: ', Acc,file=f)\n",
    "    print('Accuracy total: ', Acc)\n",
    "    print('Precision total: ', Precision )\n",
    "    print('Recall total: ', Recall )\n",
    "    print('F1 total: ', F1 )\n",
    "    print('BACC total: ', BACC)\n",
    "    print('MCC total: ', MCC)\n",
    "\n",
    "    with open(output_file_name, \"a\") as f: print('Accuracy total: ', Acc, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Precision total: ', Precision, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('Recall total: ', Recall , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('F1 total: ', F1, file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('BACC total: ', BACC , file = f)\n",
    "    with open(output_file_name, \"a\") as f: print('MCC total: ', MCC, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = tree.DecisionTreeClassifier()\n",
    "# model2 = KNeighborsClassifier()\n",
    "# model3= LogisticRegression()\n",
    "\n",
    "# model1.fit(x_train,y_train)\n",
    "# model2.fit(x_train,y_train)\n",
    "# model3.fit(x_train,y_train)\n",
    "\n",
    "# pred1=model1.predict_proba(x_test)\n",
    "# pred2=model2.predict_proba(x_test)\n",
    "# pred3=model3.predict_proba(x_test)\n",
    "\n",
    "# finalpred=(preds_svm_prob +\n",
    "#             preds_ada_prob +\n",
    "#             preds_knn_prob +\n",
    "#             preds_rf_prob +\n",
    "#             preds_dnn_prob +\n",
    "#             preds_lgbm_prob +\n",
    "#             preds_mlp_prob\n",
    "#             )/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voring_acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy cat: ', cat_acc_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_00, file = f)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision cat: ', cat_pre_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgb: ', xgb_pre_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall cat: ', cat_rec_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgb: ', xgb_rec_00, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 cat: ', cat_f1_00, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgb: ', xgb_f1_00, file = f)\n",
    "\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voting_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision LR: ', lr_pre_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Precision Voting: ', voting_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision catboosting: ', cat_pre_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Precision xgboost: ', xgb_pre_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall LR: ', lr_rec_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Recall Voting: ', voting_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall catboosting: ', cat_rec_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('Recall xgboost: ', xgb_rec_01, file = f)\n",
    "\n",
    "with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 LR: ', lr_f1_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('F1 Voting: ', voting_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 catboosting: ', cat_f1_01, file = f)\n",
    "with open(output_file_name, \"a\") as f: print('F1 xgboost: ', xgb_f1_01, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Summary', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 00', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy cat: ', cat_acc_00, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_00, file = f)\n",
    "\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision cat: ', cat_pre_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision xgb: ', xgb_pre_00, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall cat: ', cat_rec_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall xgb: ', xgb_rec_00, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 cat: ', cat_f1_00, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 xgb: ', xgb_f1_00, file = f)\n",
    "\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('-----------------------', file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Level 01', file = f)\n",
    "\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy ada: ', ada_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy dnn: ', dnn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy svm: ', svm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy knn: ', knn_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy mlp: ', mlp_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy lgbm: ', lgbm_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy rf: ', rf_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy LR: ', lr_acc_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Accuracy Voting: ', voting_acc, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy catboost: ', cat_acc_01, file = f)\n",
    "# with open(output_file_name, \"a\") as f: print('Accuracy xgb: ', xgb_acc_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision ada: ', ada_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision dnn: ', dnn_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision svm: ', svm_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision knn: ', knn_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision mlp: ', mlp_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision lgbm: ', lgbm_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision rf: ', rf_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision LR: ', lr_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision Voting: ', voting_pre, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision catboosting: ', cat_pre_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Precision xgboost: ', xgb_pre_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall ada: ', ada_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall dnn: ', dnn_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall svm: ', svm_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall knn: ', knn_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall mlp: ', mlp_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall lgbm: ', lgbm_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall rf: ', rf_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall LR: ', lr_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall Voting: ', voting_rec, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall catboosting: ', cat_rec_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('Recall xgboost: ', xgb_rec_01, file = f)\n",
    "\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 ada: ', ada_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 dnn: ', dnn_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 svm: ', svm_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 knn: ', knn_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 mlp: ', mlp_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 lgbm: ', lgbm_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 rf: ', rf_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 LR: ', lr_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 Voting: ', voting_f1, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 catboosting: ', cat_f1_01, file = f)\n",
    "# # with open(output_file_name, \"a\") as f: print('F1 xgboost: ', xgb_f1_01, file = f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sklearn\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# split = 0.7\n",
    "\n",
    "# #AUC ROC\n",
    "# #---------------------------------------------------------------------\n",
    "\n",
    "# #AUCROC\n",
    "# aucroc =[]\n",
    "# y_array = [y_0,y_1,y_2,y_3,y_4]\n",
    "# for j in range(0,len(y_array)):\n",
    "#     # print(j)\n",
    "#     #------------------------------------------------------------------------------------------------------------\n",
    "#     X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y_array[j], train_size=split)\n",
    "    \n",
    "#     # evaluate the model\n",
    "\n",
    "#     knn_clf.fit(X_train,y_train)\n",
    "#     y_pred=knn_clf.predict(X_test) #These are the predicted output value\n",
    "#     # y_pred = knn_clf.predict_proba(X_test)\n",
    "\n",
    "    \n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "\n",
    "#     # model = LGBMClassifier()\n",
    "#     # model.fit(X_train, y_train)\n",
    "#     # y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "#     y_scores = y_pred\n",
    "#     y_true = y_test\n",
    "    \n",
    "#     # Calculate AUC-ROC score\n",
    "#     auc_roc_score= roc_auc_score(y_true, y_scores,  average='weighted')  # Use 'micro' or 'macro' for different averaging strategies\n",
    "#     # print(\"AUC-ROC Score class:\", auc_roc_score)\n",
    "#     aucroc.append(auc_roc_score)\n",
    "#     #-------------------------------------------------------------------------------------------------------    -----\n",
    "#     # Calculate the average\n",
    "# average = sum(aucroc) / len(aucroc)\n",
    "\n",
    "# # Display the result\n",
    "# # with open(output_file_name, \"a\") as f:print(\"AUC ROC Average:\", average, file = f)\n",
    "# print(\"AUC ROC Average:\", average)\n",
    "\n",
    "# #End AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_acc_00 = 0 \n",
    "voting_acc_00 = 0\n",
    "\n",
    "lr_pre_00 = 0 \n",
    "voting_pre_00 = 0\n",
    "\n",
    "lr_rec_00 = 0 \n",
    "voting_rec_00 = 0\n",
    "\n",
    "lr_f1_00 = 0 \n",
    "voting_f1_00 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "voting_acc_01 = 0\n",
    "\n",
    "\n",
    "voting_pre_01 = 0\n",
    "\n",
    "\n",
    "voting_rec_01 = 0\n",
    "\n",
    "\n",
    "voting_f1_01 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+\n",
      "| Accuracy   | Level 00           | Level 01           |\n",
      "+============+====================+====================+\n",
      "| ADA        | 0.615454019041463  | 0.2292844959152527 |\n",
      "+------------+--------------------+--------------------+\n",
      "| SVM        | 0.9693909155792564 | 0.8195529221653649 |\n",
      "+------------+--------------------+--------------------+\n",
      "| DNN        | 0.8585760648540918 | 0.777179280007182  |\n",
      "+------------+--------------------+--------------------+\n",
      "| MLP        | 0.9924049610148264 | 0.9377412694137714 |\n",
      "+------------+--------------------+--------------------+\n",
      "| KNN        | 0.9889441010517244 | 0.9663793877367807 |\n",
      "+------------+--------------------+--------------------+\n",
      "| CAT        | 0.9920144359606243 | 0.8393931232606159 |\n",
      "+------------+--------------------+--------------------+\n",
      "| XGB        | 0.9919875031982656 | 0.9838854475267079 |\n",
      "+------------+--------------------+--------------------+\n",
      "| LGBM       | 0.9805545455769671 | 0.9770176856091211 |\n",
      "+------------+--------------------+--------------------+\n",
      "| RF         | 0.9517634226154406 | 0.938369692072897  |\n",
      "+------------+--------------------+--------------------+\n",
      "| LR         | 0                  | 0.8393931232606159 |\n",
      "+------------+--------------------+--------------------+\n",
      "| VOTING     | 0                  | 0                  |\n",
      "+------------+--------------------+--------------------+\n",
      "|            |                    |                    |\n",
      "+------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_acc = [ada_acc_00,\n",
    "                svm_acc_00,\n",
    "                dnn_acc_00,\n",
    "                mlp_acc_00,\n",
    "                knn_acc_00,\n",
    "                cat_acc_00,\n",
    "                xgb_acc_00,\n",
    "                lgbm_acc_00,\n",
    "                rf_acc_00,\n",
    "                lr_acc_00,\n",
    "                voting_acc_00]  \n",
    "level_01_acc = [ada_acc_01,\n",
    "                svm_acc_01,\n",
    "                dnn_acc_01,\n",
    "                mlp_acc_01,\n",
    "                knn_acc_01,\n",
    "                cat_acc_01,\n",
    "                xgb_acc_01,\n",
    "                lgbm_acc_01,\n",
    "                rf_acc_01,\n",
    "                lr_acc_01,\n",
    "                voting_acc_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Accuracy\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------------+--------------------+\n",
      "| Precision   | Level 00            | Level 01           |\n",
      "+=============+=====================+====================+\n",
      "| ADA         | 0.632923263976516   | 0.5260816525863901 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| SVM         | 0.8827645887402553  | 0.6302904680558632 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| DNN         | 0.35954806058543765 | 0.4484352807140194 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| MLP         | 0.9350853557454828  | 0.9352098919045874 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| KNN         | 0.9315441895221477  | 0.9471681715525812 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| CAT         | 0.9317815323212976  | 0.6878263380381966 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| XGB         | 0.916516593252323   | 0.978007667155827  |\n",
      "+-------------+---------------------+--------------------+\n",
      "| LGBM        | 0.7902176964513938  | 0.777514374044633  |\n",
      "+-------------+---------------------+--------------------+\n",
      "| RF          | 0.7708229249221301  | 0.7269733471723125 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| LR          | 0                   | 0.6878263380381966 |\n",
      "+-------------+---------------------+--------------------+\n",
      "| VOTING      | 0                   | 0                  |\n",
      "+-------------+---------------------+--------------------+\n",
      "|             |                     |                    |\n",
      "+-------------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_pre = [ada_pre_00,\n",
    "                svm_pre_00,\n",
    "                dnn_pre_00,\n",
    "                mlp_pre_00,\n",
    "                knn_pre_00,\n",
    "                cat_pre_00,\n",
    "                xgb_pre_00,\n",
    "                lgbm_pre_00,\n",
    "                rf_pre_00,\n",
    "                lr_pre_00,\n",
    "                voting_pre_00]  \n",
    "level_01_pre = [ada_pre_01,\n",
    "                svm_pre_01,\n",
    "                dnn_pre_01,\n",
    "                mlp_pre_01,\n",
    "                knn_pre_01,\n",
    "                cat_pre_01,\n",
    "                xgb_pre_01,\n",
    "                lgbm_pre_01,\n",
    "                rf_pre_01,\n",
    "                lr_pre_01,\n",
    "                voting_pre_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_pre[i]\n",
    "    data[i][2] = level_01_pre[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Precision\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------------+\n",
      "| Recall   | Level 00           | Level 01            |\n",
      "+==========+====================+=====================+\n",
      "| ADA      | 0.541715952581571  | 0.36308965924515896 |\n",
      "+----------+--------------------+---------------------+\n",
      "| SVM      | 0.784198426409409  | 0.5806167198793248  |\n",
      "+----------+--------------------+---------------------+\n",
      "| DNN      | 0.3900990696636019 | 0.45379527103143447 |\n",
      "+----------+--------------------+---------------------+\n",
      "| MLP      | 0.8924829434684349 | 0.9575907528934161  |\n",
      "+----------+--------------------+---------------------+\n",
      "| KNN      | 0.8698084938389755 | 0.8918226188895708  |\n",
      "+----------+--------------------+---------------------+\n",
      "| CAT      | 0.8334771923068951 | 0.6752025512643705  |\n",
      "+----------+--------------------+---------------------+\n",
      "| XGB      | 0.8943712402251023 | 0.9754028236072573  |\n",
      "+----------+--------------------+---------------------+\n",
      "| LGBM     | 0.82425692517516   | 0.8073372511731085  |\n",
      "+----------+--------------------+---------------------+\n",
      "| RF       | 0.5962348969277227 | 0.735945739363979   |\n",
      "+----------+--------------------+---------------------+\n",
      "| LR       | 0                  | 0.6752025512643705  |\n",
      "+----------+--------------------+---------------------+\n",
      "| VOTING   | 0                  | 0                   |\n",
      "+----------+--------------------+---------------------+\n",
      "|          |                    |                     |\n",
      "+----------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_rec = [ada_rec_00,\n",
    "                svm_rec_00,\n",
    "                dnn_rec_00,\n",
    "                mlp_rec_00,\n",
    "                knn_rec_00,\n",
    "                cat_rec_00,\n",
    "                xgb_rec_00,\n",
    "                lgbm_rec_00,\n",
    "                rf_rec_00,\n",
    "                lr_rec_00,\n",
    "                voting_rec_00]  \n",
    "level_01_rec = [ada_rec_01,\n",
    "                svm_rec_01,\n",
    "                dnn_rec_01,\n",
    "                mlp_rec_01,\n",
    "                knn_rec_01,\n",
    "                cat_rec_01,\n",
    "                xgb_rec_01,\n",
    "                lgbm_rec_01,\n",
    "                rf_rec_01,\n",
    "                lr_rec_01,\n",
    "                voting_rec_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_rec[i]\n",
    "    data[i][2] = level_01_rec[i]\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Recall\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+\n",
      "| F1     | Level 00           | Level 01            |\n",
      "+========+====================+=====================+\n",
      "| ADA    | 0.5048072353676654 | 0.40402296705975915 |\n",
      "+--------+--------------------+---------------------+\n",
      "| SVM    | 0.8204655892263848 | 0.5940391791421344  |\n",
      "+--------+--------------------+---------------------+\n",
      "| DNN    | 0.369539913853707  | 0.44749970805741135 |\n",
      "+--------+--------------------+---------------------+\n",
      "| MLP    | 0.9094645814131586 | 0.9458166048227209  |\n",
      "+--------+--------------------+---------------------+\n",
      "| KNN    | 0.8947376203615685 | 0.9125366326903638  |\n",
      "+--------+--------------------+---------------------+\n",
      "| CAT    | 0.8627391808106865 | 0.6752518450356755  |\n",
      "+--------+--------------------+---------------------+\n",
      "| XGB    | 0.9045829083245825 | 0.9764234163571771  |\n",
      "+--------+--------------------+---------------------+\n",
      "| LGBM   | 0.8028434238196789 | 0.7901736960971538  |\n",
      "+--------+--------------------+---------------------+\n",
      "| RF     | 0.6209815960412555 | 0.7302137693773736  |\n",
      "+--------+--------------------+---------------------+\n",
      "| LR     | 0                  | 0.6752518450356755  |\n",
      "+--------+--------------------+---------------------+\n",
      "| VOTING | 0                  | 0                   |\n",
      "+--------+--------------------+---------------------+\n",
      "|        |                    |                     |\n",
      "+--------+--------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(3)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "    data[i][1] = level_00_f1[i]\n",
    "    data[i][2] = level_01_f1[i]\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"F1\", \"Level 00\", \"Level 01\"]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming data is a 110x4 list, where each row is a sublist\n",
    "# data =  [[\"Row {} Col {}\".format(i + 1, j + 1) for j in range(4)] for i in range(110)]\n",
    "data = [[\"\" for _ in range(9)] for _ in range(12)]\n",
    "\n",
    "# Manually insert data at specific row and column\n",
    "# data[0][0] = \"ADA\"\n",
    "# data[1][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "# data[3][0] = \"ADA\"\n",
    "# data[4][0] = \"DNN\"\n",
    "# data[2][0] = \"SVM\"\n",
    "\n",
    "\n",
    "names_models = ['ADA',\n",
    "                'SVM',\n",
    "                'DNN',\n",
    "                'MLP',\n",
    "                'KNN',\n",
    "                'CAT',\n",
    "                'XGB',\n",
    "                'LGBM',\n",
    "                'RF',\n",
    "                'LR',\n",
    "                'VOTING'\n",
    "                ]\n",
    "level_00_f1 = [ada_f1_00,\n",
    "                svm_f1_00,\n",
    "                dnn_f1_00,\n",
    "                mlp_f1_00,\n",
    "                knn_f1_00,\n",
    "                cat_f1_00,\n",
    "                xgb_f1_00,\n",
    "                lgbm_f1_00,\n",
    "                rf_f1_00,\n",
    "                lr_f1_00,\n",
    "                voting_f1_00]  \n",
    "level_01_f1 = [ada_f1_01,\n",
    "                svm_f1_01,\n",
    "                dnn_f1_01,\n",
    "                mlp_f1_01,\n",
    "                knn_f1_01,\n",
    "                cat_f1_01,\n",
    "                xgb_f1_01,\n",
    "                lgbm_f1_01,\n",
    "                rf_f1_01,\n",
    "                lr_f1_01,\n",
    "                voting_f1_01]  \n",
    "                 \n",
    "\n",
    "for i in range(0,len(names_models)):\n",
    "    data[i][0] =  names_models[i]\n",
    "\n",
    "    data[i][1] = level_00_acc[i]\n",
    "    data[i][2] = level_01_acc[i]\n",
    "\n",
    "    data[i][3] = level_00_pre[i] \n",
    "    data[i][4] = level_01_pre[i]\n",
    "\n",
    "    data[i][5] = level_00_rec[i] \n",
    "    data[i][6] = level_01_rec[i]\n",
    "\n",
    "    data[i][7] = level_00_f1[i]\n",
    "    data[i][8] = level_01_f1[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# data[0][1] = ada_acc_00\n",
    "# data\n",
    "\n",
    "# Define column headers\n",
    "headers = [\"Models\", \"ACC-00\", \" ACC-01\",\"PRE-00\", \" PRE-01\",\"REC-00\", \" REC-01\",\"F1-00\", \" F1-01\",]\n",
    "\n",
    "# Print the table\n",
    "table = tabulate(data, headers=headers, tablefmt=\"grid\")\n",
    "print(table)\n",
    "with open(output_file_name, \"a\") as f: print(table, file = f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
